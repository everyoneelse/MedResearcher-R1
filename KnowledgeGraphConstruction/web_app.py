#!/usr/bin/env python3
"""
çŸ¥è¯†å›¾è°±æ„å»ºWebåº”ç”¨
æä¾›å‰ç«¯ç•Œé¢æ¥è¿è¡ŒçŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿ
"""

import json
import logging
import asyncio
import csv
import io
from datetime import datetime
from flask import Flask, render_template, request, jsonify, send_from_directory, send_file
from flask_socketio import SocketIO, emit
import threading
import sys
import os
import concurrent.futures
from threading import Lock
import time

# é¢„å®šä¹‰çš„é¢†åŸŸæ ‡ç­¾åˆ—è¡¨
PREDEFINED_DOMAIN_TAGS = {'ä½“è‚²', 'å­¦æœ¯', 'æ”¿æ²»', 'å¨±ä¹', 'æ–‡å­¦', 'æ–‡åŒ–', 'ç»æµ', 'ç§‘æŠ€', 'å†å²', 'åŒ»ç–—', 'å…¶ä»–'}

# æ·»åŠ libç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'lib'))

from lib.graphrag_builder import GraphRagBuilder
from lib.run_manager import RunManager
from lib.runs_qa_generator import RunsQAGenerator

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# å…¨å±€å˜é‡å­˜å‚¨æ„å»ºçŠ¶æ€
building_status = {
    'is_running': False,
    'current_step': '',
    'progress': 0,
    'graph_data': {'nodes': [], 'links': []},
    'logs': [],
    'run_id': None,
    'qa_result': None
}

class WebSocketHandler(logging.Handler):
    """è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—å‘é€åˆ°WebSocket - æ”¯æŒtrace"""
    
    def emit(self, record):
        # è·å–trace IDï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        trace_id = getattr(record, 'trace_id', 'NO_TRACE')
        
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'level': record.levelname,
            'message': self.format(record),
            'trace_id': trace_id
        }
        building_status['logs'].append(log_entry)
        socketio.emit('log_message', log_entry)

def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ - å¸¦æœ‰traceæ”¯æŒ"""
    from lib.trace_manager import TraceFormatter
    from config import setup_global_logging
    
    # é¦–å…ˆè®¾ç½®å…¨å±€æ—¥å¿—ï¼ˆåŒ…å«æ–‡ä»¶æ—¥å¿—å’Œtraceæ”¯æŒï¼‰
    log_filename = setup_global_logging()
    
    # è·å–æ ¹logger
    root_logger = logging.getLogger()
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰WebSocketå¤„ç†å™¨ï¼Œé¿å…é‡å¤æ·»åŠ 
    has_websocket_handler = any(isinstance(handler, WebSocketHandler) for handler in root_logger.handlers)
    
    if not has_websocket_handler:
        # åˆ›å»ºWebSocketå¤„ç†å™¨ï¼ˆå¸¦traceæ”¯æŒï¼‰
        ws_handler = WebSocketHandler()
        trace_formatter = TraceFormatter('%(asctime)s [%(trace_id)s] - %(name)s - %(levelname)s - %(message)s')
        ws_handler.setFormatter(trace_formatter)
        
        # æ·»åŠ WebSocketå¤„ç†å™¨
        root_logger.addHandler(ws_handler)
    
    # æ£€æŸ¥æ§åˆ¶å°å¤„ç†å™¨
    has_console_handler = any(isinstance(handler, logging.StreamHandler) and handler.stream == sys.stdout 
                             for handler in root_logger.handlers)
    
    if not has_console_handler:
        # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆå¸¦traceæ”¯æŒï¼‰
        console_handler = logging.StreamHandler(sys.stdout)
        console_formatter = TraceFormatter('%(asctime)s [%(trace_id)s] - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(console_formatter)
        root_logger.addHandler(console_handler)
    
    # è·å–loggerå¹¶è®°å½•åˆå§‹åŒ–ä¿¡æ¯
    setup_logger = logging.getLogger(__name__)
    setup_logger.info(f"Webåº”ç”¨æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_filename}")

setup_logging()
logger = logging.getLogger(__name__)

@app.route('/')
def index():
    """ä¸»é¡µå¯¼èˆª"""
    return render_template('navigation.html')

@app.route('/single-qa')
def single_qa():
    """å•æ¡QAæµ‹è¯•é¡µé¢"""
    return render_template('single_qa.html')

@app.route('/batch-generation')
def batch_generation():
    """æ‰¹é‡ç”Ÿæˆé¡µé¢"""
    return render_template('batch_generation.html')

@app.route('/data-evaluation')
def data_evaluation():
    """æ•°æ®è¯„æµ‹é¡µé¢"""
    return render_template('data_evaluation.html')

@app.route('/comparison-evaluation')
def comparison_evaluation():
    """å¯¹æ¯”è¯„æµ‹é¡µé¢"""
    return render_template('comparison_evaluation.html')

@app.route('/runs-qa-generation')
def runs_qa_generation():
    """Runsè®°å½•QAç”Ÿæˆé¡µé¢"""
    return render_template('runs_qa_generation.html')

@app.route('/data-management')
def data_management():
    """æ•°æ®ç®¡ç†é¡µé¢"""
    return render_template('data_management.html')

@app.route('/modern')
def modern_app():
    """ç°ä»£åŒ–ç‰ˆæœ¬ä¸»é¡µ"""
    return render_template('modern_app.html')

@app.route('/domain-tags')
def domain_tags():
    return render_template('domain_tags.html')

@app.route('/api/start_building', methods=['POST'])
def start_building():
    """å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±"""
    from lib.trace_manager import start_trace
    
    # å¯åŠ¨trace
    trace_id = start_trace(prefix="web")
    logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±è¯·æ±‚")
    
    if building_status['is_running']:
        return jsonify({'error': 'ç³»ç»Ÿæ­£åœ¨è¿è¡Œä¸­'}), 400
    
    data = request.json
    entity = data.get('entity', 'èš‚èšé›†å›¢')
    max_nodes = data.get('max_nodes', 200)
    max_iterations = data.get('max_iterations', 10)
    sample_size = data.get('sample_size', 12)
    sampling_algorithm = data.get('sampling_algorithm', 'mixed')
    
    # åˆ›å»ºè¿è¡Œç®¡ç†å™¨
    run_manager = RunManager()
    run_id = run_manager.create_new_run(f"kg_build_{entity}")
    
    # é‡ç½®çŠ¶æ€
    building_status['is_running'] = True
    building_status['current_step'] = 'åˆå§‹åŒ–'
    building_status['progress'] = 0
    building_status['graph_data'] = {'nodes': [], 'links': []}
    building_status['logs'] = []
    building_status['run_id'] = run_id
    building_status['qa_result'] = None
    
    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ„å»ºè¿‡ç¨‹
    thread = threading.Thread(target=run_building_process, args=(entity, max_nodes, sample_size, run_manager, max_iterations, sampling_algorithm))
    thread.daemon = True
    thread.start()
    
    return jsonify({'message': 'å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±', 'entity': entity, 'run_id': run_id})

@app.route('/api/status')
def get_status():
    """è·å–æ„å»ºçŠ¶æ€"""
    return jsonify(building_status)

@app.route('/api/stop_building', methods=['POST'])
def stop_building():
    """åœæ­¢æ„å»º"""
    building_status['is_running'] = False
    building_status['current_step'] = 'å·²åœæ­¢'
    return jsonify({'message': 'å·²åœæ­¢æ„å»º'})

@app.route('/api/generate/single', methods=['POST'])
def generate_single():
    """å•ç‚¹æŸ¥è¯¢æ¥å£ - æŒ‰ç ”å‘è®¡åˆ’è®¾è®¡"""
    from lib.trace_manager import start_trace
    
    # å¯åŠ¨trace
    trace_id = start_trace(prefix="single")
    logger.info(f"æ¥æ”¶å•æ¡QAç”Ÿæˆè¯·æ±‚")
    
    if building_status['is_running']:
        return jsonify({'error': 'ç³»ç»Ÿæ­£åœ¨è¿è¡Œä¸­'}), 400
    
    data = request.json
    entity = data.get('entity', 'é‡å­è®¡ç®—æœº')
    sampling_algorithm = data.get('sampling_algorithm', 'mixed')
    
    logger.info(f"å•æ¡QAç”Ÿæˆå‚æ•°: å®ä½“={entity}, é‡‡æ ·ç®—æ³•={sampling_algorithm}")
    
    # åˆ›å»ºè¿è¡Œç®¡ç†å™¨
    run_manager = RunManager()
    run_id = run_manager.create_new_run(f"single_{entity}")
    
    # é‡ç½®çŠ¶æ€
    building_status['is_running'] = True
    building_status['current_step'] = 'åˆå§‹åŒ–'
    building_status['progress'] = 0
    building_status['graph_data'] = {'nodes': [], 'links': []}
    building_status['logs'] = []
    building_status['run_id'] = run_id
    building_status['qa_result'] = None
    
    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ„å»ºè¿‡ç¨‹
    thread = threading.Thread(target=run_building_process, args=(entity, 30, 8, run_manager, 3, sampling_algorithm))
    thread.daemon = True
    thread.start()
    
    return jsonify({'job_id': run_id})

@app.route('/api/generate/batch', methods=['POST'])
def generate_batch():
    """æ‰¹é‡ç”Ÿæˆæ¥å£ - æŒ‰ç ”å‘è®¡åˆ’è®¾è®¡"""
    if building_status['is_running']:
        return jsonify({'error': 'ç³»ç»Ÿæ­£åœ¨è¿è¡Œä¸­'}), 400
    
    data = request.json
    entities = data.get('entities', ['é‡å­è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'åŸºå› ç¼–è¾‘'])
    
    # åˆ›å»ºè¿è¡Œç®¡ç†å™¨
    run_manager = RunManager()
    run_id = run_manager.create_new_run(f"batch_{len(entities)}_entities")
    
    # é‡ç½®çŠ¶æ€
    building_status['is_running'] = True
    building_status['current_step'] = 'æ‰¹é‡åˆå§‹åŒ–'
    building_status['progress'] = 0
    building_status['graph_data'] = {'nodes': [], 'links': []}
    building_status['logs'] = []
    building_status['run_id'] = run_id
    building_status['qa_result'] = None
    
    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ‰¹é‡æ„å»ºè¿‡ç¨‹
    thread = threading.Thread(target=run_batch_building_process, args=(entities, run_manager))
    thread.daemon = True
    thread.start()
    
    return jsonify({'batch_job_id': run_id})

@app.route('/api/qa/<job_id>')
def get_qa_result(job_id):
    """è·å–QAç”Ÿæˆç»“æœ"""
    if building_status['run_id'] == job_id and building_status['qa_result']:
        return jsonify(building_status['qa_result'])
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯
    return jsonify({
        'question': 'æš‚æ— å¯ç”¨çš„QAç»“æœ',
        'answer': 'è¯·å…ˆå®ŒæˆçŸ¥è¯†å›¾è°±æ„å»º'
    })

# æ‰¹é‡ç”Ÿæˆç›¸å…³API
# å®ä½“é›†ç®¡ç†API
@app.route('/api/entity_sets/save', methods=['POST'])
def save_entity_set():
    """ä¿å­˜å®ä½“é›†"""
    try:
        data = request.json
        name = data.get('name', '').strip()
        entities = data.get('entities', [])
        import_method = data.get('import_method', 'manual_newline')
        
        if not name:
            return jsonify({'error': 'è¯·æä¾›å®ä½“é›†åç§°'}), 400
        
        if not entities:
            return jsonify({'error': 'è¯·æä¾›å®ä½“æ•°æ®'}), 400
        
        # åˆ›å»ºå®ä½“é›†ç›®å½•
        entity_sets_dir = "evaluation_data/entity_sets"
        os.makedirs(entity_sets_dir, exist_ok=True)
        
        # ä¿å­˜ä¸ºCSVæ–‡ä»¶
        csv_path = os.path.join(entity_sets_dir, f"{name}.csv")
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['entity'])  # å¤´éƒ¨
            for entity in entities:
                writer.writerow([entity])
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'name': name,
            'count': len(entities),
            'created_at': datetime.now().isoformat(),
            'import_method': import_method,
            'file_path': csv_path
        }
        
        metadata_path = os.path.join(entity_sets_dir, f"{name}.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å®ä½“é›† '{name}' ä¿å­˜æˆåŠŸï¼Œå…±{len(entities)}ä¸ªå®ä½“")
        
        return jsonify({
            'success': True,
            'message': 'å®ä½“é›†ä¿å­˜æˆåŠŸ',
            'count': len(entities)
        })
        
    except Exception as e:
        logger.error(f"ä¿å­˜å®ä½“é›†å¤±è´¥: {e}")
        return jsonify({'error': f'ä¿å­˜å¤±è´¥: {str(e)}'}), 500

@app.route('/api/entity_sets/upload', methods=['POST'])
def upload_entity_set():
    """ä¸Šä¼ CSVæ–‡ä»¶ä¿å­˜å®ä½“é›†"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400
        
        file = request.files['file']
        name = request.form.get('name', '').strip()
        
        if not name:
            return jsonify({'error': 'è¯·æä¾›å®ä½“é›†åç§°'}), 400
        
        if file.filename == '':
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        # åˆ›å»ºå®ä½“é›†ç›®å½•
        entity_sets_dir = "evaluation_data/entity_sets"
        os.makedirs(entity_sets_dir, exist_ok=True)
        
        # è¯»å–CSVæ–‡ä»¶
        stream = io.StringIO(file.stream.read().decode("utf-8"))
        reader = csv.reader(stream)
        
        entities = []
        for row in reader:
            if row and row[0].strip():  # åªå–ç¬¬ä¸€åˆ—ï¼Œä¸”éç©º
                entities.append(row[0].strip())
        
        if not entities:
            return jsonify({'error': 'CSVæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å®ä½“æ•°æ®'}), 400
        
        # ä¿å­˜ä¸ºCSVæ–‡ä»¶
        csv_path = os.path.join(entity_sets_dir, f"{name}.csv")
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['entity'])  # å¤´éƒ¨
            for entity in entities:
                writer.writerow([entity])
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'name': name,
            'count': len(entities),
            'created_at': datetime.now().isoformat(),
            'import_method': 'csv_file',
            'file_path': csv_path
        }
        
        metadata_path = os.path.join(entity_sets_dir, f"{name}.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å®ä½“é›† '{name}' ä¸Šä¼ ä¿å­˜æˆåŠŸï¼Œå…±{len(entities)}ä¸ªå®ä½“")
        
        return jsonify({
            'success': True,
            'message': 'å®ä½“é›†ä¸Šä¼ æˆåŠŸ',
            'count': len(entities)
        })
        
    except Exception as e:
        logger.error(f"ä¸Šä¼ å®ä½“é›†å¤±è´¥: {e}")
        return jsonify({'error': f'ä¸Šä¼ å¤±è´¥: {str(e)}'}), 500

@app.route('/api/entity_sets/list', methods=['GET'])
def list_entity_sets():
    """è·å–å®ä½“é›†åˆ—è¡¨"""
    try:
        entity_sets_dir = "evaluation_data/entity_sets"
        
        if not os.path.exists(entity_sets_dir):
            return jsonify({'success': True, 'entity_sets': []})
        
        entity_sets = []
        for file in os.listdir(entity_sets_dir):
            if file.endswith('.json'):
                try:
                    metadata_path = os.path.join(entity_sets_dir, file)
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    entity_sets.append(metadata)
                except Exception as e:
                    logger.warning(f"è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {file}, é”™è¯¯: {e}")
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        entity_sets.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        
        return jsonify({
            'success': True,
            'entity_sets': entity_sets
        })
        
    except Exception as e:
        logger.error(f"è·å–å®ä½“é›†åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/entity_sets/info/<name>', methods=['GET'])
def get_entity_set_info(name):
    """è·å–å®ä½“é›†è¯¦æƒ…"""
    try:
        entity_sets_dir = "evaluation_data/entity_sets"
        metadata_path = os.path.join(entity_sets_dir, f"{name}.json")
        
        if not os.path.exists(metadata_path):
            return jsonify({'error': 'å®ä½“é›†ä¸å­˜åœ¨'}), 404
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        return jsonify({
            'success': True,
            'entity_set': metadata
        })
        
    except Exception as e:
        logger.error(f"è·å–å®ä½“é›†è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–è¯¦æƒ…å¤±è´¥: {str(e)}'}), 500

@app.route('/api/entity_sets/delete/<name>', methods=['DELETE'])
def delete_entity_set(name):
    """åˆ é™¤å®ä½“é›†"""
    try:
        entity_sets_dir = "evaluation_data/entity_sets"
        
        # åˆ é™¤CSVæ–‡ä»¶
        csv_path = os.path.join(entity_sets_dir, f"{name}.csv")
        if os.path.exists(csv_path):
            os.remove(csv_path)
        
        # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
        metadata_path = os.path.join(entity_sets_dir, f"{name}.json")
        if os.path.exists(metadata_path):
            os.remove(metadata_path)
        
        logger.info(f"å®ä½“é›† '{name}' åˆ é™¤æˆåŠŸ")
        
        return jsonify({
            'success': True,
            'message': 'å®ä½“é›†åˆ é™¤æˆåŠŸ'
        })
        
    except Exception as e:
        logger.error(f"åˆ é™¤å®ä½“é›†å¤±è´¥: {e}")
        return jsonify({'error': f'åˆ é™¤å¤±è´¥: {str(e)}'}), 500

@app.route('/api/data_management/directories')
def list_data_directories():
    """è·å–å¯ç”¨çš„æ•°æ®ç›®å½•åˆ—è¡¨"""
    try:
        directories = [
            {
                'path': 'evaluation_data/generated_datasets',
                'name': 'ç”Ÿæˆæ•°æ®é›†',
                'description': 'æ‰¹é‡ç”Ÿæˆçš„QAæ•°æ®é›†'
            },
            {
                'path': 'evaluation_data/final_datasets',
                'name': 'æœ€ç»ˆæ•°æ®é›†', 
                'description': 'æœ€ç»ˆç‰ˆæœ¬çš„æ•°æ®é›†'
            },
            {
                'path': 'evaluation_data/final_datasets/label_datasets',
                'name': 'æ ‡ç­¾æ•°æ®é›†',
                'description': 'å¸¦é¢†åŸŸæ ‡ç­¾çš„æ•°æ®é›†'
            }
        ]
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨å¹¶æ·»åŠ æ–‡ä»¶ç»Ÿè®¡
        available_dirs = []
        for dir_info in directories:
            if os.path.exists(dir_info['path']):
                # ç»Ÿè®¡JSONLæ–‡ä»¶æ•°é‡
                file_count = sum(1 for f in os.listdir(dir_info['path']) 
                               if f.endswith('.jsonl'))
                dir_info['file_count'] = file_count
                available_dirs.append(dir_info)
        
        return jsonify({
            'success': True,
            'directories': available_dirs
        })
        
    except Exception as e:
        logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/data_management/files')
def list_directory_files():
    """è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        directory = request.args.get('directory', '')
        if not directory:
            return jsonify({'success': False, 'error': 'è¯·æŒ‡å®šç›®å½•'}), 400
        
        if not os.path.exists(directory):
            return jsonify({'success': False, 'error': 'ç›®å½•ä¸å­˜åœ¨'}), 404
        
        files = []
        for filename in os.listdir(directory):
            if filename.endswith('.jsonl'):
                filepath = os.path.join(directory, filename)
                try:
                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    stat = os.stat(filepath)
                    modified_time = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è®¡ç®—è®°å½•æ•°
                    count = 0
                    with open(filepath, 'r', encoding='utf-8') as f:
                        count = sum(1 for line in f if line.strip())
                    
                    files.append({
                        'filename': filename,
                        'count': count,
                        'modified_time': modified_time,
                        'size': stat.st_size,
                        'directory': directory
                    })
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {filename}: {e}")
                    continue
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        return jsonify({
            'success': True,
            'files': files,
            'directory': directory
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/batch_generation/start', methods=['POST'])
def start_batch_generation():
    """å¼€å§‹æ‰¹é‡ç”Ÿæˆ"""
    from lib.trace_manager import start_trace
    
    # å¯åŠ¨trace
    trace_id = start_trace(prefix="batch")
    logger.info(f"æ¥æ”¶æ‰¹é‡ç”Ÿæˆè¯·æ±‚")
    
    if building_status['is_running']:
        return jsonify({'error': 'ç³»ç»Ÿæ­£åœ¨è¿è¡Œä¸­'}), 400
    
    data = request.json
    entity_set_name = data.get('entity_set', '')
    
    if not entity_set_name:
        return jsonify({'error': 'è¯·é€‰æ‹©å®ä½“é›†'}), 400
    
    # è¯»å–å®ä½“é›†
    try:
        entity_sets_dir = "evaluation_data/entity_sets"
        csv_path = os.path.join(entity_sets_dir, f"{entity_set_name}.csv")
        
        if not os.path.exists(csv_path):
            return jsonify({'error': 'å®ä½“é›†ä¸å­˜åœ¨'}), 404
        
        entities = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('entity', '').strip():
                    entities.append(row['entity'].strip())
        
        if not entities:
            return jsonify({'error': 'å®ä½“é›†ä¸ºç©º'}), 400
        
        # å¤„ç†æ–­ç‚¹ç»­ä¼ 
        resume_config = data.get('resume', {})
        if resume_config.get('enabled') and resume_config.get('filename'):
            resume_filename = resume_config['filename']
            resume_filepath = f"evaluation_data/generated_datasets/{resume_filename}"
            
            if os.path.exists(resume_filepath):
                # è¯»å–å·²å®Œæˆçš„å®ä½“
                completed_entities = set()
                try:
                    with open(resume_filepath, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                line_data = json.loads(line.strip())
                                entity = line_data.get('entity', '')
                                if entity:
                                    completed_entities.add(entity)
                    
                    # è¿‡æ»¤æ‰å·²å®Œæˆçš„å®ä½“
                    original_count = len(entities)
                    entities = [e for e in entities if e not in completed_entities]
                    skipped_count = original_count - len(entities)
                    
                    logger.info(f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped_count} ä¸ªå·²å®Œæˆçš„å®ä½“ï¼Œå‰©ä½™ {len(entities)} ä¸ª")
                    
                    # å¦‚æœå¯ç”¨äº†å³æ—¶ä¿å­˜ï¼Œä½¿ç”¨ç»­ä¼ æ–‡ä»¶ä½œä¸ºè¾“å‡ºæ–‡ä»¶
                    instant_save_config = data.get('instant_save', {})
                    if instant_save_config.get('enabled'):
                        instant_save_config['filename'] = resume_filename
                        data['instant_save'] = instant_save_config
                    
                except Exception as e:
                    logger.error(f"è¯»å–æ–­ç‚¹ç»­ä¼ æ–‡ä»¶å¤±è´¥: {e}")
                    return jsonify({'error': f'è¯»å–æ–­ç‚¹ç»­ä¼ æ–‡ä»¶å¤±è´¥: {str(e)}'}), 500
        
        if not entities:
            return jsonify({'error': 'æ‰€æœ‰å®ä½“éƒ½å·²å®Œæˆï¼Œæ— éœ€ç»§ç»­ç”Ÿæˆ'}), 400
        
        # å°†å®ä½“åˆ—è¡¨æ·»åŠ åˆ°é…ç½®ä¸­
        data['entities'] = entities
        data['count'] = len(entities)
        
        count = len(entities)
        logger.info(f"æ¥æ”¶åˆ°æ‰¹é‡ç”Ÿæˆè¯·æ±‚ï¼Œå®ä½“é›†: {entity_set_name}ï¼Œå®ä½“æ•°é‡: {count}")
        
        # åˆ›å»ºè¿è¡Œç®¡ç†å™¨
        run_manager = RunManager()
        batch_id = run_manager.create_new_run(f"batch_generation_{entity_set_name}_{count}")
        
        # é‡ç½®çŠ¶æ€
        building_status['is_running'] = True
        building_status['current_step'] = 'æ‰¹é‡ç”Ÿæˆåˆå§‹åŒ–'
        building_status['progress'] = 0
        building_status['run_id'] = batch_id
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ‰¹é‡ç”Ÿæˆ
        thread = threading.Thread(target=run_batch_generation_process, args=(data, run_manager))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'batch_id': batch_id, 
            'message': 'æ‰¹é‡ç”Ÿæˆå·²å¼€å§‹',
            'count': count
        })
        
    except Exception as e:
        logger.error(f"è¯»å–å®ä½“é›†å¤±è´¥: {e}")
        return jsonify({'error': f'è¯»å–å®ä½“é›†å¤±è´¥: {str(e)}'}), 500

@app.route('/api/batch_generation/stop', methods=['POST'])
def stop_batch_generation():
    """åœæ­¢æ‰¹é‡ç”Ÿæˆ"""
    building_status['is_running'] = False
    building_status['current_step'] = 'å·²åœæ­¢'
    return jsonify({'message': 'æ‰¹é‡ç”Ÿæˆå·²åœæ­¢'})

@app.route('/api/preview_entities', methods=['POST'])
def preview_entities():
    """é¢„è§ˆå®ä½“"""
    data = request.json
    source = data.get('source', 'wikidata')
    count = data.get('count', 10)
    category = data.get('category', '')
    
    # è¿™é‡Œå¯ä»¥å®ç°ä¸åŒæ•°æ®æºçš„é¢„è§ˆé€»è¾‘
    if source == 'wikidata':
        # æ¨¡æ‹ŸWikiDataå®ä½“
        sample_entities = [
            'é‡å­è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'åŸºå› ç¼–è¾‘', 'è„‘æœºæ¥å£', 'çº³ç±³ææ–™',
            'åŒºå—é“¾', 'è™šæ‹Ÿç°å®', 'å¢å¼ºç°å®', 'ç‰©è”ç½‘', '5Gé€šä¿¡',
            'å¤ªé˜³èƒ½ç”µæ± ', 'ç”µåŠ¨æ±½è½¦', 'è‡ªåŠ¨é©¾é©¶', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ '
        ]
        import random
        entities = random.sample(sample_entities, min(count, len(sample_entities)))
        return jsonify({'entities': entities})
    
    return jsonify({'entities': []})

# è¯„æµ‹æ•°æ®ç®¡ç†API
@app.route('/api/evaluation_data/list')
def list_evaluation_data():
    """åˆ—å‡ºæ‰€æœ‰è¯„æµ‹æ•°æ®é›†"""
    import os
    import json
    from datetime import datetime
    
    def scan_dataset_directory(directory):
        """æ‰«ææ•°æ®é›†ç›®å½•ï¼Œè¿”å›jsonlæ–‡ä»¶åˆ—è¡¨"""
        datasets = []
        if os.path.exists(directory):
            for filename in os.listdir(directory):
                if filename.endswith('.jsonl') and not filename.startswith('.'):
                    filepath = os.path.join(directory, filename)
                    try:
                        # è®¡ç®—æ–‡ä»¶ä¸­çš„è¡Œæ•°ï¼ˆQAå¯¹æ•°é‡ï¼‰
                        with open(filepath, 'r', encoding='utf-8') as f:
                            count = sum(1 for line in f if line.strip())
                        
                        # è·å–æ–‡ä»¶åˆ›å»ºæ—¶é—´
                        created_at = datetime.fromtimestamp(os.path.getctime(filepath)).strftime('%Y-%m-%d')
                        
                        # å»æ‰.jsonlåç¼€ä½œä¸ºå±•ç¤ºåç§°
                        display_name = filename.replace('.jsonl', '')
                        
                        datasets.append({
                            'id': filename,  # ä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºID
                            'name': display_name,  # å±•ç¤ºåç§°å»æ‰åç¼€
                            'count': count,
                            'created_at': created_at
                        })
                    except Exception as e:
                        logger.error(f"è¯»å–æ•°æ®é›†æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                        continue
        return datasets
    
    # æ‰«ææ ‡å‡†æ•°æ®é›†å’Œç”Ÿæˆæ•°æ®é›†ç›®å½•
    standard_datasets = scan_dataset_directory('evaluation_data/standard_datasets')
    generated_datasets = scan_dataset_directory('evaluation_data/generated_datasets')
    
    return jsonify({
        'standard': standard_datasets,
        'generated': generated_datasets
    })

@app.route('/api/evaluation_data/details/<dataset_id>')
def get_evaluation_data_details(dataset_id):
    """è·å–æ•°æ®é›†è¯¦æƒ…"""
    import os
    import json
    from datetime import datetime
    
    # ç¡®å®šæ•°æ®é›†æ–‡ä»¶è·¯å¾„
    standard_path = f'evaluation_data/standard_datasets/{dataset_id}'
    generated_path = f'evaluation_data/generated_datasets/{dataset_id}'
    
    filepath = None
    dataset_type = None
    
    if os.path.exists(standard_path):
        filepath = standard_path
        dataset_type = 'standard'
    elif os.path.exists(generated_path):
        filepath = generated_path
        dataset_type = 'generated'
    else:
        return jsonify({'error': 'Dataset not found'}), 404
    
    try:
        # è¯»å–jsonlæ–‡ä»¶
        qa_pairs = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        qa_pair = json.loads(line)
                        qa_pairs.append({
                            'line_num': line_num,
                            'question': qa_pair.get('question', ''),
                            'answer': qa_pair.get('answer', ''),
                            'metadata': {k: v for k, v in qa_pair.items() if k not in ['question', 'answer']}
                        })
                    except json.JSONDecodeError as e:
                        logger.error(f"è§£æç¬¬ {line_num} è¡ŒJSONå¤±è´¥: {e}")
                        continue
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_stats = os.stat(filepath)
        created_at = datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d %H:%M:%S')
        modified_at = datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        
        return jsonify({
            'id': dataset_id,
            'name': dataset_id.replace('.jsonl', ''),
            'type': dataset_type,
            'count': len(qa_pairs),
            'created_at': created_at,
            'modified_at': modified_at,
            'data': qa_pairs,
            'evaluation_history': []  # TODO: å®ç°è¯„æµ‹å†å²è®°å½•
        })
        
    except Exception as e:
        logger.error(f"è¯»å–æ•°æ®é›†è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({'error': 'Failed to read dataset'}), 500

@app.route('/api/evaluation_data/save', methods=['POST'])
def save_evaluation_data():
    """ä¿å­˜è¯„æµ‹æ•°æ®"""
    data = request.json
    name = data.get('name')
    data_type = data.get('type', 'generated')
    qa_data = data.get('data', [])
    metadata = data.get('metadata', {})
    
    # è¿™é‡Œåº”è¯¥ä¿å­˜åˆ°evaluation_dataç›®å½•
    import os
    import json
    
    # åˆ›å»ºç›®å½•
    dataset_dir = f"evaluation_data/{data_type}_datasets"
    os.makedirs(dataset_dir, exist_ok=True)
    
    # ä¿å­˜æ•°æ®ä¸ºjsonlæ ¼å¼
    filename = f"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    filepath = os.path.join(dataset_dir, filename)
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            # ä»¥jsonlæ ¼å¼ä¿å­˜ï¼Œæ¯è¡Œä¸€ä¸ªQAå¯¹
            for qa_item in qa_data:
                # å¤„ç†ä¸¤ç§æ•°æ®ç»“æ„ï¼š
                # 1. ç›´æ¥çš„QAå¯¹ï¼š{"question": "...", "answer": "..."}
                # 2. ç”Ÿæˆç»“æœå¯¹è±¡ï¼š{"qa_pair": {"question": "...", "answer": "..."}, "initial_entity": "..."}
                
                if 'qa_pair' in qa_item and qa_item['qa_pair']:
                    # æ¥è‡ªæ‰¹é‡ç”Ÿæˆçš„ç»“æœå¯¹è±¡
                    qa_pair = qa_item['qa_pair']
                    line_data = {
                        'question': qa_pair.get('question', ''),
                        'answer': qa_pair.get('answer', ''),
                        'entity': qa_item.get('initial_entity', ''),
                        'question_type': qa_pair.get('question_type', ''),
                        'complexity': qa_pair.get('complexity', ''),
                        'reasoning': qa_pair.get('reasoning', ''),  # ä¿æŒå…¼å®¹æ€§
                        'reasoning_path': qa_pair.get('reasoning_path', ''),  # æ–°å¢æ¨ç†è·¯å¾„å­—æ®µ
                        **qa_item.get('metadata', {})
                    }
                else:
                    # ç›´æ¥çš„QAå¯¹æ ¼å¼
                    line_data = {
                        'question': qa_item.get('question', ''),
                        'answer': qa_item.get('answer', ''),
                        **qa_item.get('metadata', {})
                    }
                
                f.write(json.dumps(line_data, ensure_ascii=False) + '\n')
        
        return jsonify({'success': True, 'filepath': filepath, 'filename': filename})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/evaluation_data/upload', methods=['POST'])
def upload_evaluation_data():
    """ä¸Šä¼ è¯„æµ‹æ•°æ®æ–‡ä»¶"""
    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶'})
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'success': False, 'error': 'æ–‡ä»¶åä¸ºç©º'})
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = file.read().decode('utf-8')
        
        # è§£æJSON
        data = json.loads(content)
        
        # ä¿å­˜æ–‡ä»¶
        import os
        upload_dir = "evaluation_data/uploaded_datasets"
        os.makedirs(upload_dir, exist_ok=True)
        
        filename = f"upload_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        filepath = os.path.join(upload_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return jsonify({'success': True, 'filepath': filepath})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# è¯„æµ‹æ‰§è¡ŒAPI
@app.route('/api/evaluation/start', methods=['POST'])
def start_evaluation():
    """å¼€å§‹è¯„æµ‹"""
    from lib.trace_manager import start_trace
    
    # å¯åŠ¨trace
    trace_id = start_trace(prefix="eval")
    
    data = request.json
    dataset_id = data.get('dataset_id')
    evaluator_type = data.get('evaluator_type', 'reasoning_model')
    model_name = data.get('model_name', 'gpt-4')
    
    logger.info(f"å¼€å§‹è¯„æµ‹ï¼Œæ•°æ®é›†: {dataset_id}, è¯„æµ‹å™¨: {evaluator_type}")
    
    # åˆ›å»ºè¯„æµ‹ä»»åŠ¡
    evaluation_id = f"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    total_tasks = 0
    try:
        # ç¡®å®šæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        standard_path = f'evaluation_data/standard_datasets/{dataset_id}'
        generated_path = f'evaluation_data/generated_datasets/{dataset_id}'
        
        dataset_path = None
        if os.path.exists(standard_path):
            dataset_path = standard_path
        elif os.path.exists(generated_path):
            dataset_path = generated_path
        
        if dataset_path:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                total_tasks = sum(1 for line in f if line.strip())
    except Exception as e:
        logger.error(f"è®¡ç®—ä»»åŠ¡æ€»æ•°å¤±è´¥: {e}")
        total_tasks = 0
    
    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œè¯„æµ‹
    thread = threading.Thread(target=run_evaluation_process, args=(evaluation_id, data))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'evaluation_id': evaluation_id, 
        'message': 'è¯„æµ‹å·²å¼€å§‹',
        'total_tasks': total_tasks
    })

@app.route('/api/evaluation/stop', methods=['POST'])
def stop_evaluation():
    """åœæ­¢è¯„æµ‹"""
    # è¿™é‡Œåº”è¯¥åœæ­¢æ­£åœ¨è¿è¡Œçš„è¯„æµ‹
    return jsonify({'message': 'è¯„æµ‹å·²åœæ­¢'})

# å¯¹æ¯”è¯„æµ‹ç›¸å…³API
@app.route('/api/comparison/start', methods=['POST'])
def start_comparison():
    """å¼€å§‹å¯¹æ¯”è¯„æµ‹"""
    from lib.trace_manager import start_trace
    
    # å¯åŠ¨trace
    trace_id = start_trace(prefix="comp")
    logger.info(f"æ¥æ”¶å¯¹æ¯”è¯„æµ‹è¯·æ±‚")
    
    if building_status['is_running']:
        return jsonify({'error': 'ç³»ç»Ÿæ­£åœ¨è¿è¡Œä¸­'}), 400
    
    data = request.json
    
    # éªŒè¯å¿…è¦å‚æ•°
    if not data.get('datasetA') or not data.get('datasetB'):
        return jsonify({'error': 'è¯·é€‰æ‹©ä¸¤ä¸ªæ•°æ®æ–‡ä»¶'}), 400
    
    if data.get('datasetA', {}).get('id') == data.get('datasetB', {}).get('id'):
        return jsonify({'error': 'ä¸èƒ½é€‰æ‹©ç›¸åŒçš„æ•°æ®æ–‡ä»¶'}), 400
    
    try:
        # åˆ›å»ºå¯¹æ¯”è¯„æµ‹ID
        comparison_id = f"comp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # é‡ç½®çŠ¶æ€
        building_status['is_running'] = True
        building_status['current_step'] = 'å¯¹æ¯”è¯„æµ‹åˆå§‹åŒ–'
        building_status['progress'] = 0
        building_status['run_id'] = comparison_id
        
        logger.info(f"å¼€å§‹å¯¹æ¯”è¯„æµ‹: {data.get('datasetA', {}).get('name')} vs {data.get('datasetB', {}).get('name')}")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¯¹æ¯”è¯„æµ‹
        thread = threading.Thread(target=run_comparison_process, args=(comparison_id, data))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'comparison_id': comparison_id,
            'message': 'å¯¹æ¯”è¯„æµ‹å·²å¼€å§‹'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å¯¹æ¯”è¯„æµ‹å¤±è´¥: {e}")
        building_status['is_running'] = False
        return jsonify({'error': f'å¯åŠ¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/comparison/stop', methods=['POST'])
def stop_comparison():
    """åœæ­¢å¯¹æ¯”è¯„æµ‹"""
    building_status['is_running'] = False
    building_status['current_step'] = 'å·²åœæ­¢'
    return jsonify({'message': 'å¯¹æ¯”è¯„æµ‹å·²åœæ­¢'})

@app.route('/api/comparison/history')
def get_comparison_history():
    """è·å–å¯¹æ¯”è¯„æµ‹å†å²è®°å½•"""
    try:
        from lib.comparison_evaluator import ComparisonEvaluator
        evaluator = ComparisonEvaluator()
        history = evaluator.get_comparison_history()
        
        return jsonify({
            'success': True,
            'history': history
        })
        
    except Exception as e:
        logger.error(f"è·å–å¯¹æ¯”å†å²å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/comparison/history/<comparison_id>')
def get_comparison_details(comparison_id):
    """è·å–å¯¹æ¯”è¯„æµ‹è¯¦ç»†ç»“æœ"""
    try:
        from lib.comparison_evaluator import ComparisonEvaluator
        evaluator = ComparisonEvaluator()
        details = evaluator.get_comparison_details(comparison_id)
        
        if details is None:
            return jsonify({'error': 'æœªæ‰¾åˆ°å¯¹æ¯”è®°å½•'}), 404
        
        return jsonify(details)
        
    except Exception as e:
        logger.error(f"è·å–å¯¹æ¯”è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

def run_building_process(entity, max_nodes, sample_size, run_manager, max_iterations=3, sampling_algorithm='mixed'):
    """è¿è¡Œæ„å»ºè¿‡ç¨‹"""
    from lib.trace_manager import TraceManager, start_trace
    
    # åœ¨æ–°çº¿ç¨‹ä¸­éœ€è¦é‡æ–°è®¾ç½®traceï¼ˆä½¿ç”¨building_statusä¸­çš„run_idï¼‰
    if building_status.get('run_id'):
        # åˆ›å»ºåŸºäºrun_idçš„trace
        trace_id = f"build_{building_status['run_id']}"
        start_trace(trace_id)
        logger.info(f"æ„å»ºçº¿ç¨‹å¯åŠ¨ï¼Œtrace_id: {trace_id}")
    else:
        start_trace(prefix="build")
        logger.info(f"æ„å»ºçº¿ç¨‹å¯åŠ¨ï¼Œåˆ›å»ºæ–°trace")
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        logger.info(f"å¼€å§‹å¼‚æ­¥æ„å»ºè¿‡ç¨‹: å®ä½“={entity}, æœ€å¤§èŠ‚ç‚¹={max_nodes}, é‡‡æ ·å¤§å°={sample_size}")
        result = loop.run_until_complete(async_building_process(entity, max_nodes, sample_size, run_manager, max_iterations, sampling_algorithm))
        logger.info(f"å¼‚æ­¥æ„å»ºè¿‡ç¨‹å®Œæˆ")
        
        # ä¿å­˜ç»“æœ
        run_manager.save_result(result, "knowledge_graph_result.json")
        
        # æå–QAç»“æœ
        qa_pair = result.get('qa_pair', {})
        building_status['qa_result'] = qa_pair
        
        # å‘é€å®Œæˆäº‹ä»¶ï¼ŒåŒ…å«QAç»“æœ
        socketio.emit('building_complete', {
            'success': True,
            'result': result,
            'qa_result': qa_pair,
            'run_id': building_status['run_id'],
            'message': 'çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ'
        })
        
        # æ ‡è®°è¿è¡Œå®Œæˆ
        run_manager.complete_run(success=True)
        
    except Exception as e:
        logger.error(f"æ„å»ºè¿‡ç¨‹å‡ºé”™: {e}")
        
        # æ ‡è®°è¿è¡Œå¤±è´¥
        run_manager.complete_run(success=False, error_message=str(e))
        
        socketio.emit('building_complete', {
            'success': False,
            'error': str(e),
            'run_id': building_status['run_id'],
            'message': 'æ„å»ºè¿‡ç¨‹å‡ºé”™'
        })
    finally:
        building_status['is_running'] = False
        building_status['current_step'] = 'å®Œæˆ'
        # æ¸…ç†trace
        from lib.trace_manager import end_trace
        end_trace()

async def async_building_process(entity, max_nodes, sample_size, run_manager, max_iterations=3, sampling_algorithm='mixed', progress_callback=None):
    """å¼‚æ­¥æ„å»ºè¿‡ç¨‹"""
    try:
        # è·å–è¿è¡Œè·¯å¾„
        run_paths = run_manager.get_run_paths()
        
        # åˆ›å»ºè‡ªå®šä¹‰è®¾ç½®å®ä¾‹ï¼Œä½¿ç”¨è¿è¡Œç‰¹å®šçš„è·¯å¾„
        from config import Settings
        custom_settings = Settings()
        custom_settings.GRAPHRAG_ROOT_DIR = run_paths['graphrag_root']
        custom_settings.GRAPHRAG_INPUT_DIR = run_paths['graphrag_input']
        custom_settings.GRAPHRAG_OUTPUT_DIR = run_paths['graphrag_output']
        custom_settings.GRAPHRAG_CACHE_DIR = run_paths['graphrag_cache']
        
        # æ›´æ–°è‡ªå®šä¹‰å‚æ•°
        custom_settings.MAX_NODES = max_nodes
        custom_settings.SAMPLE_SIZE = sample_size
        
        # åˆ›å»ºå›¾æ›´æ–°å›è°ƒå‡½æ•°
        def graph_update_callback(graph_data):
            """å®æ—¶å›¾æ›´æ–°å›è°ƒ"""
            try:
                socketio.emit('graph_update', graph_data)
                logger.info(f"å‘é€å®æ—¶å›¾æ›´æ–°: {len(graph_data.get('nodes', []))} ä¸ªèŠ‚ç‚¹, {len(graph_data.get('links', []))} ä¸ªå…³ç³»")
            except Exception as e:
                logger.error(f"å‘é€å®æ—¶å›¾æ›´æ–°å¤±è´¥: {e}")
        
        # åˆ›å»ºGraphRagBuilderï¼Œä¼ å…¥è‡ªå®šä¹‰è®¾ç½®å’Œå›¾æ›´æ–°å›è°ƒ
        builder = GraphRagBuilder(custom_settings, graph_update_callback)
        
        def pipeline_progress_callback(step, progress):
            """æµæ°´çº¿è¿›åº¦å›è°ƒ"""
            if progress_callback:
                progress_callback(step, progress)
            else:
                # å¦‚æœæ²¡æœ‰æµæ°´çº¿å›è°ƒï¼Œä½¿ç”¨é»˜è®¤çš„è¿›åº¦æ›´æ–°
                update_progress(step, progress)
        
        result = await builder.build_knowledge_graph(
            entity, 
            pipeline_progress_callback, 
            max_iterations,
            sampling_algorithm=sampling_algorithm
        )
        
        # åªå‘é€æ˜Ÿåº§å›¾é‡‡æ ·ä¿¡æ¯ï¼Œé¿å…é‡å¤å‘é€åŸºç¡€å›¾æ•°æ®
        if 'sample_info' in result:
            try:
                sample_info = result['sample_info']
                sampled_nodes = sample_info.get('nodes', [])
                sampled_relations = sample_info.get('relations', [])
                
                # åˆ›å»ºé‡‡æ ·èŠ‚ç‚¹æ•°æ®
                sampled_graph_nodes = []
                for node in sampled_nodes:
                    sampled_graph_nodes.append({
                        'id': node.get('name', f'Node_{len(sampled_graph_nodes)}'),
                        'name': node.get('name', f'Node_{len(sampled_graph_nodes)}'),
                        'type': node.get('type', 'unknown'),
                        'description': node.get('description', ''),
                        'group': hash(node.get('type', 'unknown')) % 10,
                        'sampled': True
                    })
                
                # åˆ›å»ºé‡‡æ ·è¿çº¿æ•°æ®
                sampled_graph_links = []
                for relation in sampled_relations:
                    source = relation.get('source') or relation.get('head') or relation.get('from')
                    target = relation.get('target') or relation.get('tail') or relation.get('to')
                    if source and target:
                        relation_type = (relation.get('relationship') or 
                                       relation.get('relation') or 
                                       relation.get('type') or 
                                       'related_to')
                        sampled_graph_links.append({
                            'source': source,
                            'target': target,
                            'relation': relation_type,
                            'description': relation.get('description', ''),
                            'weight': relation.get('weight', 1.0),
                            'sampled': True
                        })
                
                # å‘é€æ˜Ÿåº§å›¾é«˜äº®äº‹ä»¶
                socketio.emit('sampled_graph_update', {
                    'nodes': sampled_graph_nodes,
                    'links': sampled_graph_links
                })
                logger.info(f"å‘é€æ˜Ÿåº§å›¾æ›´æ–°: {len(sampled_graph_nodes)} ä¸ªé‡‡æ ·èŠ‚ç‚¹, {len(sampled_graph_links)} ä¸ªé‡‡æ ·å…³ç³»")
                
            except Exception as e:
                logger.error(f"å‘é€æ˜Ÿåº§å›¾æ›´æ–°å¤±è´¥: {e}")
        
        # å‘é€QAç»“æœ
        if 'qa_pair' in result:
            socketio.emit('qa_generated', result['qa_pair'])
        
        # è¾“å‡ºQAå¯¹åˆ°æ§åˆ¶å°å’Œæ—¥å¿—
        qa_pair = result.get('qa_pair', {})
        if qa_pair:
            logger.info("\n" + "="*60)
            logger.info("ğŸ¯ ç”Ÿæˆçš„QAå¯¹:")
            logger.info("="*60)
            logger.info(f"é—®é¢˜ç±»å‹: {qa_pair.get('question_type', 'N/A')}")
            logger.info(f"å¤æ‚åº¦: {qa_pair.get('complexity', 'N/A')}")
            logger.info(f"\nâ“ é—®é¢˜:")
            logger.info(qa_pair.get('question', 'N/A'))
            logger.info(f"\nâœ… ç­”æ¡ˆ:")
            logger.info(qa_pair.get('answer', 'N/A'))
            if qa_pair.get('reasoning_path'):
                logger.info(f"\nğŸ§  æ¨ç†è·¯å¾„:")
                logger.info(qa_pair.get('reasoning_path'))
            elif qa_pair.get('reasoning'):
                logger.info(f"\nğŸ§  æ¨ç†è¿‡ç¨‹:")
                logger.info(qa_pair.get('reasoning'))
            logger.info("="*60)
        
        return result
        
    except Exception as e:
        logger.error(f"å¼‚æ­¥æ„å»ºè¿‡ç¨‹å‡ºé”™: {e}")
        raise

def update_progress(step, progress):
    """æ›´æ–°è¿›åº¦"""
    building_status['current_step'] = step
    building_status['progress'] = progress
    
    socketio.emit('progress_update', {
        'step': step,
        'progress': progress
    })
    logger.info(f"è¿›åº¦æ›´æ–°: {step} ({progress}%)")

def update_graph_data(result):
    """æ›´æ–°å›¾æ•°æ®"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å›¾ä¿¡æ¯ï¼ˆè¿­ä»£è¿‡ç¨‹ä¸­çš„å¢é‡æ›´æ–°ï¼‰
        if 'graph_info' in result:
            graph_info = result['graph_info']
            entities = graph_info.get('entities', [])
            relationships = graph_info.get('relationships', [])
            
            graph_nodes = []
            for entity in entities:
                graph_nodes.append({
                    'id': entity.get('name', entity.get('id', f'Entity_{len(graph_nodes)}')),
                    'name': entity.get('name', entity.get('id', f'Entity_{len(graph_nodes)}')),
                    'type': entity.get('type', 'concept'),
                    'description': entity.get('description', ''),
                    'group': hash(entity.get('type', 'concept')) % 10
                })
            
            graph_links = []
            for relation in relationships:
                source = relation.get('source') or relation.get('head') or relation.get('from')
                target = relation.get('target') or relation.get('tail') or relation.get('to')
                if source and target:
                    # æ­£ç¡®è·å–å…³ç³»ç±»å‹ï¼Œä¼˜å…ˆä½¿ç”¨ relationship å­—æ®µ
                    relation_type = (relation.get('relationship') or 
                                   relation.get('relation') or 
                                   relation.get('type') or 
                                   'related_to')
                    graph_links.append({
                        'source': source,
                        'target': target,
                        'relation': relation_type,
                        'description': relation.get('description', ''),
                        'weight': relation.get('weight', 1.0)
                    })
            
            building_status['graph_data'] = {
                'nodes': graph_nodes,
                'links': graph_links
            }
            
            socketio.emit('graph_update', building_status['graph_data'])
            
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡‡æ ·ä¿¡æ¯ï¼ˆæ˜Ÿåº§å›¾é«˜äº®ï¼‰
        if 'sample_info' in result:
            sample_info = result['sample_info']
            sampled_nodes = sample_info.get('nodes', [])
            sampled_relations = sample_info.get('relations', [])
            
            # åˆ›å»ºé‡‡æ ·èŠ‚ç‚¹æ•°æ®
            sampled_graph_nodes = []
            for node in sampled_nodes:
                sampled_graph_nodes.append({
                    'id': node.get('name', f'Node_{len(sampled_graph_nodes)}'),
                    'name': node.get('name', f'Node_{len(sampled_graph_nodes)}'),
                    'type': node.get('type', 'unknown'),
                    'description': node.get('description', ''),
                    'group': hash(node.get('type', 'unknown')) % 10,
                    'sampled': True
                })
            
            # åˆ›å»ºé‡‡æ ·è¿çº¿æ•°æ®
            sampled_graph_links = []
            for relation in sampled_relations:
                source = relation.get('source') or relation.get('head') or relation.get('from')
                target = relation.get('target') or relation.get('tail') or relation.get('to')
                if source and target:
                    relation_type = (relation.get('relationship') or 
                                   relation.get('relation') or 
                                   relation.get('type') or 
                                   'related_to')
                    sampled_graph_links.append({
                        'source': source,
                        'target': target,
                        'relation': relation_type,
                        'description': relation.get('description', ''),
                        'weight': relation.get('weight', 1.0),
                        'sampled': True
                    })
            
            # å‘é€æ˜Ÿåº§å›¾é«˜äº®äº‹ä»¶
            socketio.emit('sampled_graph_update', {
                'nodes': sampled_graph_nodes,
                'links': sampled_graph_links
            })
        
    except Exception as e:
        logger.error(f"æ›´æ–°å›¾æ•°æ®å¤±è´¥: {e}")

@socketio.on('connect')
def handle_connect():
    """å¤„ç†å®¢æˆ·ç«¯è¿æ¥"""
    logger.info("å®¢æˆ·ç«¯å·²è¿æ¥")
    emit('connected', {'message': 'è¿æ¥æˆåŠŸ'})

@socketio.on('disconnect')
def handle_disconnect():
    """å¤„ç†å®¢æˆ·ç«¯æ–­å¼€è¿æ¥"""
    logger.info("å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥")

def run_batch_building_process(entities, run_manager):
    """è¿è¡Œæ‰¹é‡æ„å»ºè¿‡ç¨‹"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        results = []
        total_entities = len(entities)
        
        for i, entity in enumerate(entities):
            logger.info(f"æ‰¹é‡æ„å»º ({i+1}/{total_entities}): {entity}")
            
            # æ›´æ–°è¿›åº¦
            progress = int((i / total_entities) * 100)
            update_progress(f"å¤„ç†å®ä½“: {entity}", progress)
            
            # è¿è¡Œå•ä¸ªå®ä½“çš„æ„å»º
            result = loop.run_until_complete(async_building_process(entity, 30, 8, run_manager, 3))
            results.append({
                'entity': entity,
                'result': result
            })
            
            # æ›´æ–°å›¾æ•°æ®
            update_graph_data(result)
            
            # ä¿å­˜å•ä¸ªå®ä½“çš„ç»“æœ
            run_manager.save_result(result, f"entity_{entity}_result.json")
        
        # æ‰¹é‡æ„å»ºå®Œæˆåï¼Œä¿å­˜æ‰€æœ‰ç»“æœ
        run_manager.save_result(results, "batch_knowledge_graph_results.json")
        
        socketio.emit('batch_building_complete', {
            'success': True,
            'results': results,
            'message': f'æ‰¹é‡æ„å»ºå®Œæˆï¼Œå…±å¤„ç† {total_entities} ä¸ªå®ä½“'
        })
        
        # æ ‡è®°è¿è¡Œå®Œæˆ
        run_manager.complete_run(success=True)
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ„å»ºè¿‡ç¨‹å‡ºé”™: {e}")
        
        # æ ‡è®°è¿è¡Œå¤±è´¥
        run_manager.complete_run(success=False, error_message=str(e))
        
        socketio.emit('batch_building_complete', {
            'success': False,
            'error': str(e),
            'message': 'æ‰¹é‡æ„å»ºè¿‡ç¨‹å‡ºé”™'
        })
    finally:
        building_status['is_running'] = False
        building_status['current_step'] = 'æ‰¹é‡å®Œæˆ'

def instant_save_result(result, config):
    """å³æ—¶ä¿å­˜å•ä¸ªç»“æœåˆ°æ–‡ä»¶"""
    try:
        instant_save_config = config.get('instant_save', {})
        if not instant_save_config.get('enabled'):
            return
        
        filename = instant_save_config.get('filename')
        if not filename:
            # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"batch_generated_{timestamp}.jsonl"
        
        # ç¡®ä¿æ–‡ä»¶åä»¥.jsonlç»“å°¾
        if not filename.endswith('.jsonl'):
            filename += '.jsonl'
        
        # åˆ›å»ºç”Ÿæˆæ•°æ®é›†ç›®å½•
        dataset_dir = "evaluation_data/generated_datasets"
        os.makedirs(dataset_dir, exist_ok=True)
        
        filepath = os.path.join(dataset_dir, filename)
        
        # åªä¿å­˜æˆåŠŸç”Ÿæˆçš„QAå¯¹
        qa_pair = result.get('qa_pair', {})
        if qa_pair and qa_pair.get('question') and qa_pair.get('answer'):
            # æ„å»ºjsonlæ ¼å¼çš„æ•°æ®
            line_data = {
                'question': qa_pair.get('question', ''),
                'answer': qa_pair.get('answer', ''),
                'question_type': qa_pair.get('question_type', ''),
                'complexity': qa_pair.get('complexity', ''),
                'reasoning': qa_pair.get('reasoning', ''),  # ä¿æŒå…¼å®¹æ€§
                'reasoning_path': qa_pair.get('reasoning_path', ''),  # æ–°å¢æ¨ç†è·¯å¾„å­—æ®µ
                'entity': result.get('initial_entity', ''),
                'generated_at': datetime.now().isoformat()
            }
            
            # è¿½åŠ å†™å…¥æ–‡ä»¶
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(line_data, ensure_ascii=False) + '\n')
                f.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
            
            logger.info(f"å³æ—¶ä¿å­˜ç»“æœåˆ°: {filepath}")
        
    except Exception as e:
        logger.error(f"å³æ—¶ä¿å­˜ç»“æœå¤±è´¥: {e}")

def run_batch_generation_process(config, run_manager):
    """æ‰¹é‡ç”Ÿæˆè¿‡ç¨‹ - ä½¿ç”¨å¹¶è¡Œå¤„ç†"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        entities = config.get('entities', [])
        parallel_workers = config.get('parallel_workers', 2)
        
        total_count = len(entities)
        results = []
        
        logger.info(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆï¼Œå…± {total_count} ä¸ªå®ä½“ï¼Œå¹¶è¡Œworkeræ•°: {parallel_workers}")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å³æ—¶ä¿å­˜
        instant_save_enabled = config.get('instant_save', {}).get('enabled', False)
        if instant_save_enabled:
            logger.info("å¯ç”¨å³æ—¶ä¿å­˜æ¨¡å¼")
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æ‰¹é‡ç”Ÿæˆ
        result = loop.run_until_complete(async_batch_generation_process(
            entities, config, run_manager, parallel_workers
        ))
        
        results = result
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨å³æ—¶ä¿å­˜ï¼Œåˆ™åœ¨æœ€åç»Ÿä¸€ä¿å­˜
        if not instant_save_enabled and results:
            import os
            import json
            from datetime import datetime
            
            # åˆ›å»ºç”Ÿæˆæ•°æ®é›†ç›®å½•
            dataset_dir = "evaluation_data/generated_datasets"
            os.makedirs(dataset_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"batch_generated_{timestamp}.jsonl"
            filepath = os.path.join(dataset_dir, filename)
            
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    for result in results:
                        qa_pair = result.get('qa_pair', {})
                        if qa_pair.get('question') and qa_pair.get('answer'):
                            # æ„å»ºjsonlæ ¼å¼çš„æ•°æ®
                            line_data = {
                                'question': qa_pair.get('question', ''),
                                'answer': qa_pair.get('answer', ''),
                                'question_type': qa_pair.get('question_type', ''),
                                'complexity': qa_pair.get('complexity', ''),
                                'reasoning': qa_pair.get('reasoning', ''),  # ä¿æŒå…¼å®¹æ€§
                                'reasoning_path': qa_pair.get('reasoning_path', ''),  # æ–°å¢æ¨ç†è·¯å¾„å­—æ®µ
                                'entity': result.get('initial_entity', ''),
                                'generated_at': datetime.now().isoformat()
                            }
                            f.write(json.dumps(line_data, ensure_ascii=False) + '\n')
                
                logger.info(f"æ‰¹é‡ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ°: {filepath}")
                
                # å‘é€å®Œæˆä¿¡å·ï¼ˆåŒ…å«ä¿å­˜çš„æ–‡ä»¶ä¿¡æ¯ï¼‰
                socketio.emit('batch_complete', {
                    'total': len(results),
                    'message': f'æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªQAå¯¹',
                    'saved_file': filename,
                    'saved_path': filepath
                })
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ‰¹é‡ç”Ÿæˆç»“æœå¤±è´¥: {e}")
                # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œä¹Ÿå‘é€å®Œæˆä¿¡å·
                socketio.emit('batch_complete', {
                    'total': len(results),
                    'message': f'æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªQAå¯¹ï¼ˆä¿å­˜å¤±è´¥ï¼‰'
                })
        else:
            # å³æ—¶ä¿å­˜æ¨¡å¼ä¸‹çš„å®Œæˆä¿¡å·
            instant_save_config = config.get('instant_save', {})
            filename = instant_save_config.get('filename', 'unknown')
            socketio.emit('batch_complete', {
                'total': len(results),
                'message': f'æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªQAå¯¹ï¼ˆå³æ—¶ä¿å­˜ï¼‰',
                'saved_file': filename,
                'instant_save': True
            })
        
        building_status['is_running'] = False
        building_status['current_step'] = 'æ‰¹é‡ç”Ÿæˆå®Œæˆ'
        building_status['progress'] = 100
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        socketio.emit('batch_error', {'message': f'æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}'})
        building_status['is_running'] = False

@app.route('/api/evaluation_data/results')
def get_evaluation_results():
    """è·å–è¯„æµ‹ç»“æœæ±‡æ€»"""
    mode = request.args.get('mode', 'R1-0528')
    
    # æ‰«æevaluation_resultsç›®å½•
    results_dir = 'evaluation_data/evaluation_results'
    dataset_results = {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªæ•°æ®é›†çš„æœ€æ–°ç»“æœ
    
    if os.path.exists(results_dir):
        for filename in os.listdir(results_dir):
            if filename.endswith('.json'):
                try:
                    filepath = os.path.join(results_dir, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    if result_data.get('mode') == mode:
                        dataset_name = result_data.get('dataset_name', '')
                        submitted_at = result_data.get('submitted_at', '')
                        
                        # ä¿ç•™æœ€æ–°çš„è¯„æµ‹ç»“æœ
                        if dataset_name not in dataset_results or submitted_at > dataset_results[dataset_name]['submitted_at']:
                            dataset_results[dataset_name] = {
                                'name': dataset_name,
                                'count': result_data.get('total_questions', 0),
                                'accuracy': result_data.get('accuracy', 0) * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                                'last_evaluation': result_data.get('timestamp', '').split('_')[0] if result_data.get('timestamp') else '',
                                'submitted_at': submitted_at,
                                'evaluation_id': result_data.get('evaluation_id', ''),
                                'status': 'completed',
                                'correct_count': result_data.get('correct_answers', 0)  # ä¿®å¤å­—æ®µåä¸åŒ¹é…é—®é¢˜
                            }
                except Exception as e:
                    logger.error(f"è¯»å–è¯„æµ‹ç»“æœæ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    continue
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æäº¤æ—¶é—´æ’åº
    results = list(dataset_results.values())
    results.sort(key=lambda x: x['submitted_at'], reverse=True)
    
    return jsonify({'results': results})

@app.route('/api/evaluation_data/history/<dataset_id>')
def get_evaluation_history(dataset_id):
    """è·å–æ•°æ®é›†çš„è¯„æµ‹å†å²"""
    results_dir = 'evaluation_data/evaluation_results'
    history = []
    
    if os.path.exists(results_dir):
        for filename in os.listdir(results_dir):
            if filename.endswith('.json') and dataset_id.replace('.jsonl', '') in filename:
                try:
                    filepath = os.path.join(results_dir, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    history.append({
                        'mode': result_data.get('mode', 'R1-0528'),
                        'completed_at': result_data.get('submitted_at', '').replace('T', ' ').split('.')[0] if result_data.get('submitted_at') else '',
                        'total_questions': result_data.get('total_questions', 0),
                        'correct_count': result_data.get('correct_answers', 0),  # ä¿®å¤å­—æ®µåä¸åŒ¹é…é—®é¢˜
                        'accuracy': round(result_data.get('accuracy', 0) * 100, 1)  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å¹¶ä¿ç•™1ä½å°æ•°
                    })
                except Exception as e:
                    logger.error(f"è¯»å–å†å²è®°å½•æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    continue
    
    # æŒ‰å®Œæˆæ—¶é—´å€’åºæ’åˆ—
    history.sort(key=lambda x: x['completed_at'], reverse=True)
    
    return jsonify({'history': history})

def run_evaluation_process(evaluation_id, config):
    """è¯„æµ‹è¿‡ç¨‹"""
    from lib.trace_manager import start_trace
    
    # åœ¨æ–°çº¿ç¨‹ä¸­é‡æ–°è®¾ç½®trace
    trace_id = f"eval_{evaluation_id}"
    start_trace(trace_id)
    logger.info(f"è¯„æµ‹çº¿ç¨‹å¯åŠ¨ï¼Œevaluation_id: {evaluation_id}")
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        logger.info(f"å¼€å§‹å¼‚æ­¥è¯„æµ‹è¿‡ç¨‹")
        result = loop.run_until_complete(async_evaluation_process(evaluation_id, config))
        logger.info(f"å¼‚æ­¥è¯„æµ‹è¿‡ç¨‹å®Œæˆ")
        
        socketio.emit('evaluation_complete', {
            'evaluation_id': evaluation_id,
            'results': result
        })
        
    except Exception as e:
        logger.error(f"è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        socketio.emit('evaluation_error', {'message': f'è¯„æµ‹å¤±è´¥: {str(e)}'})
    finally:
        # æ¸…ç†trace
        from lib.trace_manager import end_trace
        end_trace()

def run_comparison_process(comparison_id, config):
    """å¯¹æ¯”è¯„æµ‹è¿‡ç¨‹"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(async_comparison_process(comparison_id, config))
        
        socketio.emit('comparison_complete', {
            'comparison_id': comparison_id,
            'results': result
        })
        
        # æ ‡è®°è¿è¡Œå®Œæˆ
        building_status['is_running'] = False
        building_status['current_step'] = 'å¯¹æ¯”è¯„æµ‹å®Œæˆ'
        
    except Exception as e:
        logger.error(f"å¯¹æ¯”è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        socketio.emit('comparison_error', {'message': f'å¯¹æ¯”è¯„æµ‹å¤±è´¥: {str(e)}'})
        building_status['is_running'] = False
    finally:
        # æ¸…ç†trace
        from lib.trace_manager import end_trace
        end_trace()

async def _process_single_generation_task(task_item: dict, config: dict, parent_run_manager, progress_callback=None):
    """å¤„ç†å•ä¸ªç”Ÿæˆä»»åŠ¡çš„å®Œæ•´æµæ°´çº¿ï¼šå›¾æ„å»º â†’ å›¾é‡‡æ · â†’ ä¿¡æ¯æ¨¡ç³ŠåŒ– â†’ QAç”Ÿæˆ"""
    from lib.trace_manager import start_trace, TraceManager
    
    index = task_item["index"]
    entity = task_item["entity"]
    task_id = task_item["task_id"]
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„trace
    batch_trace_id = TraceManager.get_trace_id()
    if batch_trace_id:
        item_trace_id = TraceManager.create_batch_trace_id(batch_trace_id, index)
        start_trace(item_trace_id)
    else:
        # å¦‚æœæ²¡æœ‰batch traceï¼Œåˆ›å»ºç‹¬ç«‹çš„trace
        start_trace(prefix=f"task_{index}")
    
    try:
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„è¿è¡Œç®¡ç†å™¨
        from lib.run_manager import RunManager
        task_run_manager = RunManager()
        task_run_id = task_run_manager.create_new_run(f"task_{index}_{entity}")
        
        # é˜¶æ®µ1ï¼šå›¾æ„å»º
        if progress_callback:
            progress_callback(
                f"ç¬¬{index}é¢˜: æ„å»ºçŸ¥è¯†å›¾è°±ä¸­...", 
                None,
                task_id=task_id,
                status="running"
            )
        
        logger.info(f"ç¬¬{index}é¢˜: å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±")
        
        # è¿è¡Œå›¾æ„å»ºè¿‡ç¨‹
        result = await async_building_process(
            entity, 
            config.get('max_nodes', 30), 
            config.get('sample_size', 8), 
            task_run_manager,  # ä½¿ç”¨ç‹¬ç«‹çš„è¿è¡Œç®¡ç†å™¨
            config.get('max_iterations', 3),
            config.get('sampling_algorithm', 'mixed'),
            progress_callback=lambda step, prog: progress_callback(
                f"ç¬¬{index}é¢˜: {step}", 
                None,
                task_id=task_id,
                status="running"
            ) if progress_callback else None
        )
        
        if not result:
            raise ValueError("å›¾æ„å»ºå¤±è´¥")
            
        logger.info(f"ç¬¬{index}é¢˜: çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
        
        # é˜¶æ®µ2ï¼šç»“æœéªŒè¯
        if progress_callback:
            progress_callback(
                f"ç¬¬{index}é¢˜: éªŒè¯ç»“æœå®Œæ•´æ€§...", 
                None,
                task_id=task_id,
                status="running"
            )
        
        # éªŒè¯ç»“æœå®Œæ•´æ€§
        if not result.get('qa_pair') or not result.get('graph_info'):
            raise ValueError("ç”Ÿæˆç»“æœä¸å®Œæ•´")
            
        logger.info(f"ç¬¬{index}é¢˜: ç”Ÿæˆä»»åŠ¡å®Œæˆ")
        
        # å°†ç»“æœä¿å­˜åˆ°çˆ¶è¿è¡Œç®¡ç†å™¨ä¸­
        parent_run_manager.save_result(result, f"task_{index}_{entity}_result.json")
        
        # æ ‡è®°ä»»åŠ¡è¿è¡Œå®Œæˆ
        task_run_manager.complete_run(success=True)
        
        return result
        
    except Exception as e:
        logger.error(f"ç¬¬{index}é¢˜: ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
        if progress_callback:
            progress_callback(
                f"ç¬¬{index}é¢˜: ç”Ÿæˆå¤±è´¥ - {str(e)}", 
                None,
                task_id=task_id,
                status="error"
            )
        
        # å¦‚æœåˆ›å»ºäº†ä»»åŠ¡è¿è¡Œç®¡ç†å™¨ï¼Œæ ‡è®°ä¸ºå¤±è´¥
        if 'task_run_manager' in locals():
            task_run_manager.complete_run(success=False, error_message=str(e))
        
        # è¿”å›é”™è¯¯ç»“æœ
        return {
            "initial_entity": entity,
            "qa_pair": None,
            "graph_info": {"node_count": 0, "relationship_count": 0},
            "error": str(e)
        }
    finally:
        # æ¸…ç†trace
        from lib.trace_manager import end_trace
        end_trace()

async def async_batch_generation_process(entities, config, run_manager, parallel_workers):
    """å¼‚æ­¥æ‰¹é‡ç”Ÿæˆè¿‡ç¨‹ - ä½¿ç”¨æµæ°´çº¿å¹¶å‘æ¨¡å¼"""
    try:
        total_tasks = len(entities)
        results = [None] * total_tasks  # é¢„åˆ†é…ç»“æœæ•°ç»„
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        task_queue = asyncio.Queue()
        
        # å®Œæˆè®¡æ•°å™¨å’Œé”
        completed_count = 0
        progress_lock = asyncio.Lock()
        
        # å°†æ‰€æœ‰ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        for i, entity in enumerate(entities):
            task_item = {
                "index": i + 1,
                "array_index": i,
                "entity": entity,
                "task_id": f"task_{i + 1}"
            }
            await task_queue.put(task_item)
        
        # æ·»åŠ ç»“æŸä¿¡å·
        for _ in range(parallel_workers):
            await task_queue.put(None)
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
        semaphore = asyncio.Semaphore(parallel_workers)
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(message, progress, task_id=None, status=None):
            progress_data = {
                'step': message,
                'progress': progress
            }
            
            if task_id:
                progress_data['task_id'] = task_id
            if message:
                progress_data['message'] = message
            if status:
                progress_data['status'] = status
                
            socketio.emit('batch_progress', progress_data)
        
        # åˆ›å»ºå·¥ä½œåç¨‹
        async def worker(worker_id: int):
            """å·¥ä½œåç¨‹ - å¤„ç†å®Œæ•´çš„æµæ°´çº¿"""
            nonlocal completed_count
            
            while True:
                try:
                    # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                    task_item = await task_queue.get()
                    
                    if task_item is None:  # ç»“æŸä¿¡å·
                        logger.debug(f"æ‰¹é‡ç”Ÿæˆå·¥ä½œåç¨‹ {worker_id} æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œé€€å‡º")
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†å®Œæ‰€æœ‰ä»»åŠ¡
                    async with progress_lock:
                        if completed_count >= total_tasks:
                            logger.warning(f"æ‰¹é‡ç”Ÿæˆå·¥ä½œåç¨‹ {worker_id}: å·²å®Œæˆæ‰€æœ‰ä»»åŠ¡ï¼Œè·³è¿‡é¢å¤–ä»»åŠ¡")
                            break
                    
                    async with semaphore:
                        # å¤„ç†å•ä¸ªä»»åŠ¡
                        result = await _process_single_generation_task(
                            task_item, 
                            config, 
                            run_manager,
                            progress_callback=progress_callback
                        )
                        
                        # å°†ç»“æœå­˜å‚¨åˆ°æ­£ç¡®çš„ä½ç½®
                        if result and "array_index" in task_item:
                            array_index = task_item["array_index"]
                            if 0 <= array_index < total_tasks:
                                results[array_index] = result
                        
                        # å³æ—¶ä¿å­˜ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        instant_save_result(result, config)
                        
                        # å‘é€å•ä¸ªç»“æœ
                        socketio.emit('batch_result', result)
                    
                    # æ›´æ–°å®Œæˆè®¡æ•°å¹¶é€šçŸ¥è¿›åº¦
                    async with progress_lock:
                        completed_count += 1
                        progress_percent = min(completed_count / total_tasks * 100, 100)
                        
                        # é€šçŸ¥ä»»åŠ¡å®Œæˆ
                        if progress_callback and result:
                            status = "completed" if result.get("qa_pair") else "error"
                            progress_callback(
                                f"ç¬¬{result.get('index', task_item['index'])}é¢˜: å®Œæˆ - {status.upper()} ({completed_count}/{total_tasks})", 
                                progress_percent,
                                task_id=task_item["task_id"],
                                status="completed"
                            )
                            
                        logger.debug(f"æ‰¹é‡ç”Ÿæˆå·¥ä½œåç¨‹ {worker_id}: å®Œæˆç¬¬{task_item['index']}ä¸ªå®ä½“ ({completed_count}/{total_tasks})")
                        
                except Exception as e:
                    logger.error(f"æ‰¹é‡ç”Ÿæˆå·¥ä½œåç¨‹ {worker_id} å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                    # å³ä½¿å‡ºé”™ä¹Ÿè¦æ ‡è®°ä»»åŠ¡å®Œæˆ
                    async with progress_lock:
                        completed_count += 1
                        progress_percent = min(completed_count / total_tasks * 100, 100)
                        
                        # é€šçŸ¥ä»»åŠ¡å¤±è´¥
                        if progress_callback and task_item:
                            progress_callback(
                                f"ç¬¬{task_item.get('index', '?')}é¢˜: ç³»ç»Ÿé”™è¯¯ ({completed_count}/{total_tasks})", 
                                progress_percent,
                                task_id=task_item.get("task_id", "unknown"),
                                status="error"
                            )
        
        # å¯åŠ¨å·¥ä½œåç¨‹
        workers = [asyncio.create_task(worker(i)) for i in range(parallel_workers)]
        
        # ç­‰å¾…æ‰€æœ‰å·¥ä½œåç¨‹ç»“æŸ
        await asyncio.gather(*workers, return_exceptions=True)
        
        # è¿‡æ»¤æ‰Noneç»“æœå¹¶ç¡®ä¿è¿ç»­æ€§
        valid_results = [r for r in results if r is not None]
        
        # éªŒè¯ç»“æœå®Œæ•´æ€§
        if len(valid_results) != total_tasks:
            logger.warning(f"æ‰¹é‡ç”Ÿæˆç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ›{total_tasks}ä¸ªï¼Œå®é™…{len(valid_results)}ä¸ª")
        
        # æŒ‰ç´¢å¼•æ’åºç»“æœ
        valid_results.sort(key=lambda x: x.get("initial_entity", ""))
        
        return valid_results
        
    except Exception as e:
        logger.error(f"å¼‚æ­¥æ‰¹é‡ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        return []

async def async_evaluation_process(evaluation_id, config):
    """å¼‚æ­¥è¯„æµ‹è¿‡ç¨‹"""
    from lib.trace_manager import TraceManager, start_trace
    
    # ç»§æ‰¿æˆ–åˆ›å»ºtrace
    parent_trace = TraceManager.get_trace_id()
    if parent_trace:
        logger.info(f"å¼‚æ­¥è¯„æµ‹ç»§æ‰¿trace: {parent_trace}")
    else:
        start_trace(prefix=f"eval_async")
        logger.info(f"å¼‚æ­¥è¯„æµ‹åˆ›å»ºæ–°trace")
    
    try:
        from lib.evaluator import Evaluator
        
        dataset_id = config.get('dataset_id')
        evaluation_mode = config.get('evaluation_mode', 'R1-0528')
        
        logger.info(f"å¼€å§‹è¯„æµ‹ï¼Œè¯„æµ‹ID: {evaluation_id}, æ•°æ®é›†: {dataset_id}, æ¨¡å¼: {evaluation_mode}")
        
        # ç¡®å®šæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        standard_path = f'evaluation_data/standard_datasets/{dataset_id}'
        generated_path = f'evaluation_data/generated_datasets/{dataset_id}'
        
        dataset_path = None
        if os.path.exists(standard_path):
            dataset_path = standard_path
        elif os.path.exists(generated_path):
            dataset_path = generated_path
        else:
            raise ValueError(f"æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {dataset_id}")
            
        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_tasks = 0
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                total_tasks = sum(1 for line in f if line.strip())
        except Exception as e:
            logger.error(f"è®¡ç®—ä»»åŠ¡æ€»æ•°å¤±è´¥: {e}")
            total_tasks = 0
        
        # åˆ›å»ºè¯„æµ‹å™¨
        evaluator = Evaluator()
        
        def progress_callback(message, progress, task_id=None, status=None):
            progress_data = {
                'evaluation_id': evaluation_id,
                'step': message,  # ä½¿ç”¨messageä½œä¸ºstep
                'progress': progress
            }
            
            # æ·»åŠ ä»»åŠ¡çº§åˆ«çš„ä¿¡æ¯
            if task_id:
                progress_data['task_id'] = task_id
            if message:
                progress_data['message'] = message
            if status:
                progress_data['status'] = status
                
            socketio.emit('evaluation_progress', progress_data)
        
        # æ‰§è¡Œè¯„æµ‹
        dataset_name = dataset_id.replace('.jsonl', '')
        batch_size = config.get('batch_size', 10)
        result = await evaluator.evaluate_dataset(
            dataset_path=dataset_path,
            dataset_name=dataset_name,
            mode=evaluation_mode,
            progress_callback=progress_callback,
            batch_size=batch_size
        )
        
        # å°†total_tasksä¿¡æ¯ä¼ é€’ç»™å‰ç«¯
        config['total_tasks'] = total_tasks
        
        return result
        
    except Exception as e:
        logger.error(f"å¼‚æ­¥è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        raise

async def async_comparison_process(comparison_id, config):
    """å¼‚æ­¥å¯¹æ¯”è¯„æµ‹è¿‡ç¨‹"""
    try:
        from lib.comparison_evaluator import ComparisonEvaluator
        
        logger.info(f"å¼€å§‹å¯¹æ¯”è¯„æµ‹ï¼Œå¯¹æ¯”ID: {comparison_id}")
        
        # åˆ›å»ºå¯¹æ¯”è¯„æµ‹å™¨
        evaluator = ComparisonEvaluator()
        
        def progress_callback(message, progress, task_id=None, status=None, details=None):
            progress_data = {
                'comparison_id': comparison_id,
                'message': message,
                'percentage': progress if progress is not None else 0
            }
            
            # æ·»åŠ ä»»åŠ¡çº§åˆ«çš„ä¿¡æ¯
            if task_id:
                progress_data['task_id'] = task_id
            if status:
                progress_data['status'] = status
            if details:
                progress_data['details'] = details
                
            socketio.emit('comparison_progress', progress_data)
        
        # æ‰§è¡Œå¯¹æ¯”è¯„æµ‹
        result = await evaluator.compare_datasets(config, progress_callback)
        
        return result
        
    except Exception as e:
        logger.error(f"å¼‚æ­¥å¯¹æ¯”è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        raise


# Runs QAç”Ÿæˆç›¸å…³API
runs_qa_generator = RunsQAGenerator()

@app.route('/api/runs/list', methods=['GET'])
def list_runs():
    """è·å–æ‰€æœ‰å¯ç”¨çš„è¿è¡Œè®°å½•"""
    try:
        runs = runs_qa_generator.list_available_runs()
        return jsonify({
            'success': True,
            'runs': runs
        })
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œè®°å½•åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/runs/<run_id>/graph', methods=['GET'])
def get_run_graph(run_id):
    """è·å–æŒ‡å®šè¿è¡Œè®°å½•çš„å›¾æ•°æ®"""
    try:
        graph_data = asyncio.run(runs_qa_generator.extract_graph_from_run(run_id))
        return jsonify({
            'success': True,
            'graph_data': graph_data
        })
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œè®°å½•å›¾æ•°æ®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/runs/generate-qa', methods=['POST'])
def generate_qa_from_runs():
    """ä»è¿è¡Œè®°å½•ç”ŸæˆQAï¼ˆæ”¯æŒQPSé™åˆ¶ï¼‰"""
    from lib.trace_manager import start_trace
    
    try:
        # å¯åŠ¨trace
        trace_id = start_trace(prefix="runs")
        
        data = request.get_json()
        run_ids = data.get('run_ids', [])
        sample_size = data.get('sample_size', 10)
        sampling_algorithm = data.get('sampling_algorithm', 'mixed')
        questions_per_run = data.get('questions_per_run', 1)
        use_unified_qa = data.get('use_unified_qa', True)
        qps_limit = data.get('qps_limit', 2.0)
        parallel_workers = data.get('parallel_workers', 1)
        
        logger.info(f"å¼€å§‹ä»Runsç”ŸæˆQAï¼Œè¿è¡Œè®°å½•æ•°: {len(run_ids)}, QPSé™åˆ¶: {qps_limit}")
        
        if not run_ids:
            return jsonify({
                'success': False,
                'error': 'è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¿è¡Œè®°å½•'
            }), 400
        
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
        task_id = f"runs_qa_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        def generate_qa_task():
            try:
                # è¿›åº¦å›è°ƒå‡½æ•°
                def progress_callback(message, progress):
                    socketio.emit('runs_qa_progress', {
                        'task_id': task_id,
                        'message': message,
                        'progress': progress
                    })
                
                if len(run_ids) == 1:
                    # å•ä¸ªè¿è¡Œè®°å½•
                    progress_callback("æ­£åœ¨å¤„ç†å•ä¸ªè¿è¡Œè®°å½•...", 10)
                    results = asyncio.run(runs_qa_generator.generate_qa_from_run(
                        run_id=run_ids[0],
                        sample_size=sample_size,
                        sampling_algorithm=sampling_algorithm,
                        num_questions=questions_per_run,
                        use_unified_qa=use_unified_qa
                    ))
                    
                    progress_callback("ä¿å­˜ç»“æœæ–‡ä»¶...", 90)
                    # ä¿å­˜ç»“æœ
                    output_file = f"qa_output/runs_qa_{task_id}.jsonl"
                    runs_qa_generator.save_qa_results(results, output_file)
                    
                    socketio.emit('runs_qa_complete', {
                        'task_id': task_id,
                        'success': True,
                        'results_count': len(results),
                        'output_file': output_file,
                        'qa_results': results  # åŒ…å«å®é™…çš„QAå†…å®¹
                    })
                else:
                    # å¤šä¸ªè¿è¡Œè®°å½• - ä½¿ç”¨QPSé™åˆ¶
                    progress_callback(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(run_ids)} ä¸ªè®°å½•ï¼ˆQPSé™åˆ¶: {qps_limit}ï¼‰...", 5)
                    
                    results = asyncio.run(runs_qa_generator.batch_generate_from_multiple_runs_with_qps_limit(
                        run_ids=run_ids,
                        sample_size=sample_size,
                        sampling_algorithm=sampling_algorithm,
                        questions_per_run=questions_per_run,
                        use_unified_qa=use_unified_qa,
                        qps_limit=qps_limit,
                        parallel_workers=parallel_workers,
                        progress_callback=progress_callback
                    ))
                    
                    # åˆå¹¶æ‰€æœ‰ç»“æœ
                    all_results = []
                    for run_id, run_results in results.items():
                        all_results.extend(run_results)
                    
                    # ä¿å­˜ç»“æœ
                    output_file = f"qa_output/runs_qa_batch_{task_id}.jsonl"
                    runs_qa_generator.save_qa_results(all_results, output_file)
                    
                    socketio.emit('runs_qa_complete', {
                        'task_id': task_id,
                        'success': True,
                        'results_count': len(all_results),
                        'output_file': output_file,
                        'runs_processed': len(run_ids),
                        'qa_results': all_results  # åŒ…å«å®é™…çš„QAå†…å®¹
                    })
                    
            except Exception as e:
                logger.error(f"Runs QAç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
                socketio.emit('runs_qa_complete', {
                    'task_id': task_id,
                    'success': False,
                    'error': str(e)
                })
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ
        thread = threading.Thread(target=generate_qa_task)
        thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'QAç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨Runs QAç”Ÿæˆå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/qa_output/<filename>')
def download_qa_file(filename):
    """ä¸‹è½½QAç»“æœæ–‡ä»¶"""
    try:
        return send_from_directory('qa_output', filename, as_attachment=True)
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®'}), 404


# æ•°æ®ç®¡ç†ç›¸å…³API
@app.route('/api/data_management/load/<filename>')
def load_data_file(filename):
    """åŠ è½½æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒä»å¤šä¸ªç›®å½•æŸ¥æ‰¾"""
    try:
        import os
        import json
        from datetime import datetime
        
        # å®šä¹‰è¦æœç´¢çš„ç›®å½•ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        search_dirs = [
            "evaluation_data/generated_datasets",
            "evaluation_data/final_datasets", 
            "evaluation_data/final_datasets/label_datasets"
        ]
        
        # æŸ¥æ‰¾æ–‡ä»¶
        file_path = None
        for search_dir in search_dirs:
            potential_path = os.path.join(search_dir, filename)
            if os.path.exists(potential_path):
                file_path = potential_path
                break
        
        if not file_path:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¯»å–æ–‡ä»¶æ•°æ®
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        item = json.loads(line.strip())
                        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                        if 'question' not in item:
                            item['question'] = ''
                        if 'answer' not in item:
                            item['answer'] = ''
                        if 'reasoning_path' not in item:
                            item['reasoning_path'] = item.get('reasoning', '')
                        if 'mapped_reasoning_path' not in item:
                            item['mapped_reasoning_path'] = ''
                        if 'question_language' not in item:
                            item['question_language'] = 'unknown'
                        if 'answer_language' not in item:
                            item['answer_language'] = 'unknown'
                        
                        data.append(item)
                    except json.JSONDecodeError as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num}: {e}")
                        continue
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_stats = os.stat(file_path)
        file_info = {
            'filename': filename,
            'count': len(data),
            'size': file_stats.st_size,
            'modified_time': datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
            'directory': os.path.dirname(file_path)
        }
        
        return jsonify({
            'success': True,
            'data': data,
            'fileInfo': file_info
        })
        
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/detect_languages', methods=['POST'])
def detect_languages():
    """æ£€æµ‹é—®é¢˜å’Œç­”æ¡ˆçš„è¯­è¨€"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        items = data.get('data', [])
        
        if not filename or not items:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        logger.info(f"å¼€å§‹è¯­è¨€æ£€æµ‹ï¼Œå…± {len(items)} æ¡æ•°æ®")
        
        # å…ˆä½¿ç”¨ç®€å•æ£€æµ‹ï¼Œå¦‚æœç”¨æˆ·éœ€è¦LLMæ£€æµ‹å¯ä»¥å•ç‹¬è°ƒç”¨
        results = []
        for item in items:
            question = item.get('question', '')
            answer = item.get('answer', '')
            
            # ä½¿ç”¨ç®€å•è§„åˆ™æ£€æµ‹è¯­è¨€
            question_lang = detect_language_simple(question)
            answer_lang = detect_language_simple(answer)
            
            results.append({
                **item,
                'question_language': question_lang,
                'answer_language': answer_lang
            })
        
        logger.info(f"è¯­è¨€æ£€æµ‹å®Œæˆï¼Œå¤„ç†äº† {len(results)} æ¡æ•°æ®")
        
        return jsonify({
            'success': True,
            'data': results
        })
        
    except Exception as e:
        logger.error(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/detect_languages_llm', methods=['POST'])
def detect_languages_llm():
    """ä½¿ç”¨LLMæ£€æµ‹é—®é¢˜å’Œç­”æ¡ˆçš„è¯­è¨€(é«˜ç²¾åº¦ä½†è¾ƒæ…¢)"""
    try:
        data = request.get_json()
        items = data.get('data', [])
        
        if not items:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        logger.info(f"å¼€å§‹LLMè¯­è¨€æ£€æµ‹ï¼Œå…± {len(items)} æ¡æ•°æ®")
        
        # æ‰¹é‡æ£€æµ‹è¯­è¨€
        from lib.llm_client import LLMClient
        llm_client = LLMClient()
        
        # å‡†å¤‡æ‰¹é‡æ£€æµ‹çš„æ–‡æœ¬
        batch_texts = []
        for i, item in enumerate(items):
            question = item.get('question', '')
            answer = item.get('answer', '')
            
            # æˆªå–æ–‡æœ¬é¿å…tokenè¿‡å¤šï¼Œæ¯ä¸ªå­—æ®µæœ€å¤š500å­—ç¬¦
            question_sample = question[:500] if question else ''
            answer_sample = answer[:500] if answer else ''
            
            batch_texts.append({
                'index': i,
                'question': question_sample,
                'answer': answer_sample
            })
        
        # åˆ›å»ºæ‰¹é‡è¯­è¨€æ£€æµ‹çš„prompt
        prompt_parts = ["è¯·æ£€æµ‹ä»¥ä¸‹æ‰¹é‡æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼Œè¿”å›JSONæ•°ç»„æ ¼å¼ï¼š\n"]
        
        for text_item in batch_texts:
            prompt_parts.append(f"[{text_item['index']}] é—®é¢˜: {text_item['question']}")
            prompt_parts.append(f"[{text_item['index']}] ç­”æ¡ˆ: {text_item['answer']}\n")
        
        prompt_parts.append("""
è¯·ä¸ºæ¯ä¸ªç´¢å¼•è¿”å›è¯­è¨€æ£€æµ‹ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {"index": 0, "question_language": "è¯­è¨€ä»£ç ", "answer_language": "è¯­è¨€ä»£ç "},
  {"index": 1, "question_language": "è¯­è¨€ä»£ç ", "answer_language": "è¯­è¨€ä»£ç "}
]

è¯·ä½¿ç”¨ISO 639-1è¯­è¨€ä»£ç ï¼Œæ”¯æŒçš„è¯­è¨€åŒ…æ‹¬ï¼š
- zh: ä¸­æ–‡ (Chinese)
- en: è‹±æ–‡ (English)
- ja: æ—¥æ–‡ (Japanese)
- ko: éŸ©æ–‡ (Korean)
- fr: æ³•æ–‡ (French)
- de: å¾·æ–‡ (German)
- es: è¥¿ç­ç‰™æ–‡ (Spanish)
- it: æ„å¤§åˆ©æ–‡ (Italian)
- pt: è‘¡è„ç‰™æ–‡ (Portuguese)
- ru: ä¿„æ–‡ (Russian)
- ar: é˜¿æ‹‰ä¼¯æ–‡ (Arabic)
- hi: å°åœ°æ–‡ (Hindi)
- th: æ³°æ–‡ (Thai)
- vi: è¶Šå—æ–‡ (Vietnamese)
- unknown: å®åœ¨æ— æ³•ç¡®å®šçš„è¯­è¨€

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""")
        
        full_prompt = '\n'.join(prompt_parts)
        
        # æ£€æŸ¥prompté•¿åº¦
        if len(full_prompt) > 150000:
            # å¦‚æœpromptå¤ªé•¿ï¼Œåˆ†æ‰¹å¤„ç†
            return process_large_batch_detection(items, llm_client)
        
        try:
            import asyncio
            response = asyncio.run(llm_client.generate_response(full_prompt))
            logger.info(f"LLMè¯­è¨€æ£€æµ‹å“åº”é•¿åº¦: {len(response)}")
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = response.find('[')
                json_end = response.rfind(']') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    detection_results = json.loads(json_str)
                else:
                    raise json.JSONDecodeError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°ç»„", response, 0)
                
                # åˆ›å»ºç»“æœå­—å…¸
                results_dict = {}
                for result in detection_results:
                    if isinstance(result, dict) and 'index' in result:
                        results_dict[result['index']] = result
                
                # ç»„è£…æœ€ç»ˆç»“æœ
                results = []
                for i, item in enumerate(items):
                    if i in results_dict:
                        result = results_dict[i]
                        question_lang = result.get('question_language', 'unknown')
                        answer_lang = result.get('answer_language', 'unknown')
                        
                        # éªŒè¯è¯­è¨€ä»£ç 
                        valid_langs = ['zh', 'en', 'ja', 'ko', 'fr', 'de', 'es', 'it', 'pt', 'ru', 'ar', 'hi', 'th', 'vi', 'unknown']
                        if question_lang not in valid_langs:
                            question_lang = 'unknown'
                        if answer_lang not in valid_langs:
                            answer_lang = 'unknown'
                            
                        results.append({
                            **item,
                            'question_language': question_lang,
                            'answer_language': answer_lang
                        })
                    else:
                        # å¦‚æœLLMæ²¡æœ‰è¿”å›è¿™ä¸ªç´¢å¼•çš„ç»“æœï¼Œä½¿ç”¨ç®€å•æ£€æµ‹
                        results.append({
                            **item,
                            'question_language': detect_language_simple(item.get('question', '')),
                            'answer_language': detect_language_simple(item.get('answer', ''))
                        })
                
                logger.info(f"LLMè¯­è¨€æ£€æµ‹å®Œæˆï¼Œå¤„ç†äº† {len(results)} æ¡æ•°æ®")
                
                return jsonify({
                    'success': True,
                    'data': results
                })
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLMè¿”å›éJSONæ ¼å¼ï¼Œå›é€€åˆ°ç®€å•æ£€æµ‹: {e}")
                # å¦‚æœLLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSONï¼Œå›é€€åˆ°ç®€å•æ£€æµ‹
                results = []
                for item in items:
                    results.append({
                        **item,
                        'question_language': detect_language_simple(item.get('question', '')),
                        'answer_language': detect_language_simple(item.get('answer', ''))
                    })
                
                return jsonify({
                    'success': True,
                    'data': results
                })
                
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            # å‘ç”Ÿé”™è¯¯æ—¶å›é€€åˆ°ç®€å•æ£€æµ‹
            results = []
            for item in items:
                results.append({
                    **item,
                    'question_language': detect_language_simple(item.get('question', '')),
                    'answer_language': detect_language_simple(item.get('answer', ''))
                })
            
            return jsonify({
                'success': True,
                'data': results
            })
        
    except Exception as e:
        logger.error(f"LLMè¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

def process_large_batch_detection(items, llm_client):
    """å¤„ç†å¤§æ‰¹é‡æ•°æ®çš„è¯­è¨€æ£€æµ‹"""
    import asyncio
    
    async def detect_single_item(item):
        """æ£€æµ‹å•ä¸ªæ¡ç›®çš„è¯­è¨€"""
        try:
            question = item.get('question', '')
            answer = item.get('answer', '')
            
            # æˆªå–æ–‡æœ¬é¿å…tokenè¿‡å¤š
            question_sample = question[:500] if question else ''
            answer_sample = answer[:500] if answer else ''
            
            # åˆ›å»ºå•ä¸ªæ£€æµ‹çš„prompt
            prompt = f"""è¯·æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬çš„è¯­è¨€ç±»å‹ï¼š

é—®é¢˜æ–‡æœ¬: {question_sample}
ç­”æ¡ˆæ–‡æœ¬: {answer_sample}

è¯·åˆ¤æ–­æ¯ä¸ªæ–‡æœ¬çš„ä¸»è¦è¯­è¨€ï¼Œåªè¿”å›ä»¥ä¸‹æ ¼å¼çš„JSONï¼š
{{"question_language": "è¯­è¨€ä»£ç ", "answer_language": "è¯­è¨€ä»£ç "}}

è¯·ä½¿ç”¨ISO 639-1è¯­è¨€ä»£ç ï¼Œæ”¯æŒçš„è¯­è¨€åŒ…æ‹¬ï¼š
- zh: ä¸­æ–‡ (Chinese)
- en: è‹±æ–‡ (English)
- ja: æ—¥æ–‡ (Japanese)
- ko: éŸ©æ–‡ (Korean)
- fr: æ³•æ–‡ (French)
- de: å¾·æ–‡ (German)
- es: è¥¿ç­ç‰™æ–‡ (Spanish)
- it: æ„å¤§åˆ©æ–‡ (Italian)
- pt: è‘¡è„ç‰™æ–‡ (Portuguese)
- ru: ä¿„æ–‡ (Russian)
- ar: é˜¿æ‹‰ä¼¯æ–‡ (Arabic)
- hi: å°åœ°æ–‡ (Hindi)
- th: æ³°æ–‡ (Thai)
- vi: è¶Šå—æ–‡ (Vietnamese)
- unknown: å®åœ¨æ— æ³•ç¡®å®šçš„è¯­è¨€"""
            
            response = await llm_client.generate_response(prompt)
            
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    result = json.loads(json_str)
                    
                    question_lang = result.get('question_language', 'unknown')
                    answer_lang = result.get('answer_language', 'unknown')
                    
                    # éªŒè¯è¯­è¨€ä»£ç 
                    valid_langs = ['zh', 'en', 'ja', 'ko', 'fr', 'de', 'es', 'it', 'pt', 'ru', 'ar', 'hi', 'th', 'vi', 'unknown']
                    if question_lang not in valid_langs:
                        question_lang = 'unknown'
                    if answer_lang not in valid_langs:
                        answer_lang = 'unknown'
                    
                    return {
                        **item,
                        'question_language': question_lang,
                        'answer_language': answer_lang
                    }
                else:
                    raise json.JSONDecodeError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON", response, 0)
                    
            except json.JSONDecodeError:
                # å›é€€åˆ°ç®€å•æ£€æµ‹
                return {
                    **item,
                    'question_language': detect_language_simple(question),
                    'answer_language': detect_language_simple(answer)
                }
                
        except Exception as e:
            logger.error(f"å•ä¸ªé¡¹ç›®æ£€æµ‹å¤±è´¥: {e}")
            return {
                **item,
                'question_language': detect_language_simple(item.get('question', '')),
                'answer_language': detect_language_simple(item.get('answer', ''))
            }
    
    async def process_all():
        tasks = [detect_single_item(item) for item in items]
        return await asyncio.gather(*tasks)
    
    results = asyncio.run(process_all())
    
    return jsonify({
        'success': True,
        'data': results
    })

def detect_language_simple(text):
    """ç®€å•çš„è¯­è¨€æ£€æµ‹å‡½æ•°"""
    if not text or len(text.strip()) == 0:
        return 'unknown'
    
    # ç§»é™¤ç©ºç™½å­—ç¬¦è¿›è¡Œç»Ÿè®¡
    text_clean = ''.join(text.split())
    if len(text_clean) == 0:
        return 'unknown'
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ï¼‰
    chinese_count = sum(1 for char in text_clean if '\u4e00' <= char <= '\u9fff' or 
                       '\u3000' <= char <= '\u303f' or '\uff00' <= char <= '\uffef')
    
    # ç»Ÿè®¡è‹±æ–‡å­—æ¯
    english_count = sum(1 for char in text_clean if char.isalpha() and ord(char) < 128)
    
    # ç»Ÿè®¡æ€»å­—ç¬¦æ•°ï¼ˆæ’é™¤ç©ºæ ¼å’Œå¸¸è§æ ‡ç‚¹ï¼‰
    total_chars = len(text_clean)
    
    if total_chars == 0:
        return 'unknown'
    
    chinese_ratio = chinese_count / total_chars
    english_ratio = english_count / total_chars
    
    logger.debug(f"è¯­è¨€æ£€æµ‹ - æ–‡æœ¬é•¿åº¦: {total_chars}, ä¸­æ–‡æ¯”ä¾‹: {chinese_ratio:.2f}, è‹±æ–‡æ¯”ä¾‹: {english_ratio:.2f}")
    
    # è°ƒæ•´é˜ˆå€¼ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§
    if chinese_ratio > 0.05:  # å¦‚æœä¸­æ–‡å­—ç¬¦è¶…è¿‡5%
        return 'zh'
    elif english_ratio > 0.3:  # å¦‚æœè‹±æ–‡å­—ç¬¦è¶…è¿‡30%
        return 'en'
    else:
        return 'unknown'

@app.route('/api/data_management/save', methods=['POST'])
def save_data_file():
    """ä¿å­˜æ•°æ®æ–‡ä»¶"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        items = data.get('data', [])
        
        if not filename or not items:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        import os
        import json
        from datetime import datetime
        
        # å®šä¹‰è¦æœç´¢çš„ç›®å½•ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        search_dirs = [
            "evaluation_data/generated_datasets",
            "evaluation_data/final_datasets", 
            "evaluation_data/final_datasets/label_datasets"
        ]
        
        # æŸ¥æ‰¾åŸå§‹æ–‡ä»¶ä½ç½®
        file_path = None
        for search_dir in search_dirs:
            potential_path = os.path.join(search_dir, filename)
            if os.path.exists(potential_path):
                file_path = potential_path
                break
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œé»˜è®¤ä¿å­˜åˆ°generated_datasetsç›®å½•
        if not file_path:
            file_path = f'evaluation_data/generated_datasets/{filename}'
        
        # åˆ›å»ºå¤‡ä»½
        if os.path.exists(file_path):
            backup_path = f'{file_path}.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
            import shutil
            shutil.copy2(file_path, backup_path)
            logger.info(f"åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path}")
        
        # ä¿å­˜æ•°æ®
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in items:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        logger.info(f"æ•°æ®æ–‡ä»¶å·²ä¿å­˜: {file_path}, å…± {len(items)} æ¡è®°å½•")
        
        return jsonify({
            'success': True,
            'message': f'å·²ä¿å­˜ {len(items)} æ¡è®°å½•'
        })
        
    except Exception as e:
        logger.error(f"ä¿å­˜æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/save_as', methods=['POST'])
def save_as_data_file():
    """å¦å­˜ä¸ºæ–°æ•°æ®æ–‡ä»¶"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        items = data.get('data', [])
        scope = data.get('scope', 'filtered')
        original_file = data.get('original_file', '')
        
        if not filename or not items:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        import os
        import json
        from datetime import datetime
        
        # ç¡®å®šä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¿å­˜åˆ°generated_datasetsç›®å½•
        file_path = f'evaluation_data/generated_datasets/{filename}'
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(file_path):
            return jsonify({'success': False, 'error': f'æ–‡ä»¶ {filename} å·²å­˜åœ¨ï¼Œè¯·é€‰æ‹©å…¶ä»–æ–‡ä»¶å'}), 400
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in items:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        # è®°å½•æ“ä½œä¿¡æ¯
        scope_desc = {
            'filtered': 'ç­›é€‰åçš„æ•°æ®',
            'all': 'å…¨éƒ¨æ•°æ®',
            'selected': 'é€‰ä¸­çš„æ•°æ®'
        }.get(scope, 'æ•°æ®')
        
        logger.info(f"å¦å­˜ä¸ºæ–°æ–‡ä»¶: {file_path}, æ¥æº: {original_file}, èŒƒå›´: {scope_desc}, å…± {len(items)} æ¡æ•°æ®")
        
        return jsonify({
            'success': True,
            'filename': filename,
            'count': len(items),
            'scope': scope_desc,
            'path': file_path
        })
        
    except Exception as e:
        logger.error(f"å¦å­˜ä¸ºå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# å®ä½“æ˜ å°„å’Œå˜é‡æ›¿æ¢ç›¸å…³API
@app.route('/api/data_management/extract_entities', methods=['POST'])
def extract_entities():
    """ä»æ¨ç†è·¯å¾„ä¸­æå–å®ä½“å’Œå˜é‡"""
    try:
        data = request.get_json()
        reasoning_path = data.get('reasoning_path', '')
        
        if not reasoning_path:
            return jsonify({'success': False, 'error': 'æ¨ç†è·¯å¾„ä¸èƒ½ä¸ºç©º'}), 400
        
        # æå–å®ä½“å’Œå˜é‡
        entities = extract_entities_from_text(reasoning_path)
        
        return jsonify({
            'success': True,
            'entities': entities
        })
        
    except Exception as e:
        logger.error(f"æå–å®ä½“å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/replace_entities', methods=['POST'])
def replace_entities():
    """æ›¿æ¢æ¨ç†è·¯å¾„ä¸­çš„å®ä½“"""
    try:
        data = request.get_json()
        reasoning_path = data.get('reasoning_path', '')
        entity_mapping = data.get('entity_mapping', {})
        
        if not reasoning_path:
            return jsonify({'success': False, 'error': 'æ¨ç†è·¯å¾„ä¸èƒ½ä¸ºç©º'}), 400
        
        # æ‰§è¡Œå®ä½“æ›¿æ¢
        new_reasoning_path = replace_entities_in_text(reasoning_path, entity_mapping)
        
        return jsonify({
            'success': True,
            'new_reasoning_path': new_reasoning_path,
            'replacements_made': len([k for k, v in entity_mapping.items() if k != v and k in reasoning_path])
        })
        
    except Exception as e:
        logger.error(f"æ›¿æ¢å®ä½“å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

def extract_entities_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå–å¯èƒ½çš„å®ä½“"""
    import re
    
    entities = []
    
    # 1. æå–ä¸“æœ‰åè¯ï¼ˆå¤§å†™å­—æ¯å¼€å¤´çš„è¯ï¼‰
    proper_nouns = re.findall(r'\b[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*\b', text)
    entities.extend(proper_nouns)
    
    # 2. æå–æ‹¬å·ä¸­çš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯å®ä½“çš„å…·ä½“åç§°ï¼‰
    parentheses_content = re.findall(r'\*\*([^*]+)\*\*', text)
    entities.extend(parentheses_content)
    
    # 3. æå–å¹´ä»½
    years = re.findall(r'\b(19|20)\d{2}\b', text)
    entities.extend(years)
    
    # 4. æå–åœ°åå’Œäººåçš„ç‰¹æ®Šæ¨¡å¼
    geographic_patterns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s*,\s*[A-Z][a-z]+)*\b', text)
    entities.extend(geographic_patterns)
    
    # 5. å»é‡å¹¶æ’åºï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œé•¿çš„åœ¨å‰é¢ï¼Œé¿å…æ›¿æ¢æ—¶çš„å†²çªï¼‰
    unique_entities = list(set(entities))
    unique_entities = [e for e in unique_entities if len(e.strip()) > 2]  # è¿‡æ»¤å¤ªçŸ­çš„
    unique_entities.sort(key=len, reverse=True)
    
    # 6. ç»Ÿè®¡æ¯ä¸ªå®ä½“çš„å‡ºç°æ¬¡æ•°
    entity_info = []
    for entity in unique_entities:
        count = text.count(entity)
        if count > 0:
            entity_info.append({
                'entity': entity,
                'count': count,
                'type': classify_entity_type(entity)
            })
    
    return entity_info

def classify_entity_type(entity):
    """ç®€å•åˆ†ç±»å®ä½“ç±»å‹"""
    import re
    
    if re.match(r'\b(19|20)\d{2}\b', entity):
        return 'year'
    elif entity[0].isupper() and ' ' in entity:
        return 'proper_name'
    elif entity[0].isupper():
        return 'name'
    else:
        return 'other'

def replace_entities_in_text(text, entity_mapping):
    """åœ¨æ–‡æœ¬ä¸­æ›¿æ¢å®ä½“"""
    import re
    
    new_text = text
    
    # æŒ‰é•¿åº¦å€’åºæ’åˆ—ï¼Œç¡®ä¿å…ˆæ›¿æ¢é•¿çš„å®ä½“ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…é—®é¢˜
    sorted_entities = sorted(entity_mapping.items(), key=lambda x: len(x[0]), reverse=True)
    
    for old_entity, new_entity in sorted_entities:
        if old_entity != new_entity and old_entity.strip() and new_entity.strip():
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œç²¾ç¡®åŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
            pattern = re.escape(old_entity)
            new_text = re.sub(pattern, new_entity, new_text)
    
    return new_text

@app.route('/api/data_management/get_languages', methods=['POST'])
def get_available_languages():
    """è·å–å½“å‰æ•°æ®é›†ä¸­å­˜åœ¨çš„è¯­è¨€åˆ—è¡¨"""
    try:
        data = request.get_json()
        items = data.get('data', [])
        
        if not items:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        # æ”¶é›†æ‰€æœ‰å‡ºç°çš„è¯­è¨€
        question_languages = set()
        answer_languages = set()
        
        for item in items:
            q_lang = item.get('question_language', 'unknown')
            a_lang = item.get('answer_language', 'unknown')
            
            if q_lang and q_lang.strip():
                question_languages.add(q_lang.strip())
            if a_lang and a_lang.strip():
                answer_languages.add(a_lang.strip())
        
        # è¯­è¨€ä»£ç åˆ°æ˜¾ç¤ºåç§°çš„æ˜ å°„
        language_names = {
            'zh': 'ä¸­æ–‡',
            'en': 'English',
            'ja': 'æ—¥æœ¬èª',
            'ko': 'í•œêµ­ì–´',
            'fr': 'FranÃ§ais',
            'de': 'Deutsch',
            'es': 'EspaÃ±ol',
            'it': 'Italiano',
            'pt': 'PortuguÃªs',
            'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹',
            'ar': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©',
            'hi': 'à¤¹à¤¿à¤¨à¥à¤¦à¥€',
            'th': 'à¹„à¸—à¸¢',
            'vi': 'Tiáº¿ng Viá»‡t',
            'unknown': 'æœªçŸ¥è¯­è¨€'
        }
        
        # æ„å»ºè¯­è¨€é€‰é¡¹
        question_options = []
        answer_options = []
        
        for lang in sorted(question_languages):
            question_options.append({
                'code': lang,
                'name': language_names.get(lang, lang)
            })
        
        for lang in sorted(answer_languages):
            answer_options.append({
                'code': lang,
                'name': language_names.get(lang, lang)
            })
        
        return jsonify({
            'success': True,
            'question_languages': question_options,
            'answer_languages': answer_options
        })
        
    except Exception as e:
        logger.error(f"è·å–è¯­è¨€åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/detect_domain_tags', methods=['POST'])
def detect_domain_tags():
    """
    [å·²åºŸå¼ƒ] ä½¿ç”¨LLMæ£€æµ‹é—®é¢˜çš„é¢†åŸŸæ ‡ç­¾
    æ³¨æ„ï¼šæ­¤æ¥å£å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ detect_folder_domain_tags æ¥å£ï¼Œè¯¥æ¥å£å…·æœ‰å®Œæ•´çš„æ ‡ç­¾ç®¡ç†åŠŸèƒ½
    """
    try:
        data = request.get_json()
        items = data.get('data', [])
        existing_tags = data.get('existing_tags', [])
        
        if not items:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        logger.info(f"[å·²åºŸå¼ƒæ¥å£] å¼€å§‹é¢†åŸŸæ ‡ç­¾æ£€æµ‹ï¼Œå…± {len(items)} æ¡æ•°æ®")
        
        from lib.llm_client import LLMClient
        llm_client = LLMClient()
        
        # ç»Ÿä¸€ä½¿ç”¨æŒ‰å­—ç¬¦æ•°åˆ†æ‰¹çš„é€»è¾‘
        results = process_batch_domain_detection(items, existing_tags, llm_client)
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
        all_tags = set(existing_tags)
        for result in results:
            if 'domain_tags' in result:
                all_tags.update(result['domain_tags'])
        
        logger.info(f"[å·²åºŸå¼ƒæ¥å£] é¢†åŸŸæ ‡ç­¾æ£€æµ‹å®Œæˆï¼Œæ£€æµ‹åˆ° {len(all_tags)} ä¸ªä¸åŒæ ‡ç­¾")
        return jsonify({
            'success': True,
            'results': results,
            'all_tags': sorted(list(all_tags))
        })
        
    except Exception as e:
        logger.error(f"[å·²åºŸå¼ƒæ¥å£] é¢†åŸŸæ ‡ç­¾æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/data_management/detect_folder_domain_tags', methods=['POST'])
def detect_folder_domain_tags():
    """æ£€æµ‹æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSONLæ–‡ä»¶çš„é¢†åŸŸæ ‡ç­¾"""
    try:
        data = request.get_json()
        folder_path = data.get('folder_path', '')
        force_reprocess = data.get('force_reprocess', False)
        
        if not folder_path:
            return jsonify({'success': False, 'error': 'è¯·æä¾›æ–‡ä»¶å¤¹è·¯å¾„'}), 400
        
        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        if not os.path.exists(folder_path):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}), 400
        
        if force_reprocess:
            logger.info(f"å¼€å§‹å¼ºåˆ¶é‡æ–°å¤„ç†æ–‡ä»¶å¤¹: {folder_path}")
        else:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶å¤¹: {folder_path}")
        
        from lib.llm_client import LLMClient
        llm_client = LLMClient()
        
        # å¤„ç†æ–‡ä»¶å¤¹
        result = process_folder_domain_detection(folder_path, llm_client, force_reprocess)
        
        return jsonify({
            'success': True,
            'result': result
        })
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤¹é¢†åŸŸæ ‡ç­¾æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/data_management/get_domain_tags_info', methods=['GET'])
def get_domain_tags_info():
    """è·å–é¢†åŸŸæ ‡ç­¾ä¿¡æ¯"""
    try:
        folder_path = request.args.get('folder_path', '')
        logger.info(f"è·å–é¢†åŸŸæ ‡ç­¾ä¿¡æ¯è¯·æ±‚ï¼Œæ–‡ä»¶å¤¹è·¯å¾„: {folder_path}")
        
        if not folder_path:
            logger.warning("æœªæä¾›æ–‡ä»¶å¤¹è·¯å¾„")
            return jsonify({'success': False, 'error': 'è¯·æä¾›æ–‡ä»¶å¤¹è·¯å¾„'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(folder_path):
            logger.error(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return jsonify({'success': False, 'error': f'æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}'}), 400
        
        if not os.path.isdir(folder_path):
            logger.error(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")
            return jsonify({'success': False, 'error': f'è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}'}), 400
            
        info_file = os.path.join(folder_path, 'domain_tags_info.json')
        logger.info(f"æ£€æŸ¥æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶: {info_file}")
        
        if os.path.exists(info_file):
            logger.info("æ‰¾åˆ°æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶ï¼Œæ­£åœ¨è¯»å–...")
            try:
                with open(info_file, 'r', encoding='utf-8') as f:
                    info = json.load(f)
                logger.info(f"æˆåŠŸè¯»å–æ ‡ç­¾ä¿¡æ¯ï¼ŒåŒ…å« {len(info.get('tags', {}))} ä¸ªæ ‡ç­¾")
            except json.JSONDecodeError as e:
                logger.error(f"æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                return jsonify({'success': False, 'error': f'æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}'}), 500
            except UnicodeDecodeError as e:
                logger.error(f"æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶ç¼–ç é”™è¯¯: {e}")
                return jsonify({'success': False, 'error': f'æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶ç¼–ç é”™è¯¯: {e}'}), 500
        else:
            logger.info("æœªæ‰¾åˆ°æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶ï¼Œè¿”å›é»˜è®¤ä¿¡æ¯")
            info = {
                'tags': {},
                'total_processed': 0,
                'last_updated': None,
                'file_processing_status': {}
            }
        
        logger.info(f"æˆåŠŸè·å–é¢†åŸŸæ ‡ç­¾ä¿¡æ¯")
        return jsonify({
            'success': True,
            'info': info
        })
        
    except PermissionError as e:
        logger.error(f"æƒé™é”™è¯¯: {e}")
        return jsonify({'success': False, 'error': f'æƒé™é”™è¯¯ï¼Œæ— æ³•è®¿é—®æ–‡ä»¶å¤¹: {e}'}), 403
    except Exception as e:
        logger.error(f"è·å–é¢†åŸŸæ ‡ç­¾ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'}), 500


@app.route('/api/data_management/get_folder_data', methods=['GET'])
def get_folder_data():
    """è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰JSONæ•°æ®"""
    try:
        folder_path = request.args.get('folder_path', '')
        logger.info(f"è·å–æ–‡ä»¶å¤¹æ•°æ®è¯·æ±‚ï¼Œæ–‡ä»¶å¤¹è·¯å¾„: {folder_path}")
        
        if not folder_path:
            logger.warning("æœªæä¾›æ–‡ä»¶å¤¹è·¯å¾„")
            return jsonify({'success': False, 'error': 'è¯·æä¾›æ–‡ä»¶å¤¹è·¯å¾„'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(folder_path):
            logger.error(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return jsonify({'success': False, 'error': f'æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}'}), 400
        
        if not os.path.isdir(folder_path):
            logger.error(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")
            return jsonify({'success': False, 'error': f'è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}'}), 400
        
        # æŸ¥æ‰¾æ‰€æœ‰å¸¦æ ‡ç­¾çš„JSONLæ–‡ä»¶
        all_data = []
        processed_files = 0
        
        # æ£€æŸ¥taggedå­ç›®å½•æ˜¯å¦å­˜åœ¨
        tagged_dir = os.path.join(folder_path, 'tagged')
        if os.path.exists(tagged_dir):
            search_dir = tagged_dir
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œå¦‚æœtaggedç›®å½•ä¸å­˜åœ¨ï¼Œè¿˜æ˜¯ä»åŸç›®å½•æŸ¥æ‰¾
            search_dir = folder_path
            
        for filename in os.listdir(search_dir):
            if filename.endswith('_with_tags.jsonl'):
                file_path = os.path.join(search_dir, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line_num, line in enumerate(f, 1):
                            line = line.strip()
                            if line:
                                try:
                                    item = json.loads(line)
                                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                                    item['_source_file'] = filename
                                    item['_line_number'] = line_num
                                    all_data.append(item)
                                except json.JSONDecodeError as e:
                                    logger.warning(f"æ–‡ä»¶ {filename} ç¬¬ {line_num} è¡ŒJSONæ ¼å¼é”™è¯¯: {e}")
                                    continue
                    processed_files += 1
                    logger.info(f"å·²å¤„ç†æ–‡ä»¶: {filename}")
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    continue
        
        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶å¤¹æ•°æ®ï¼Œå…± {len(all_data)} æ¡æ•°æ®ï¼Œæ¥è‡ª {processed_files} ä¸ªæ–‡ä»¶")
        return jsonify({
            'success': True,
            'data': all_data,
            'total_items': len(all_data),
            'processed_files': processed_files
        })
        
    except PermissionError as e:
        logger.error(f"æƒé™é”™è¯¯: {e}")
        return jsonify({'success': False, 'error': f'æƒé™é”™è¯¯ï¼Œæ— æ³•è®¿é—®æ–‡ä»¶å¤¹: {e}'}), 403
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å¤¹æ•°æ®å¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'}), 500


def process_folder_domain_detection(folder_path, llm_client, force_reprocess=False):
    """å¤„ç†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSONLæ–‡ä»¶çš„é¢†åŸŸæ ‡ç­¾æ£€æµ‹"""
    import os
    import json
    from datetime import datetime
    
    try:
        # åˆå§‹åŒ–æ ‡ç­¾ä¿¡æ¯ç®¡ç†å™¨
        tag_manager = DomainTagManager(folder_path)
        
        # è·å–æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSONLæ–‡ä»¶
        jsonl_files = []
        for file in os.listdir(folder_path):
            if file.endswith('.jsonl') and not file.endswith('_with_tags.jsonl'):
                jsonl_files.append(file)
        
        if not jsonl_files:
            return {
                'message': 'æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°JSONLæ–‡ä»¶',
                'processed_files': 0,
                'total_items': 0
            }
        
        logger.info(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ªJSONLæ–‡ä»¶")
        
        total_processed_items = 0
        processed_files = 0
        cleared_files = 0
        
        for filename in jsonl_files:
            file_path = os.path.join(folder_path, filename)
            
                            # å¦‚æœæ˜¯å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Œå…ˆæ¸…ç†æ—§æ•°æ®ï¼ˆæ— è®ºä¹‹å‰æ˜¯å¦å¤„ç†è¿‡ï¼‰
            if force_reprocess:
                if filename in tag_manager.info['file_processing_status']:
                    logger.info(f"å¼ºåˆ¶é‡æ–°å¤„ç†æ–‡ä»¶: {filename}ï¼Œå…ˆæ¸…ç†æ—§çš„å¤„ç†æ•°æ®")
                    tag_manager.clear_file_processing_data(filename)
                    cleared_files += 1
                    
                    # åˆ é™¤æ—§çš„with_tagsæ–‡ä»¶
                    tagged_dir = os.path.join(folder_path, 'tagged')
                    output_filename = filename.replace('.jsonl', '_with_tags.jsonl')
                    output_path = os.path.join(tagged_dir, output_filename)
                    if os.path.exists(output_path):
                        try:
                            os.remove(output_path)
                            logger.info(f"å·²åˆ é™¤æ—§çš„æ ‡ç­¾æ–‡ä»¶: {output_path}")
                        except Exception as e:
                            logger.warning(f"åˆ é™¤æ—§æ ‡ç­¾æ–‡ä»¶å¤±è´¥: {e}")
                else:
                    logger.info(f"å¼ºåˆ¶é‡æ–°å¤„ç†æ–‡ä»¶: {filename}ï¼Œæ–‡ä»¶ä¹‹å‰æœªå¤„ç†è¿‡")
            else:
                # éå¼ºåˆ¶é‡æ–°å¤„ç†æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¯¥æ–‡ä»¶
                if tag_manager.is_file_processed(filename, file_path):
                    logger.info(f"è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {filename}")
                    continue
            
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")
            
            # åŠ è½½æ–‡ä»¶æ•°æ®
            items = []
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            items.append(json.loads(line.strip()))
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                continue
            
            if not items:
                logger.info(f"æ–‡ä»¶ {filename} ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # å¤„ç†æ–‡ä»¶ä¸­çš„æ•°æ®
            try:
                # è·å–å½“å‰æœ€æ–°çš„æ ‡ç­¾é›†åˆ
                current_tags = tag_manager.get_all_tags()
                
                # åˆ†æ‰¹å¤„ç†æ•°æ®
                results = process_batch_domain_detection_with_manager(
                    items, current_tags, llm_client, tag_manager
                )
                
                # åˆ›å»ºtaggedå­ç›®å½•
                tagged_dir = os.path.join(folder_path, 'tagged')
                os.makedirs(tagged_dir, exist_ok=True)
                
                # ä¿å­˜å¸¦æ ‡ç­¾çš„ç»“æœæ–‡ä»¶åˆ°å­ç›®å½•
                output_filename = filename.replace('.jsonl', '_with_tags.jsonl')
                output_path = os.path.join(tagged_dir, output_filename)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    for i, item in enumerate(items):
                        # æ·»åŠ é¢†åŸŸæ ‡ç­¾åˆ°åŸæ•°æ®
                        enhanced_item = item.copy()
                        if i < len(results):
                            enhanced_item['domain_tags'] = results[i].get('domain_tags', [])
                        else:
                            enhanced_item['domain_tags'] = []
                        f.write(json.dumps(enhanced_item, ensure_ascii=False) + '\n')
                
                # æ›´æ–°å¤„ç†çŠ¶æ€
                tag_manager.mark_file_processed(filename, len(items), results)
                
                total_processed_items += len(items)
                processed_files += 1
                
                if force_reprocess:
                    logger.info(f"æ–‡ä»¶ {filename} é‡æ–°å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(items)} æ¡æ•°æ®ï¼Œå·²è¦†ç›–æ—§çš„æ ‡ç­¾æ–‡ä»¶")
                else:
                    logger.info(f"æ–‡ä»¶ {filename} å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(items)} æ¡æ•°æ®")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                continue
        
        # å¦‚æœæ˜¯å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Œé‡æ–°è®¡ç®—æ‰€æœ‰æ ‡ç­¾ç»Ÿè®¡ç¡®ä¿å‡†ç¡®æ€§
        if force_reprocess:
            logger.info("å¼ºåˆ¶é‡æ–°å¤„ç†å®Œæˆï¼Œé‡æ–°è®¡ç®—æ‰€æœ‰æ ‡ç­¾ç»Ÿè®¡")
            tag_manager.recalculate_all_tags()
            logger.info(f"å¼ºåˆ¶é‡æ–°å¤„ç†æ¦‚è¦: æ¸…ç†äº† {cleared_files} ä¸ªæ–‡ä»¶çš„æ—§æ•°æ®ï¼Œé‡æ–°å¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶ï¼Œæ¸…ç†äº†æ— ç”¨çš„æ ‡ç­¾")
        
        # ä¿å­˜æœ€ç»ˆçš„æ ‡ç­¾ä¿¡æ¯
        tag_manager.save_info()
        
        mode_message = "é‡æ–°å¤„ç†" if force_reprocess else "å¤„ç†"
        result_data = {
            'message': f'{mode_message}å®Œæˆ',
            'processed_files': processed_files,
            'total_files': len(jsonl_files),
            'total_items': total_processed_items,
            'tags_count': len(tag_manager.get_all_tags()),
            'force_reprocess': force_reprocess
        }
        
        if force_reprocess:
            result_data['cleared_files'] = cleared_files
            # æ·»åŠ æ ‡ç­¾ä½¿ç”¨æƒ…å†µç»Ÿè®¡
            active_tags = sum(1 for tag_info in tag_manager.info['tags'].values() if tag_info.get('count', 0) > 0)
            result_data['active_tags'] = active_tags
            result_data['empty_tags'] = len(tag_manager.get_all_tags()) - active_tags
            
        return result_data
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥: {e}")
        raise


class DomainTagManager:
    """é¢†åŸŸæ ‡ç­¾ä¿¡æ¯ç®¡ç†å™¨"""
    
    def __init__(self, folder_path):
        self.folder_path = folder_path
        self.info_file = os.path.join(folder_path, 'domain_tags_info.json')
        self.info = self._load_info()
    
    def _load_info(self):
        """åŠ è½½æ ‡ç­¾ä¿¡æ¯"""
        if os.path.exists(self.info_file):
            try:
                with open(self.info_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤ç»“æ„
        return {
            'tags': {},
            'total_processed': 0,
            'last_updated': None,
            'file_processing_status': {}
        }
    
    def save_info(self):
        """ä¿å­˜æ ‡ç­¾ä¿¡æ¯"""
        from datetime import datetime
        self.info['last_updated'] = datetime.now().isoformat()
        
        # æœ€ç»ˆéªŒè¯æ•°æ®ä¸€è‡´æ€§
        total_tag_count = sum(tag_info.get('count', 0) for tag_info in self.info['tags'].values())
        total_processed = self.info.get('total_processed', 0)
        
        if total_tag_count != total_processed and total_processed > 0:
            logger.warning(f"ä¿å­˜å‰å‘ç°æ•°æ®ä¸ä¸€è‡´: æ ‡ç­¾æ€»è®¡æ•°({total_tag_count}) != æ¡ç›®æ€»æ•°({total_processed})")
            # å¯é€‰ï¼šè‡ªåŠ¨ä¿®æ­£
            logger.info("æ­£åœ¨ä¿®æ­£total_processed...")
            self.info['total_processed'] = total_tag_count
        
        try:
            with open(self.info_file, 'w', encoding='utf-8') as f:
                json.dump(self.info, f, ensure_ascii=False, indent=2)
            logger.info(f"æ ‡ç­¾ä¿¡æ¯å·²ä¿å­˜: {len(self.info['tags'])} ä¸ªæ ‡ç­¾ï¼Œ{self.info['total_processed']} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"ä¿å­˜æ ‡ç­¾ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_all_tags(self):
        """è·å–æ‰€æœ‰æ ‡ç­¾åˆ—è¡¨"""
        return list(self.info['tags'].keys())
    
    def add_tag(self, tag_name, description=None):
        """æ·»åŠ æ–°æ ‡ç­¾"""
        if tag_name not in self.info['tags']:
            self.info['tags'][tag_name] = {
                'count': 0,
                'description': description or ''
            }
    
    def update_tag_count(self, tag_name, count_increment=1):
        """æ›´æ–°æ ‡ç­¾è®¡æ•°"""
        if tag_name in self.info['tags']:
            self.info['tags'][tag_name]['count'] += count_increment
        else:
            self.add_tag(tag_name)
            self.info['tags'][tag_name]['count'] = count_increment
    
    def is_file_processed(self, filename, file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        if filename not in self.info['file_processing_status']:
            return False
        
        file_status = self.info['file_processing_status'][filename]
        if not file_status.get('processed', False):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹
        try:
            from datetime import datetime
            file_modified_time = datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
            recorded_modified_time = file_status.get('file_modified', '')
            
            if file_modified_time != recorded_modified_time:
                logger.info(f"æ–‡ä»¶ {filename} å·²è¢«ä¿®æ”¹ï¼Œéœ€è¦é‡æ–°å¤„ç†")
                return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´å¤±è´¥: {e}")
            return False
        
        return True
    
    def clear_file_processing_data(self, filename):
        """æ¸…é™¤æ–‡ä»¶çš„å¤„ç†æ•°æ®ï¼ˆç”¨äºé‡æ–°å¤„ç†ï¼‰"""
        if filename in self.info['file_processing_status']:
            file_status = self.info['file_processing_status'][filename]
            
            # ä»æ€»å¤„ç†æ•°ä¸­å‡å»è¯¥æ–‡ä»¶çš„å¤„ç†æ•°é‡
            if 'processed_count' in file_status:
                self.info['total_processed'] -= file_status['processed_count']
                if self.info['total_processed'] < 0:
                    self.info['total_processed'] = 0
            
            # å°è¯•ä»with_tagsæ–‡ä»¶ä¸­ç²¾ç¡®è®¡ç®—è¯¥æ–‡ä»¶çš„æ ‡ç­¾è´¡çŒ®
            try:
                tagged_file_path = os.path.join(self.folder_path, 'tagged', filename.replace('.jsonl', '_with_tags.jsonl'))
                if os.path.exists(tagged_file_path):
                    logger.info(f"ä»with_tagsæ–‡ä»¶ç²¾ç¡®è®¡ç®—æ–‡ä»¶ {filename} çš„æ ‡ç­¾è®¡æ•°")
                    with open(tagged_file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                item = json.loads(line.strip())
                                domain_tags = item.get('domain_tags', [])
                                if isinstance(domain_tags, list):
                                    for tag in domain_tags:
                                        if tag in self.info['tags']:
                                            old_count = self.info['tags'][tag]['count']
                                            self.info['tags'][tag]['count'] -= 1
                                            if self.info['tags'][tag]['count'] < 0:
                                                self.info['tags'][tag]['count'] = 0
                                            logger.debug(f"æ ‡ç­¾ '{tag}' è®¡æ•°: {old_count} -> {self.info['tags'][tag]['count']}")
                    logger.info(f"å·²ä»with_tagsæ–‡ä»¶ç²¾ç¡®å‡å»æ–‡ä»¶ {filename} çš„æ ‡ç­¾è®¡æ•°")
                else:
                    # å¦‚æœwith_tagsæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                    file_tags = file_status.get('tags', [])
                    if file_tags and file_status.get('processed_count', 0) > 0:
                        avg_per_tag = file_status['processed_count'] // len(file_tags)
                        logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•å‡å»æ–‡ä»¶ {filename} çš„æ ‡ç­¾è®¡æ•°ï¼Œæ¯ä¸ªæ ‡ç­¾å¹³å‡: {avg_per_tag}")
                        for tag in file_tags:
                            if tag in self.info['tags']:
                                old_count = self.info['tags'][tag]['count']
                                self.info['tags'][tag]['count'] -= avg_per_tag
                                if self.info['tags'][tag]['count'] < 0:
                                    self.info['tags'][tag]['count'] = 0
                                logger.debug(f"æ ‡ç­¾ '{tag}' è®¡æ•°: {old_count} -> {self.info['tags'][tag]['count']}")
                    logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•å®Œæˆæ–‡ä»¶ {filename} çš„æ ‡ç­¾è®¡æ•°æ¸…ç†")
            except Exception as e:
                logger.warning(f"æ¸…é™¤æ–‡ä»¶ {filename} çš„æ ‡ç­¾è®¡æ•°æ—¶å‡ºé”™: {e}")
            
            # æ¸…é™¤æ–‡ä»¶å¤„ç†çŠ¶æ€
            del self.info['file_processing_status'][filename]
            logger.info(f"å·²æ¸…é™¤æ–‡ä»¶ {filename} çš„å¤„ç†æ•°æ®ï¼Œå‡†å¤‡é‡æ–°å¤„ç†")

    def mark_file_processed(self, filename, processed_count, file_tags=None):
        """æ ‡è®°æ–‡ä»¶å·²å¤„ç†"""
        from datetime import datetime
        file_path = os.path.join(self.folder_path, filename)
        try:
            file_modified_time = datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
        except Exception:
            file_modified_time = datetime.now().isoformat()
        
        # æ”¶é›†æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ ‡ç­¾
        all_file_tags = set()
        if file_tags:
            for result in file_tags:
                tags = result.get('domain_tags', [])
                if isinstance(tags, list):
                    all_file_tags.update(tags)
        
        self.info['file_processing_status'][filename] = {
            'processed': True,
            'processed_count': processed_count,
            'last_processed': datetime.now().isoformat(),
            'file_modified': file_modified_time,
            'tags': list(all_file_tags)  # ä¿å­˜æ–‡ä»¶åŒ…å«çš„æ‰€æœ‰æ ‡ç­¾
        }
        
        self.info['total_processed'] += processed_count

    def recalculate_all_tags(self):
        """é‡æ–°è®¡ç®—æ‰€æœ‰æ ‡ç­¾ç»Ÿè®¡ï¼ˆç”¨äºå¼ºåˆ¶é‡æ–°å¤„ç†åç¡®ä¿æ•°æ®å‡†ç¡®æ€§ï¼‰"""
        logger.info("å¼€å§‹é‡æ–°è®¡ç®—æ‰€æœ‰æ ‡ç­¾ç»Ÿè®¡...")
        
        # æ¸…ç©ºæ‰€æœ‰ç°æœ‰æ ‡ç­¾ï¼Œé‡æ–°å¼€å§‹
        old_tags = list(self.info['tags'].keys())
        self.info['tags'] = {}
        
        # åˆå§‹åŒ–é¢„å®šä¹‰æ ‡ç­¾ï¼ˆç¡®ä¿æ‰€æœ‰é¢„å®šä¹‰æ ‡ç­¾éƒ½å­˜åœ¨ï¼Œå³ä½¿è®¡æ•°ä¸º0ï¼‰
        predefined_tags = {'ä½“è‚²', 'å­¦æœ¯', 'æ”¿æ²»', 'å¨±ä¹', 'æ–‡å­¦', 'æ–‡åŒ–', 'ç»æµ', 'ç§‘æŠ€', 'å†å²', 'åŒ»ç–—', 'å…¶ä»–'}
        for tag in predefined_tags:
            self.add_tag(tag)
        
        # é‡ç½®æ€»å¤„ç†æ•°
        self.info['total_processed'] = 0
        
        # éå†æ‰€æœ‰å·²å¤„ç†çš„æ–‡ä»¶ï¼Œé‡æ–°è®¡ç®—
        tagged_dir = os.path.join(self.folder_path, 'tagged')
        if os.path.exists(tagged_dir):
            recalculated_files = 0
            found_tags = set()
            
            for filename, file_status in self.info['file_processing_status'].items():
                if file_status.get('processed', False):
                    tagged_filename = filename.replace('.jsonl', '_with_tags.jsonl')
                    tagged_file_path = os.path.join(tagged_dir, tagged_filename)
                    
                    if os.path.exists(tagged_file_path):
                        try:
                            file_item_count = 0
                            file_tags = set()
                            
                            with open(tagged_file_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    if line.strip():
                                        file_item_count += 1
                                        item = json.loads(line.strip())
                                        domain_tags = item.get('domain_tags', [])
                                        if isinstance(domain_tags, list):
                                            # åœ¨å•æ ‡ç­¾æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªæ¡ç›®åº”è¯¥åªæœ‰ä¸€ä¸ªæ ‡ç­¾
                                            if len(domain_tags) > 1:
                                                logger.warning(f"å‘ç°å¤šæ ‡ç­¾æ¡ç›® {domain_tags}ï¼Œå°†åªä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ ‡ç­¾")
                                            
                                            # åªé€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é¢„å®šä¹‰æ ‡ç­¾
                                            selected_tag = None
                                            for tag in domain_tags:
                                                found_tags.add(tag)
                                                if tag in predefined_tags:
                                                    selected_tag = tag
                                                    break
                                                else:
                                                    logger.warning(f"å‘ç°éé¢„å®šä¹‰æ ‡ç­¾ '{tag}'")
                                            
                                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œä½¿ç”¨"å…¶ä»–"
                                            if not selected_tag:
                                                selected_tag = 'å…¶ä»–'
                                            
                                            # åªä¸ºé€‰ä¸­çš„æ ‡ç­¾è®¡æ•°ä¸€æ¬¡
                                            file_tags.add(selected_tag)
                                            if selected_tag in self.info['tags']:
                                                self.info['tags'][selected_tag]['count'] += 1
                                            else:
                                                # å¦‚æœæ ‡ç­¾ä¸åœ¨infoä¸­ï¼Œæ·»åŠ å®ƒ
                                                self.add_tag(selected_tag)
                                                self.info['tags'][selected_tag]['count'] += 1
                            
                            # æ›´æ–°æ–‡ä»¶çš„å¤„ç†æ•°é‡å’Œæ ‡ç­¾åˆ—è¡¨
                            file_status['processed_count'] = file_item_count
                            file_status['tags'] = list(file_tags)
                            self.info['total_processed'] += file_item_count
                            recalculated_files += 1
                            
                            logger.debug(f"æ–‡ä»¶ {filename}: {file_item_count} æ¡æ•°æ®ï¼Œæ ‡ç­¾: {sorted(file_tags)}")
                            
                        except Exception as e:
                            logger.warning(f"é‡æ–°è®¡ç®—æ–‡ä»¶ {filename} çš„æ ‡ç­¾ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
                    else:
                        logger.warning(f"æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {tagged_file_path}")
            
            # æ¸…ç†ä¸å†ä½¿ç”¨çš„æ ‡ç­¾ï¼ˆä¸åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­çš„æ ‡ç­¾ï¼‰
            tags_to_remove = []
            for tag_name in self.info['tags']:
                if tag_name not in predefined_tags:
                    tags_to_remove.append(tag_name)
            
            if tags_to_remove:
                logger.info(f"æ¸…ç†éé¢„å®šä¹‰æ ‡ç­¾: {tags_to_remove}")
                for tag_name in tags_to_remove:
                    old_count = self.info['tags'][tag_name].get('count', 0)
                    del self.info['tags'][tag_name]
                    logger.debug(f"å·²åˆ é™¤æ ‡ç­¾ '{tag_name}' (åŸè®¡æ•°: {old_count})")
            else:
                logger.info("æ²¡æœ‰éœ€è¦æ¸…ç†çš„éé¢„å®šä¹‰æ ‡ç­¾")
            
            logger.info(f"é‡æ–°è®¡ç®—å®Œæˆï¼Œå¤„ç†äº† {recalculated_files} ä¸ªæ–‡ä»¶çš„æ ‡ç­¾ç»Ÿè®¡")
            logger.info(f"å‘ç°çš„æ ‡ç­¾: {sorted(found_tags)}")
            logger.info(f"æ¸…ç†äº† {len(old_tags) - len(self.info['tags'])} ä¸ªæ—§æ ‡ç­¾")
            logger.info(f"æ€»å¤„ç†æ¡ç›®æ•°: {self.info['total_processed']}")
            
            # æ‰“å°æœ€ç»ˆçš„æ ‡ç­¾ç»Ÿè®¡ï¼ˆåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ ‡ç­¾ï¼‰
            active_tags = 0
            total_tag_count = 0
            for tag_name, tag_info in sorted(self.info['tags'].items()):
                count = tag_info['count']
                total_tag_count += count
                if count > 0:
                    percentage = (count / self.info['total_processed'] * 100) if self.info['total_processed'] > 0 else 0
                    logger.info(f"æ ‡ç­¾ '{tag_name}': {count} æ¬¡ ({percentage:.1f}%)")
                    active_tags += 1
                else:
                    logger.debug(f"æ ‡ç­¾ '{tag_name}': {count} æ¬¡ (æ— æ•°æ®)")
            
            logger.info(f"å…± {active_tags} ä¸ªæ ‡ç­¾æœ‰æ•°æ®ï¼Œ{len(self.info['tags']) - active_tags} ä¸ªæ ‡ç­¾æ— æ•°æ®")
            logger.info(f"æ ‡ç­¾æ€»è®¡æ•°: {total_tag_count}ï¼Œæ¡ç›®æ€»æ•°: {self.info['total_processed']}")
            
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            if total_tag_count != self.info['total_processed']:
                logger.warning(f"æ•°æ®ä¸ä¸€è‡´ï¼æ ‡ç­¾æ€»è®¡æ•°({total_tag_count}) != æ¡ç›®æ€»æ•°({self.info['total_processed']})")
            else:
                logger.info("æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        else:
            logger.warning(f"æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {tagged_dir}")


def process_batch_domain_detection_with_manager(items, existing_tags, llm_client, tag_manager):
    """å¸¦æ ‡ç­¾ç®¡ç†å™¨çš„æ‰¹é‡é¢†åŸŸæ ‡ç­¾æ£€æµ‹"""
    try:
        max_chars_per_batch = 70000  # æ¯æ‰¹æœ€å¤§å­—ç¬¦æ•°
        all_results = []
        
        logger.info(f"åˆ†æ‰¹å¤„ç† {len(items)} ä¸ªæ•°æ®é¡¹ï¼Œæ¯æ‰¹æœ€å¤§ {max_chars_per_batch} å­—ç¬¦")
        
        current_batch = []
        current_batch_chars = 0
        batch_count = 0
        
        for i, item in enumerate(items):
            question = item.get('question', '')
            reasoning_path = item.get('mapped_reasoning_path', '')
            
            # è®¡ç®—å½“å‰é¡¹ç›®çš„å­—ç¬¦æ•°ï¼ˆä¸æˆªæ–­ï¼‰
            item_chars = len(question) + len(reasoning_path)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°æ‰¹æ¬¡
            if current_batch and (current_batch_chars + item_chars > max_chars_per_batch):
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_count += 1
                logger.info(f"å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼ŒåŒ…å« {len(current_batch)} ä¸ªé¡¹ç›®ï¼Œæ€»å­—ç¬¦æ•°: {current_batch_chars}")
                
                # è·å–æœ€æ–°çš„æ ‡ç­¾é›†åˆï¼ˆæ¯æ‰¹æ¬¡éƒ½è·å–æœ€æ–°çš„ï¼‰
                current_tags = tag_manager.get_all_tags()
                batch_results = _process_batch_with_manager(current_batch, current_tags, llm_client, tag_manager, batch_count)
                all_results.extend(batch_results)
                
                # é‡ç½®æ‰¹æ¬¡
                current_batch = []
                current_batch_chars = 0
            
            # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
            current_batch.append({
                'item': item,
                'global_index': i,
                'question': question,
                'reasoning_path': reasoning_path
            })
            current_batch_chars += item_chars
        
        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            batch_count += 1
            logger.info(f"å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼ŒåŒ…å« {len(current_batch)} ä¸ªé¡¹ç›®ï¼Œæ€»å­—ç¬¦æ•°: {current_batch_chars}")
            current_tags = tag_manager.get_all_tags()
            batch_results = _process_batch_with_manager(current_batch, current_tags, llm_client, tag_manager, batch_count)
            all_results.extend(batch_results)
        
        # æŒ‰ç´¢å¼•æ’åºç»“æœ
        all_results.sort(key=lambda x: x['index'])
        
        # ç¡®ä¿æ‰€æœ‰é¡¹ç›®éƒ½æœ‰ç»“æœï¼Œç¼ºå¤±çš„æ ‡è®°ä¸º"å…¶ä»–"
        for i in range(len(items)):
            if not any(result['index'] == i for result in all_results):
                all_results.append({'index': i, 'domain_tags': ['å…¶ä»–']})
                tag_manager.update_tag_count('å…¶ä»–')
        
        # å†æ¬¡æ’åº
        all_results.sort(key=lambda x: x['index'])
        
        return all_results
        
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†å‡ºé”™: {e}")
        # è¿”å›é»˜è®¤ç»“æœï¼ˆæ ‡è®°ä¸º"å…¶ä»–"ï¼‰
        for i in range(len(items)):
            tag_manager.update_tag_count('å…¶ä»–')
        return [{'index': i, 'domain_tags': ['å…¶ä»–']} for i in range(len(items))]


def _process_batch_with_manager(batch_items, existing_tags, llm_client, tag_manager, batch_number):
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡å¹¶æ›´æ–°æ ‡ç­¾ç®¡ç†å™¨"""
    try:
        # å‡†å¤‡å½“å‰æ‰¹æ¬¡çš„æ–‡æœ¬
        batch_texts = []
        for j, batch_item in enumerate(batch_items):
            batch_texts.append({
                'index': j,  # æ‰¹æ¬¡å†…çš„ç´¢å¼•
                'global_index': batch_item['global_index'],  # å…¨å±€ç´¢å¼•
                'question': batch_item['question'],
                'reasoning_path': batch_item['reasoning_path']
            })
        
        # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„prompt
        existing_tags_str = ', '.join(sorted(existing_tags)) if existing_tags else 'æ— '
        
        prompt_parts = [
            "è¯·åˆ†æä»¥ä¸‹é—®é¢˜å’Œæ¨ç†è·¯å¾„ï¼Œä¸ºæ¯ä¸ªé—®é¢˜è¯†åˆ«å¯¹åº”çš„é¢†åŸŸæ ‡ç­¾ã€‚",
            "",
            "è¯·æ ¹æ®é—®é¢˜å†…å®¹å’Œæ¨ç†è¿‡ç¨‹ï¼Œä¸ºæ¯ä¸ªé—®é¢˜é€‰æ‹©ä¸€ä¸ªæœ€ç¬¦åˆçš„é¢†åŸŸæ ‡ç­¾ã€‚",
            "è¯·åªä»ä»¥ä¸‹é¢„å®šä¹‰çš„é¢†åŸŸæ ‡ç­¾ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ç¬¦åˆçš„æ ‡ç­¾ï¼Œä¸è¦åˆ›å»ºæ–°æ ‡ç­¾ï¼š",
            "",
            "å¯é€‰æ ‡ç­¾ï¼š",
            "- ä½“è‚²ï¼šè¿åŠ¨ç«æŠ€ã€ä½“è‚²èµ›äº‹ã€å¥èº«é”»ç‚¼",
            "- å­¦æœ¯ï¼šç§‘å­¦ç ”ç©¶ã€å­¦æœ¯ç†è®ºã€æ•™è‚²çŸ¥è¯†",
            "- æ”¿æ²»ï¼šæ”¿åºœæ”¿ç­–ã€æ”¿æ²»åˆ¶åº¦ã€å›½é™…å…³ç³»",
            "- å¨±ä¹ï¼šå½±è§†éŸ³ä¹ã€æ¸¸æˆå¨±ä¹ã€æ˜æ˜Ÿå…«å¦",
            "- æ–‡å­¦ï¼šæ–‡å­¦ä½œå“ã€è¯—æ­Œæ•£æ–‡ã€æ–‡å­¦åˆ›ä½œ",
            "- æ–‡åŒ–ï¼šä¼ ç»Ÿæ–‡åŒ–ã€è‰ºæœ¯æ–‡åŒ–ã€ç¤¾ä¼šæ–‡åŒ–",
            "- ç»æµï¼šå•†ä¸šé‡‘èã€å¸‚åœºç»æµã€æŠ•èµ„ç†è´¢",
            "- ç§‘æŠ€ï¼šè®¡ç®—æœºæŠ€æœ¯ã€äººå·¥æ™ºèƒ½ã€ç§‘æŠ€äº§å“",
            "- å†å²ï¼šå†å²äº‹ä»¶ã€å†å²äººç‰©ã€å†å²ç ”ç©¶",
            "- åŒ»ç–—ï¼šåŒ»å­¦æ²»ç–—ã€å¥åº·å…»ç”Ÿã€ç–¾ç—…è¯Šæ–­",
            "- å…¶ä»–ï¼šä¸å±äºä»¥ä¸Šä»»ä½•é¢†åŸŸçš„é—®é¢˜",
            "",
            "åˆ†ç±»è§„åˆ™ï¼š",
            "1. å¤šé¢†åŸŸç›¸å…³æ—¶ï¼šé€‰æ‹©é—®é¢˜æ ¸å¿ƒå†…å®¹å’Œä¸»è¦ç›®çš„æœ€ç›¸å…³çš„é¢†åŸŸ",
            "2. ç›¸å…³æ€§åˆ¤æ–­ï¼šåªæœ‰å½“é—®é¢˜ä¸æŸä¸ªé¢†åŸŸæœ‰æ˜ç¡®ã€ç›´æ¥çš„å…³è”æ—¶æ‰é€‰æ‹©è¯¥é¢†åŸŸ",
            "3. é¿å…å¼ºè¡Œå½’ç±»ï¼šå¦‚æœé—®é¢˜ä¸æ‰€æœ‰é¢„å®šä¹‰é¢†åŸŸçš„ç›¸å…³æ€§éƒ½å¾ˆå¼±ï¼Œæœæ–­é€‰æ‹©'å…¶ä»–'",
            "4. ä¼˜å…ˆçº§åˆ¤æ–­ï¼šé—®é¢˜çš„ä¸»è¦è®¨è®ºç„¦ç‚¹ > èƒŒæ™¯ä¿¡æ¯ > é™„å¸¦æåŠçš„å†…å®¹",
            "",
            "ç¤ºä¾‹ï¼š",
            "- 'NBAçƒå‘˜çš„è¥å…»é¥®é£Ÿå»ºè®®' â†’ ä½“è‚²ï¼ˆæ ¸å¿ƒæ˜¯ä½“è‚²ç›¸å…³çš„é¥®é£Ÿï¼Œè€Œéä¸€èˆ¬åŒ»ç–—ï¼‰",
            "- 'å¦‚ä½•ç”¨Pythonåˆ†æè‚¡ç¥¨æ•°æ®' â†’ ç§‘æŠ€ï¼ˆæ ¸å¿ƒæ˜¯ç¼–ç¨‹æŠ€æœ¯ï¼Œè€Œéé‡‘èåˆ†æï¼‰",
            "- 'æ˜æ˜Ÿåœ¨ç”µå½±ä¸­çš„å†å²æœè£…' â†’ å¨±ä¹ï¼ˆæ ¸å¿ƒæ˜¯å½±è§†å†…å®¹ï¼Œå†å²åªæ˜¯èƒŒæ™¯ï¼‰",
            "- 'ä»Šå¤©å¤©æ°”çœŸå¥½' â†’ å…¶ä»–ï¼ˆä¸æ‰€æœ‰é¢†åŸŸç›¸å…³æ€§éƒ½å¾ˆå¼±ï¼‰",
            "",
            "æ•°æ®åˆ—è¡¨ï¼š\n"
        ]
        
        for text_item in batch_texts:
            prompt_parts.append(f"[{text_item['index']}] é—®é¢˜: {text_item['question']}")
            if text_item['reasoning_path']:
                prompt_parts.append(f"[{text_item['index']}] æ¨ç†: {text_item['reasoning_path']}")
            prompt_parts.append("")
        
        prompt_parts.append("""
è¯·ä¸ºæ¯ä¸ªç´¢å¼•è¿”å›é¢†åŸŸæ ‡ç­¾è¯†åˆ«ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {"index": 0, "domain_tags": ["ä½“è‚²"]},
  {"index": 1, "domain_tags": ["åŒ»ç–—"]},
  {"index": 2, "domain_tags": ["å…¶ä»–"]}
]

é‡è¦æé†’ï¼š
- åªèƒ½ä½¿ç”¨ä»¥ä¸Šåˆ—å‡ºçš„11ä¸ªæ ‡ç­¾ï¼šä½“è‚²ã€å­¦æœ¯ã€æ”¿æ²»ã€å¨±ä¹ã€æ–‡å­¦ã€æ–‡åŒ–ã€ç»æµã€ç§‘æŠ€ã€å†å²ã€åŒ»ç–—ã€å…¶ä»–
- æ¯ä¸ªé—®é¢˜åªèƒ½é€‰æ‹©ä¸€ä¸ªæœ€ç¬¦åˆçš„æ ‡ç­¾
- å¤šé¢†åŸŸäº¤å‰æ—¶ï¼Œé€‰æ‹©é—®é¢˜æ ¸å¿ƒå†…å®¹æœ€ç›¸å…³çš„é‚£ä¸ªé¢†åŸŸ
- ç›¸å…³æ€§ä¸è¶³æ—¶ï¼Œå®å¯é€‰æ‹©"å…¶ä»–"ä¹Ÿä¸è¦å¼ºè¡Œå½’ç±»åˆ°ä¸å¤ªç›¸å…³çš„é¢†åŸŸ
- å¿…é¡»ä»é¢„å®šä¹‰åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œåªè¿”å›ä¸­æ–‡æ ‡ç­¾åç§°

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""")
        
        full_prompt = '\n'.join(prompt_parts)
        logger.info(f"ç¬¬ {batch_number} æ‰¹æ¬¡åŒ…å« {len(batch_texts)} ä¸ªé—®é¢˜ï¼Œç´¢å¼•èŒƒå›´: {[item['global_index'] for item in batch_texts]}ï¼Œå•æ ‡ç­¾æ¨¡å¼: {sorted(PREDEFINED_DOMAIN_TAGS)}")
        
        batch_results = []
        
        try:
            import asyncio
            response = asyncio.run(llm_client.generate_response(full_prompt))
            
            # è§£æå½“å‰æ‰¹æ¬¡çš„ç»“æœ
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                detection_results = json.loads(json_str)
                
            for result in detection_results:
                if isinstance(result, dict) and 'index' in result:
                    batch_index = result['index']
                    if 0 <= batch_index < len(batch_texts):
                        global_index = batch_texts[batch_index]['global_index']
                        tags = result.get('domain_tags', [])
                        if isinstance(tags, list):
                            # ä½¿ç”¨é¢„å®šä¹‰çš„æ ‡ç­¾åˆ—è¡¨
                            valid_tags = PREDEFINED_DOMAIN_TAGS
                            
                            # å¦‚æœæ¨¡å‹è¿”å›äº†å¤šä¸ªæ ‡ç­¾ï¼Œè®°å½•è­¦å‘Š
                            if len(tags) > 1:
                                logger.warning(f"æ¨¡å‹è¿”å›äº†å¤šä¸ªæ ‡ç­¾ {tags}ï¼Œå°†åªé€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ ‡ç­¾")
                            
                            selected_tag = None
                            
                            # åªé€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„æ ‡ç­¾
                            for tag in tags:
                                if isinstance(tag, str) and tag.strip():
                                    clean_tag = tag.strip()
                                    # éªŒè¯æ ‡ç­¾æ˜¯å¦åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­
                                    if clean_tag in valid_tags:
                                        selected_tag = clean_tag
                                        break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ ‡ç­¾å°±åœæ­¢
                                    else:
                                        # å¦‚æœæ ‡ç­¾ä¸åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­ï¼Œè®°å½•æ—¥å¿—
                                        logger.warning(f"æ£€æµ‹åˆ°æœªé¢„å®šä¹‰æ ‡ç­¾ '{clean_tag}'ï¼Œå°†å¯»æ‰¾å…¶ä»–æœ‰æ•ˆæ ‡ç­¾æˆ–ä½¿ç”¨'å…¶ä»–'")
                            
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œé»˜è®¤ä½¿ç”¨"å…¶ä»–"
                            if not selected_tag:
                                selected_tag = 'å…¶ä»–'
                                logger.info(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œé»˜è®¤ä½¿ç”¨'å…¶ä»–'")
                            
                            clean_tags = [selected_tag]
                            # å®æ—¶æ›´æ–°æ ‡ç­¾ç®¡ç†å™¨
                            tag_manager.update_tag_count(selected_tag)
                            
                            # åªä¸ºæ¯ä¸ªé—®é¢˜æ·»åŠ ä¸€æ¬¡ç»“æœï¼Œä¸æ˜¯æ¯ä¸ªæ ‡ç­¾æ·»åŠ ä¸€æ¬¡
                            batch_results.append({
                                'index': global_index,
                                'domain_tags': clean_tags
                            })
                            logger.debug(f"æ‰¹æ¬¡ {batch_number}: ä¸ºå…¨å±€ç´¢å¼• {global_index} é€‰æ‹©æ ‡ç­¾ '{selected_tag}'")
                    else:
                        logger.warning(f"æ‰¹æ¬¡ç´¢å¼• {batch_index} è¶…å‡ºèŒƒå›´ï¼Œæ‰¹æ¬¡å¤§å°: {len(batch_texts)}")
        
        except Exception as e:
            logger.error(f"å¤„ç†ç¬¬ {batch_number} æ‰¹æ—¶å‡ºé”™: {e}")
            # ä¸ºå½“å‰æ‰¹æ¬¡æ·»åŠ é»˜è®¤ç»“æœï¼ˆæ ‡è®°ä¸º"å…¶ä»–"ï¼‰
            for batch_item in batch_items:
                batch_results.append({
                    'index': batch_item['global_index'],
                    'domain_tags': ['å…¶ä»–']
                })
                tag_manager.update_tag_count('å…¶ä»–')
        
        logger.info(f"æ‰¹æ¬¡ {batch_number} å¤„ç†å®Œæˆï¼Œè¾“å…¥ {len(batch_texts)} ä¸ªé—®é¢˜ï¼Œè¾“å‡º {len(batch_results)} ä¸ªç»“æœ")
        return batch_results

    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ {batch_number} å¤„ç†å¼‚å¸¸: {e}")
        # ä¸ºå½“å‰æ‰¹æ¬¡æ·»åŠ é»˜è®¤ç»“æœï¼ˆæ ‡è®°ä¸º"å…¶ä»–"ï¼‰
        batch_results = []
        for batch_item in batch_items:
            batch_results.append({
                'index': batch_item['global_index'],
                'domain_tags': ['å…¶ä»–']
            })
            tag_manager.update_tag_count('å…¶ä»–')
        return batch_results


def process_batch_domain_detection(items, existing_tags, llm_client):
    """
    [å·²åºŸå¼ƒ] å¤„ç†é¢†åŸŸæ ‡ç­¾æ£€æµ‹ï¼ŒæŒ‰å­—ç¬¦æ•°æ™ºèƒ½åˆ†æ‰¹
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ process_batch_domain_detection_with_managerï¼Œè¯¥å‡½æ•°å…·æœ‰å®Œæ•´çš„æ ‡ç­¾ç®¡ç†åŠŸèƒ½
    """
    try:
        max_chars_per_batch = 70000  # æ¯æ‰¹æœ€å¤§å­—ç¬¦æ•°
        all_results = []
        all_tags = set(existing_tags)
        
        logger.info(f"[å·²åºŸå¼ƒå‡½æ•°] åˆ†æ‰¹å¤„ç† {len(items)} ä¸ªæ•°æ®é¡¹ï¼Œæ¯æ‰¹æœ€å¤§ {max_chars_per_batch} å­—ç¬¦")
        
        current_batch = []
        current_batch_chars = 0
        batch_count = 0
        
        for i, item in enumerate(items):
            question = item.get('question', '')
            reasoning_path = item.get('mapped_reasoning_path', '')
            
            # è®¡ç®—å½“å‰é¡¹ç›®çš„å­—ç¬¦æ•°ï¼ˆä¸æˆªæ–­ï¼‰
            item_chars = len(question) + len(reasoning_path)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°æ‰¹æ¬¡
            if current_batch and (current_batch_chars + item_chars > max_chars_per_batch):
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_count += 1
                logger.info(f"[å·²åºŸå¼ƒå‡½æ•°] å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼ŒåŒ…å« {len(current_batch)} ä¸ªé¡¹ç›®ï¼Œæ€»å­—ç¬¦æ•°: {current_batch_chars}")
                _process_batch(current_batch, all_results, all_tags, llm_client, batch_count)
                
                # é‡ç½®æ‰¹æ¬¡
                current_batch = []
                current_batch_chars = 0
            
            # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
            current_batch.append({
                'item': item,
                'global_index': i,
                'question': question,
                'reasoning_path': reasoning_path
            })
            current_batch_chars += item_chars
        
        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            batch_count += 1
            logger.info(f"[å·²åºŸå¼ƒå‡½æ•°] å¤„ç†ç¬¬ {batch_count} æ‰¹ï¼ŒåŒ…å« {len(current_batch)} ä¸ªé¡¹ç›®ï¼Œæ€»å­—ç¬¦æ•°: {current_batch_chars}")
            _process_batch(current_batch, all_results, all_tags, llm_client, batch_count)
        
        # æŒ‰ç´¢å¼•æ’åºç»“æœ
        all_results.sort(key=lambda x: x['index'])
        
        # ç¡®ä¿æ‰€æœ‰é¡¹ç›®éƒ½æœ‰ç»“æœ
        for i in range(len(items)):
            if not any(result['index'] == i for result in all_results):
                all_results.append({'index': i, 'domain_tags': []})
        
        # å†æ¬¡æ’åº
        all_results.sort(key=lambda x: x['index'])
        
        return all_results
        
    except Exception as e:
        logger.error(f"å¤§æ‰¹é‡å¤„ç†å‡ºé”™: {e}")
        # è¿”å›ç©ºç»“æœ
        return [{'index': i, 'domain_tags': []} for i in range(len(items))]


def _process_batch(batch_items, all_results, all_tags, llm_client, batch_number):
    """
    [å·²åºŸå¼ƒ] å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„é¢†åŸŸæ ‡ç­¾æ£€æµ‹
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ _process_batch_with_managerï¼Œè¯¥å‡½æ•°å…·æœ‰å®Œæ•´çš„æ ‡ç­¾ç®¡ç†åŠŸèƒ½
    """
    try:
        # å‡†å¤‡å½“å‰æ‰¹æ¬¡çš„æ–‡æœ¬
        batch_texts = []
        for j, batch_item in enumerate(batch_items):
            batch_texts.append({
                'index': j,  # æ‰¹æ¬¡å†…çš„ç´¢å¼•
                'global_index': batch_item['global_index'],  # å…¨å±€ç´¢å¼•
                'question': batch_item['question'],
                'reasoning_path': batch_item['reasoning_path']
            })
            
            # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„prompt
            existing_tags_str = ', '.join(sorted(all_tags)) if all_tags else 'æ— '
            
            prompt_parts = [
                "è¯·åˆ†æä»¥ä¸‹é—®é¢˜å’Œæ¨ç†è·¯å¾„ï¼Œä¸ºæ¯ä¸ªé—®é¢˜è¯†åˆ«å¯¹åº”çš„é¢†åŸŸæ ‡ç­¾ã€‚",
                f"å½“å‰å·²å­˜åœ¨çš„æ ‡ç­¾: {existing_tags_str}",
                "",
                "è¯·æ ¹æ®é—®é¢˜å†…å®¹å’Œæ¨ç†è¿‡ç¨‹ï¼Œè¯†åˆ«æ¯ä¸ªé—®é¢˜å±äºçš„é¢†åŸŸã€‚ä¸€ä¸ªé—®é¢˜å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾ã€‚",
                "ä¼˜å…ˆä½¿ç”¨å·²å­˜åœ¨çš„æ ‡ç­¾ï¼Œå¦‚æœéœ€è¦æ–°æ ‡ç­¾è¯·ç¡®ä¿æ ‡ç­¾ç®€æ´æ˜ç¡®ã€‚",
                "",
                "æ•°æ®åˆ—è¡¨ï¼š\n"
            ]
            
            for text_item in batch_texts:
                prompt_parts.append(f"[{text_item['index']}] é—®é¢˜: {text_item['question']}")
                if text_item['reasoning_path']:
                    prompt_parts.append(f"[{text_item['index']}] æ¨ç†: {text_item['reasoning_path']}")
                prompt_parts.append("")
            
            prompt_parts.append("""
è¯·ä¸ºæ¯ä¸ªç´¢å¼•è¿”å›é¢†åŸŸæ ‡ç­¾è¯†åˆ«ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {"index": 0, "domain_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]},
  {"index": 1, "domain_tags": ["æ ‡ç­¾3"]}
]

æ ‡ç­¾å»ºè®®åŒ…æ‹¬ä½†ä¸é™äºï¼š
- åŒ»å­¦: åŒ»ç–—è¯Šæ–­ã€ç–¾ç—…æ²»ç–—ã€è¯ç‰©çŸ¥è¯†ã€åŒ»å­¦æ£€æŸ¥
- æ³•å¾‹: æ³•å¾‹æ¡æ–‡ã€å¸æ³•ç¨‹åºã€æ³•å¾‹å’¨è¯¢ã€åˆåŒæ³•åŠ¡
- ç§‘æŠ€: è®¡ç®—æœºç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€è½¯ä»¶å·¥ç¨‹ã€ç½‘ç»œæŠ€æœ¯
- æ•™è‚²: æ•™å­¦æ–¹æ³•ã€å­¦ç§‘çŸ¥è¯†ã€æ•™è‚²ç†è®ºã€å­¦ä¹ æŒ‡å¯¼
- é‡‘è: æŠ•èµ„ç†è´¢ã€é“¶è¡Œä¸šåŠ¡ã€ä¿é™©çŸ¥è¯†ã€ç»æµåˆ†æ
- ç”Ÿæ´»: æ—¥å¸¸ç”Ÿæ´»ã€å¥åº·å…»ç”Ÿã€ç¾é£Ÿçƒ¹é¥ªã€å®¶å±…è£…ä¿®
- å†å²: å†å²äº‹ä»¶ã€äººç‰©ä¼ è®°ã€æ–‡åŒ–ä¼ æ‰¿ã€å†å²ç ”ç©¶
- æ–‡å­¦: æ–‡å­¦ä½œå“ã€å†™ä½œæŠ€å·§ã€è¯­è¨€è‰ºæœ¯ã€æ–‡å­¦æ‰¹è¯„
- ç§‘å­¦: ç‰©ç†åŒ–å­¦ã€ç”Ÿç‰©å­¦ã€åœ°ç†å­¦ã€å¤©æ–‡å­¦
- å•†ä¸š: ä¼ä¸šç®¡ç†ã€å¸‚åœºè¥é”€ã€å•†ä¸šç­–ç•¥ã€åˆ›ä¸šæŒ‡å¯¼

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚""")
            
            full_prompt = '\n'.join(prompt_parts)
            logger.info(f"å½“å‰æ‰¹æ¬¡çš„prompt: {full_prompt}")
            
            try:
                import asyncio
                response = asyncio.run(llm_client.generate_response(full_prompt))
                
                # è§£æå½“å‰æ‰¹æ¬¡çš„ç»“æœ
                json_start = response.find('[')
                json_end = response.rfind(']') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    detection_results = json.loads(json_str)
                    
                    for result in detection_results:
                        if isinstance(result, dict) and 'index' in result:
                            batch_index = result['index']
                            if 0 <= batch_index < len(batch_texts):
                                global_index = batch_texts[batch_index]['global_index']
                                tags = result.get('domain_tags', [])
                                if isinstance(tags, list):
                                    clean_tags = []
                                    for tag in tags:
                                        if isinstance(tag, str) and tag.strip():
                                            clean_tag = tag.strip()
                                            clean_tags.append(clean_tag)
                                            all_tags.add(clean_tag)
                                    
                                    all_results.append({
                                        'index': global_index,
                                        'domain_tags': clean_tags
                                    })
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {batch_number} æ‰¹æ—¶å‡ºé”™: {e}")
                # ä¸ºå½“å‰æ‰¹æ¬¡æ·»åŠ ç©ºç»“æœ
                for batch_item in batch_items:
                    all_results.append({
                    'index': batch_item['global_index'],
                        'domain_tags': []
                    })
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ {batch_number} å¤„ç†å¼‚å¸¸: {e}")
        # ä¸ºå½“å‰æ‰¹æ¬¡æ·»åŠ ç©ºç»“æœ
        for batch_item in batch_items:
            all_results.append({
                'index': batch_item['global_index'],
                    'domain_tags': []
        })
        
    except Exception as e:
        logger.error(f"å¤§æ‰¹é‡é¢†åŸŸæ ‡ç­¾æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/convert_json_to_jsonl', methods=['POST'])
def convert_json_to_jsonl():
    """å°†JSONæ–‡ä»¶è½¬æ¢ä¸ºJSONLæ ¼å¼"""
    try:
        data = request.get_json()
        filename = data.get('filename', '')
        content = data.get('content', '')
        count = data.get('count', 0)
        
        if not filename or not content:
            return jsonify({'success': False, 'error': 'å‚æ•°ä¸å®Œæ•´'}), 400
        
        # ç¡®ä¿æ–‡ä»¶åä»¥.jsonlç»“å°¾
        if not filename.endswith('.jsonl'):
            filename += '.jsonl'
        
        # ä¿å­˜åˆ°generated_datasetsç›®å½•
        output_dir = os.path.join('evaluation_data', 'generated_datasets')
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            base_name = filename.replace('.jsonl', '')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{base_name}_{timestamp}.jsonl"
            output_path = os.path.join(output_dir, filename)
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"JSONè½¬JSONLå®Œæˆ: {filename}, åŒ…å« {count} ä¸ªå¯¹è±¡")
        
        return jsonify({
            'success': True,
            'filename': filename,
            'count': count,
            'path': output_path
        })
        
    except Exception as e:
        logger.error(f"JSONè½¬JSONLå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data_management/detect_leakage', methods=['POST'])
def detect_information_leakage():
    """æ£€æµ‹æ¨ç†è·¯å¾„ä¸­çš„ä¿¡æ¯æ³„æ¼"""
    try:
        data = request.get_json()
        question = data.get('question', '')
        reasoning_map = data.get('reasoning_map', '')
        entity_mapping = data.get('entity_mapping', {})
        qps_limit = data.get('qps_limit', 2.0)  # æ·»åŠ QPSé™åˆ¶å‚æ•°
        
        if not question or not reasoning_map:
            return jsonify({
                'success': False, 
                'error': 'é—®é¢˜å’Œæ¨ç†è·¯å¾„ä¸èƒ½ä¸ºç©º'
            }), 400
        
        logger.info(f"å¼€å§‹ä¿¡æ¯æ³„æ¼æ£€æµ‹ - é—®é¢˜é•¿åº¦: {len(question)}, æ¨ç†è·¯å¾„é•¿åº¦: {len(reasoning_map)}, QPSé™åˆ¶: {qps_limit}")
        
        # ä½¿ç”¨OpenRouterå®¢æˆ·ç«¯è¿›è¡Œæ£€æµ‹ï¼Œæ”¯æŒQPSé™åˆ¶
        from lib.llm_client import get_qa_llm_client
        llm_client = get_qa_llm_client(enable_rate_limiting=True, qps=qps_limit)
        
        # è°ƒç”¨åŒæ­¥æ£€æµ‹æ–¹æ³•
        try:
            detection_result = llm_client.detect_information_leakage(
                question=question,
                reasoning_map=reasoning_map,
                entity_mapping=entity_mapping
            )
                
        except Exception as e:
            logger.error(f"ä¿¡æ¯æ³„æ¼æ£€æµ‹è°ƒç”¨å¤±è´¥: {e}")
            detection_result = {'has_leakage': False, 'error': str(e)}
        
        logger.info(f"æ£€æµ‹ç»“æœ: æœ‰æ³„æ¼={detection_result.get('has_leakage', False)}")
        
        return jsonify({
            'success': True,
            'data': detection_result
        })
        
    except Exception as e:
        logger.error(f"ä¿¡æ¯æ³„æ¼æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({
            'success': False, 
            'error': f'æ£€æµ‹å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/data_management/detect_leakage_batch', methods=['POST'])
def detect_information_leakage_batch():
    """æ‰¹é‡æ£€æµ‹ä¿¡æ¯æ³„æ¼ï¼ˆåç«¯å¹¶å‘å¤„ç†ï¼Œçªç ´æµè§ˆå™¨å¹¶å‘é™åˆ¶ï¼‰"""
    
    try:
        data = request.get_json()
        items = data.get('items', [])
        auto_fix = data.get('auto_fix', True)
        qps_limit = data.get('qps_limit', 2.0)
        max_workers = min(int(qps_limit), 50)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
        
        if not items:
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰æä¾›æ£€æµ‹æ•°æ®'
            }), 400
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®é¡¹
        valid_items = []
        for i, item in enumerate(items):
            if item.get('question') and (item.get('reasoning_path') or item.get('reasoning_map')):
                valid_items.append((i, item))
        
        if not valid_items:
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰æ‰¾åˆ°åŒ…å«questionå’Œreasoning_pathçš„æœ‰æ•ˆæ•°æ®'
            }), 400
        
        logger.info(f"å¼€å§‹æ‰¹é‡ä¿¡æ¯æ³„æ¼æ£€æµ‹ - æ•°æ®é¡¹æ•°: {len(valid_items)}, QPSé™åˆ¶: {qps_limit}, æœ€å¤§å¹¶å‘: {max_workers}")
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        results = {}
        stats = {
            'processed': 0,
            'leakage_count': 0,
            'fixed_count': 0,
            'error_count': 0,
            'unknown_count': 0
        }
        stats_lock = Lock()
        
        def process_single_item(index_and_item):
            """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
            original_index, item = index_and_item
            
            try:
                question = item.get('question', '')
                reasoning_map = item.get('reasoning_path') or item.get('reasoning_map', '')
                entity_mapping = item.get('entity_mapping', {})
                
                # ä½¿ç”¨ç‹¬ç«‹çš„å®¢æˆ·ç«¯å®ä¾‹é¿å…çŠ¶æ€å†²çª
                from lib.llm_client import get_qa_llm_client
                llm_client = get_qa_llm_client(enable_rate_limiting=True, qps=qps_limit)
                
                detection_result = llm_client.detect_information_leakage(
                    question=question,
                    reasoning_map=reasoning_map,
                    entity_mapping=entity_mapping
                )
                
                has_leakage = detection_result.get('has_leakage', False)
                
                # å¤„ç†ç»“æœ
                result = {
                    'original_index': original_index,
                    'processed': True,
                    'has_leakage': has_leakage,
                    'leaked_info': detection_result.get('leaked_info', []),
                    'detection_time': time.time(),
                    'fixed': False
                }
                
                # è‡ªåŠ¨ä¿®å¤é€»è¾‘
                if has_leakage == True and auto_fix and detection_result.get('fixed_reasoning_map'):
                    result['fixed_reasoning_map'] = detection_result.get('fixed_reasoning_map')
                    result['fixed_entity_mapping'] = detection_result.get('fixed_entity_mapping')
                    result['fixed'] = True
                
                # æ›´æ–°ç»Ÿè®¡
                with stats_lock:
                    stats['processed'] += 1
                    if has_leakage == True:
                        stats['leakage_count'] += 1
                        if result['fixed']:
                            stats['fixed_count'] += 1
                    elif has_leakage == 'unknown':
                        stats['unknown_count'] += 1
                
                return result
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬{original_index}é¡¹æ—¶å‡ºé”™: {e}")
                with stats_lock:
                    stats['error_count'] += 1
                return {
                    'original_index': original_index,
                    'processed': False,
                    'error': str(e)
                }
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_item = {
                executor.submit(process_single_item, item_data): item_data 
                for item_data in valid_items
            }
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_item):
                try:
                    result = future.result()
                    results[result['original_index']] = result
                except Exception as e:
                    item_data = future_to_item[future]
                    logger.error(f"çº¿ç¨‹å¤„ç†å¤±è´¥: {e}")
                    with stats_lock:
                        stats['error_count'] += 1
                    results[item_data[0]] = {
                        'original_index': item_data[0],
                        'processed': False,
                        'error': str(e)
                    }
        
        logger.info(f"æ‰¹é‡æ£€æµ‹å®Œæˆ - å¤„ç†: {stats['processed']}, æ³„æ¼: {stats['leakage_count']}, ä¿®å¤: {stats['fixed_count']}, é”™è¯¯: {stats['error_count']}")
        
        return jsonify({
            'success': True,
            'data': {
                'results': results,
                'stats': stats,
                'total_items': len(valid_items),
                'max_workers': max_workers
            }
        })
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¿¡æ¯æ³„æ¼æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æ‰¹é‡æ£€æµ‹å¤±è´¥: {str(e)}'
        }), 500

# æœ€ç»ˆæ•°æ®ç®¡ç†ç›¸å…³API
@app.route('/final-datasets')
def final_datasets():
    """æœ€ç»ˆæ•°æ®ç®¡ç†é¡µé¢"""
    return render_template('final_datasets.html')

@app.route('/api/final_datasets/load', methods=['GET'])
def load_final_datasets():
    """åŠ è½½æœ€ç»ˆæ•°æ®é›†"""
    try:
        logger.info("å¼€å§‹åŠ è½½æœ€ç»ˆæ•°æ®é›†")
        
        import hashlib
        import json
        from pathlib import Path
        
        final_datasets_dir = Path('evaluation_data/final_datasets')
        if not final_datasets_dir.exists():
            return jsonify({
                'success': False,
                'error': 'æœ€ç»ˆæ•°æ®é›†ç›®å½•ä¸å­˜åœ¨'
            }), 404
        
        all_data = []
        file_mapping = {}  # è·Ÿè¸ªå“ªäº›æ–‡ä»¶è¢«åŠ è½½äº†
        
        def load_jsonl_file(file_path, source_name):
            """åŠ è½½JSONLæ–‡ä»¶"""
            data = []
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                item = json.loads(line)
                                # æ·»åŠ æ•°æ®æºä¿¡æ¯
                                item['source'] = source_name
                                # æ³¨æ„ï¼šä¸åœ¨åŠ è½½æ—¶è‡ªåŠ¨ç”ŸæˆIDï¼Œè®©ç”¨æˆ·æ˜ç¡®ç‚¹å‡»"ç”ŸæˆID"æŒ‰é’®æ¥ç”Ÿæˆå¹¶ä¿å­˜
                                data.append(item)
                            except json.JSONDecodeError as e:
                                logger.warning(f"è§£æJSONå¤±è´¥ {file_path}:{line_num} - {e}")
                                continue
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return data
        

        
        # å…ˆæ‰«ætaggedç›®å½•ï¼Œæ”¶é›†å¸¦æ ‡ç­¾çš„æ–‡ä»¶
        tagged_dir = final_datasets_dir / 'tagged'
        tagged_files = set()
        
        if tagged_dir.exists():
            for tagged_file in tagged_dir.glob('*.jsonl'):
                # æå–åŸºç¡€æ–‡ä»¶åï¼ˆå»æ‰_with_tagsåç¼€ï¼‰
                base_name = tagged_file.stem
                if base_name.endswith('_with_tags'):
                    original_name = base_name[:-10]  # å»æ‰'_with_tags'
                    tagged_files.add(original_name)
                    
                    # åŠ è½½å¸¦æ ‡ç­¾çš„æ–‡ä»¶
                    data = load_jsonl_file(tagged_file, base_name)
                    all_data.extend(data)
                    file_mapping[base_name] = len(data)
                    logger.info(f"åŠ è½½å¸¦æ ‡ç­¾æ–‡ä»¶: {tagged_file.name} ({len(data)} æ¡è®°å½•)")
        
        # å†æ‰«æä¸»ç›®å½•ï¼ŒåŠ è½½æ²¡æœ‰æ ‡ç­¾ç‰ˆæœ¬çš„æ–‡ä»¶
        for jsonl_file in final_datasets_dir.glob('*.jsonl'):
            file_stem = jsonl_file.stem
            # å¦‚æœå·²ç»æœ‰å¸¦æ ‡ç­¾çš„ç‰ˆæœ¬ï¼Œè·³è¿‡åŸç‰ˆæœ¬
            if file_stem not in tagged_files:
                data = load_jsonl_file(jsonl_file, file_stem)
                all_data.extend(data)
                file_mapping[file_stem] = len(data)
                logger.info(f"åŠ è½½åŸå§‹æ–‡ä»¶: {jsonl_file.name} ({len(data)} æ¡è®°å½•)")
        
        # å»é‡å¤„ç†ï¼ˆåŸºäºquestionå’Œanswerçš„ç»„åˆï¼‰
        seen_combinations = set()
        unique_data = []
        
        for item in all_data:
            question = item.get('question', '')
            answer = item.get('answer', '')
            combination = (question.strip(), answer.strip())
            
            if combination not in seen_combinations:
                seen_combinations.add(combination)
                unique_data.append(item)
        
        removed_duplicates = len(all_data) - len(unique_data)
        if removed_duplicates > 0:
            logger.info(f"å»é‡å¤„ç†: ç§»é™¤äº† {removed_duplicates} æ¡é‡å¤æ•°æ®")
        
        logger.info(f"æœ€ç»ˆæ•°æ®é›†åŠ è½½å®Œæˆ: {len(unique_data)} æ¡è®°å½•ï¼Œæ¥è‡ª {len(file_mapping)} ä¸ªæ–‡ä»¶")
        
        return jsonify({
            'success': True,
            'data': unique_data,
            'file_mapping': file_mapping,
            'stats': {
                'total_records': len(unique_data),
                'total_files': len(file_mapping),
                'duplicates_removed': removed_duplicates
            }
        })
        
    except Exception as e:
        logger.error(f"åŠ è½½æœ€ç»ˆæ•°æ®é›†å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'åŠ è½½æ•°æ®å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/final_datasets/update_id', methods=['POST'])
def update_data_id():
    """æ›´æ–°æ•°æ®é¡¹çš„å”¯ä¸€ID"""
    try:
        data = request.get_json()
        old_id = data.get('old_id')
        new_id = data.get('new_id')
        source_file = data.get('source_file')
        
        if not all([old_id, new_id, source_file]):
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'
            }), 400
        
        # å®é™…æ›´æ–°æ–‡ä»¶ä¸­çš„ID
        file_path = Path('evaluation_data/final_datasets') / f"{source_file}.jsonl"
        if not file_path.exists():
            # å°è¯•åœ¨taggedç›®å½•ä¸­æŸ¥æ‰¾
            file_path = Path('evaluation_data/final_datasets/tagged') / f"{source_file}.jsonl"
        
        if not file_path.exists():
            return jsonify({
                'success': False,
                'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {source_file}'
            }), 404
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        updated_lines = []
        found = False
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        if item.get('unique_id') == old_id:
                            item['unique_id'] = new_id
                            found = True
                            logger.info(f"æ›´æ–°ID: {old_id} -> {new_id} åœ¨æ–‡ä»¶ {source_file}")
                        updated_lines.append(json.dumps(item, ensure_ascii=False))
                    except json.JSONDecodeError:
                        updated_lines.append(line)
        
        if not found:
            return jsonify({
                'success': False,
                'error': f'æœªæ‰¾åˆ°IDä¸º {old_id} çš„æ•°æ®é¡¹'
            }), 404
        
        # å†™å›æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            for line in updated_lines:
                f.write(line + '\n')
        
        return jsonify({
            'success': True,
            'message': 'IDæ›´æ–°æˆåŠŸ'
        })
        
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®IDå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æ›´æ–°IDå¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/final_datasets/generate_missing_ids', methods=['POST'])
def generate_missing_ids():
    """ä¸ºæ²¡æœ‰unique_idçš„æ•°æ®é¡¹ç”ŸæˆIDå¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        import hashlib
        from pathlib import Path
        
        final_datasets_dir = Path('evaluation_data/final_datasets')
        if not final_datasets_dir.exists():
            return jsonify({
                'success': False,
                'error': 'æœ€ç»ˆæ•°æ®é›†ç›®å½•ä¸å­˜åœ¨'
            }), 404
        
        updated_files = []
        total_generated = 0
        existing_ids = set()  # è·Ÿè¸ªå·²å­˜åœ¨çš„IDï¼Œç¡®ä¿ä¸é‡å¤
        
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰ç°æœ‰çš„ID
        def collect_existing_ids():
            nonlocal existing_ids
            for file_path in final_datasets_dir.rglob('*.jsonl'):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                try:
                                    item = json.loads(line)
                                    if item.get('unique_id'):
                                        existing_ids.add(item['unique_id'])
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        collect_existing_ids()
        logger.info(f"æ”¶é›†åˆ° {len(existing_ids)} ä¸ªç°æœ‰ID")
        
        def process_file(file_path, source_name):
            """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆç¼ºå¤±çš„ID"""
            nonlocal total_generated
            updated_lines = []
            generated_count = 0
            
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            item = json.loads(line)
                            if 'unique_id' not in item or not item['unique_id'] or item['unique_id'].strip() == '':
                                # ç”Ÿæˆæ–°çš„å”¯ä¸€IDï¼Œç¡®ä¿ä¸é‡å¤
                                content = str(item.get('question', '')) + str(item.get('answer', ''))
                                import uuid
                                import random
                                
                                # é‡å¤ç”Ÿæˆç›´åˆ°æ‰¾åˆ°å”¯ä¸€ID
                                attempts = 0
                                while attempts < 10:  # æœ€å¤šå°è¯•10æ¬¡
                                    # ä½¿ç”¨å¤šä¸ªå› å­ç¡®ä¿å”¯ä¸€æ€§
                                    hash_input = f"{content}{source_name}{line_num}{time.time()}{random.random()}".encode('utf-8')
                                    hash_value = hashlib.md5(hash_input).hexdigest()[:8]
                                    
                                    # ä½¿ç”¨UUIDçš„ä¸€éƒ¨åˆ†å¢åŠ å”¯ä¸€æ€§
                                    uuid_part = str(uuid.uuid4()).replace('-', '')[:8]
                                    
                                    # ä½¿ç”¨å®Œæ•´æ—¶é—´æˆ³ï¼ˆå¾®ç§’çº§ï¼‰
                                    timestamp = str(int(time.time() * 1000000))[-8:]
                                    
                                    new_id = f"fd_{hash_value}_{uuid_part}_{timestamp}"
                                    
                                    if new_id not in existing_ids:
                                        item['unique_id'] = new_id
                                        existing_ids.add(new_id)  # æ·»åŠ åˆ°å·²å­˜åœ¨IDé›†åˆ
                                        break
                                    
                                    attempts += 1
                                    time.sleep(0.001)  # çŸ­æš‚å»¶è¿Ÿç¡®ä¿æ—¶é—´æˆ³ä¸åŒ
                                
                                if attempts >= 10:
                                    # å¦‚æœ10æ¬¡éƒ½é‡å¤ï¼Œä½¿ç”¨UUIDå…œåº•
                                    item['unique_id'] = f"fd_uuid_{str(uuid.uuid4()).replace('-', '')[:16]}"
                                    existing_ids.add(item['unique_id'])
                                generated_count += 1
                                logger.info(f"ç”ŸæˆID: {item['unique_id']} åœ¨æ–‡ä»¶ {source_name}:{line_num}")
                            updated_lines.append(json.dumps(item, ensure_ascii=False))
                        except json.JSONDecodeError as e:
                            logger.warning(f"è§£æJSONå¤±è´¥ {file_path}:{line_num} - {e}")
                            updated_lines.append(line)
            
            if generated_count > 0:
                # å†™å›æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    for line in updated_lines:
                        f.write(line + '\n')
                
                updated_files.append({
                    'file': source_name,
                    'generated_count': generated_count
                })
                total_generated += generated_count
                logger.info(f"æ–‡ä»¶ {source_name} ç”Ÿæˆäº† {generated_count} ä¸ªID")
            
            return generated_count
        
        # ä½¿ç”¨ä¸åŠ è½½æ•°æ®æ—¶ç›¸åŒçš„é€»è¾‘ï¼šä¼˜å…ˆå¤„ç†taggedç‰ˆæœ¬ï¼Œé¿å…é‡å¤å¤„ç†
        tagged_dir = final_datasets_dir / 'tagged'
        tagged_files = set()
        
        # å…ˆå¤„ç†taggedç›®å½•ä¸‹çš„æ–‡ä»¶
        if tagged_dir.exists():
            for tagged_file in tagged_dir.glob('*.jsonl'):
                base_name = tagged_file.stem
                if base_name.endswith('_with_tags'):
                    original_name = base_name[:-10]  # å»æ‰'_with_tags'
                    tagged_files.add(original_name)
                    process_file(tagged_file, base_name)
                else:
                    process_file(tagged_file, base_name)
        
        # å†å¤„ç†ä¸»ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œä½†è·³è¿‡å·²ç»æœ‰taggedç‰ˆæœ¬çš„æ–‡ä»¶
        for jsonl_file in final_datasets_dir.glob('*.jsonl'):
            file_stem = jsonl_file.stem
            # å¦‚æœå·²ç»æœ‰å¸¦æ ‡ç­¾çš„ç‰ˆæœ¬ï¼Œè·³è¿‡åŸç‰ˆæœ¬
            if file_stem not in tagged_files:
                process_file(jsonl_file, file_stem)
        
        logger.info(f"æ‰¹é‡ç”ŸæˆIDå®Œæˆ: æ€»å…±ç”Ÿæˆäº† {total_generated} ä¸ªIDï¼Œæ¶‰åŠ {len(updated_files)} ä¸ªæ–‡ä»¶")
        
        return jsonify({
            'success': True,
            'message': f'æ‰¹é‡ç”ŸæˆIDå®Œæˆ',
            'stats': {
                'total_generated': total_generated,
                'updated_files': updated_files
            }
        })
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ç”ŸæˆIDå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æ‰¹é‡ç”ŸæˆIDå¤±è´¥: {str(e)}'
                 }), 500

@app.route('/api/final_datasets/clean_dirty_ids', methods=['POST'])
def clean_dirty_ids():
    """æ¸…ç†è„æ•°æ®ï¼šåˆ é™¤æ™®é€šç‰ˆæœ¬æ–‡ä»¶ä¸­çš„unique_idå­—æ®µï¼ˆå¦‚æœå­˜åœ¨å¯¹åº”çš„with_tagsç‰ˆæœ¬ï¼‰"""
    try:
        from pathlib import Path
        
        final_datasets_dir = Path('evaluation_data/final_datasets')
        if not final_datasets_dir.exists():
            return jsonify({
                'success': False,
                'error': 'æœ€ç»ˆæ•°æ®é›†ç›®å½•ä¸å­˜åœ¨'
            }), 404
        
        cleaned_files = []
        total_cleaned = 0
        
        # å…ˆæ‰«ætaggedç›®å½•ï¼Œæ”¶é›†with_tagsæ–‡ä»¶åˆ—è¡¨
        tagged_dir = final_datasets_dir / 'tagged'
        tagged_files = set()
        
        if tagged_dir.exists():
            for tagged_file in tagged_dir.glob('*.jsonl'):
                base_name = tagged_file.stem
                if base_name.endswith('_with_tags'):
                    original_name = base_name[:-10]  # å»æ‰'_with_tags'
                    tagged_files.add(original_name)
        
        logger.info(f"æ‰¾åˆ° {len(tagged_files)} ä¸ªwith_tagsæ–‡ä»¶: {tagged_files}")
        
        # å¤„ç†ä¸»ç›®å½•ä¸‹å¯¹åº”çš„æ™®é€šç‰ˆæœ¬æ–‡ä»¶
        for original_name in tagged_files:
            original_file = final_datasets_dir / f"{original_name}.jsonl"
            if original_file.exists():
                logger.info(f"æ¸…ç†æ–‡ä»¶ {original_file.name} ä¸­çš„unique_idå­—æ®µ")
                
                updated_lines = []
                cleaned_count = 0
                
                with open(original_file, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                item = json.loads(line)
                                if 'unique_id' in item:
                                    del item['unique_id']
                                    cleaned_count += 1
                                    logger.info(f"åˆ é™¤ID: {original_file.name}:{line_num}")
                                updated_lines.append(json.dumps(item, ensure_ascii=False))
                            except json.JSONDecodeError as e:
                                logger.warning(f"è§£æJSONå¤±è´¥ {original_file}:{line_num} - {e}")
                                updated_lines.append(line)
                
                if cleaned_count > 0:
                    # å†™å›æ–‡ä»¶
                    with open(original_file, 'w', encoding='utf-8') as f:
                        for line in updated_lines:
                            f.write(line + '\n')
                    
                    cleaned_files.append({
                        'file': original_name,
                        'cleaned_count': cleaned_count
                    })
                    total_cleaned += cleaned_count
                    logger.info(f"æ–‡ä»¶ {original_name} æ¸…ç†äº† {cleaned_count} ä¸ªID")
                else:
                    logger.info(f"æ–‡ä»¶ {original_name} æ²¡æœ‰éœ€è¦æ¸…ç†çš„ID")
        
        logger.info(f"è„æ•°æ®æ¸…ç†å®Œæˆ: æ€»å…±æ¸…ç†äº† {total_cleaned} ä¸ªIDï¼Œæ¶‰åŠ {len(cleaned_files)} ä¸ªæ–‡ä»¶")
        
        return jsonify({
            'success': True,
            'message': f'è„æ•°æ®æ¸…ç†å®Œæˆ',
            'stats': {
                'total_cleaned': total_cleaned,
                'cleaned_files': cleaned_files,
                'scanned_files': len(tagged_files)
            }
        })
        
    except Exception as e:
        logger.error(f"æ¸…ç†è„æ•°æ®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æ¸…ç†å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/final_datasets/check_duplicates', methods=['GET'])
def check_duplicate_ids():
    """æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰é‡å¤çš„å”¯ä¸€ID"""
    try:
        from pathlib import Path
        from collections import Counter
        
        final_datasets_dir = Path('evaluation_data/final_datasets')
        if not final_datasets_dir.exists():
            return jsonify({
                'success': False,
                'error': 'æœ€ç»ˆæ•°æ®é›†ç›®å½•ä¸å­˜åœ¨'
            }), 404
        
        all_ids = []
        id_sources = {}  # è®°å½•æ¯ä¸ªIDæ¥è‡ªå“ªä¸ªæ–‡ä»¶
        
        # æ”¶é›†æ‰€æœ‰ID
        for file_path in final_datasets_dir.rglob('*.jsonl'):
            source_name = file_path.stem
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                item = json.loads(line)
                                unique_id = item.get('unique_id')
                                if unique_id:
                                    all_ids.append(unique_id)
                                    if unique_id not in id_sources:
                                        id_sources[unique_id] = []
                                    id_sources[unique_id].append({
                                        'file': source_name,
                                        'line': line_num,
                                        'question': item.get('question', '')[:50] + '...' if len(item.get('question', '')) > 50 else item.get('question', '')
                                    })
                            except json.JSONDecodeError:
                                continue
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # ç»Ÿè®¡é‡å¤
        id_counts = Counter(all_ids)
        duplicates = {id_val: count for id_val, count in id_counts.items() if count > 1}
        
        duplicate_details = {}
        for dup_id in duplicates:
            duplicate_details[dup_id] = {
                'count': duplicates[dup_id],
                'sources': id_sources[dup_id]
            }
        
        logger.info(f"IDé‡å¤æ£€æŸ¥å®Œæˆ: æ€»è®¡ {len(all_ids)} ä¸ªIDï¼Œ{len(duplicates)} ä¸ªé‡å¤")
        
        return jsonify({
            'success': True,
            'stats': {
                'total_ids': len(all_ids),
                'unique_ids': len(id_counts),
                'duplicate_count': len(duplicates),
                'missing_ids': len([1 for file_path in final_datasets_dir.rglob('*.jsonl') 
                                  for line in open(file_path, 'r', encoding='utf-8') 
                                  if line.strip() and not json.loads(line.strip()).get('unique_id', '')])
            },
            'duplicates': duplicate_details
        })
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥IDé‡å¤å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æ£€æŸ¥å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/final_datasets/export', methods=['POST'])
def export_final_datasets():
    """å¯¼å‡ºç­›é€‰åçš„æ•°æ®"""
    try:
        data = request.get_json()
        filtered_data = data.get('data', [])
        export_format = data.get('format', 'jsonl')
        
        if not filtered_data:
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰è¦å¯¼å‡ºçš„æ•°æ®'
            }), 400
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if export_format == 'jsonl':
            # å¯¼å‡ºä¸ºJSONLæ ¼å¼
            output_lines = []
            for item in filtered_data:
                output_lines.append(json.dumps(item, ensure_ascii=False))
            
            output_content = '\n'.join(output_lines)
            filename = f'final_datasets_export_{timestamp}.jsonl'
            
        elif export_format == 'json':
            # å¯¼å‡ºä¸ºJSONæ ¼å¼
            output_content = json.dumps(filtered_data, ensure_ascii=False, indent=2)
            filename = f'final_datasets_export_{timestamp}.json'
            
        else:
            return jsonify({
                'success': False,
                'error': 'ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼'
            }), 400
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        temp_dir = tempfile.gettempdir()
        temp_file_path = os.path.join(temp_dir, filename)
        
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(output_content)
        
        logger.info(f"å¯¼å‡ºæ•°æ®æˆåŠŸ: {filename} ({len(filtered_data)} æ¡è®°å½•)")
        
        return jsonify({
            'success': True,
            'filename': filename,
            'download_url': f'/api/download/{filename}',
            'record_count': len(filtered_data)
        })
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºæ•°æ®å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/download/<filename>')
def download_exported_file(filename):
    """ä¸‹è½½å¯¼å‡ºçš„æ–‡ä»¶"""
    try:
        import tempfile
        temp_dir = tempfile.gettempdir()
        file_path = os.path.join(temp_dir, filename)
        
        if not os.path.exists(file_path):
            return jsonify({
                'success': False,
                'error': 'æ–‡ä»¶ä¸å­˜åœ¨'
            }), 404
        
        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/octet-stream'
        )
        
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'
        }), 500

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºWebåº”ç”¨...")
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:5000")
    print("ğŸ“Š åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹å®æ—¶æ„å»ºè¿‡ç¨‹")
    print("ğŸ“ æ—¥å¿—å’ŒTraceåŠŸèƒ½å·²å¯ç”¨")
    print("ğŸ” æ—¥å¿—æ–‡ä»¶ä½ç½®: ~/Downloads/logs/app_YYYYMMDD.log") 
    print("ğŸ“‹ TraceåŠŸèƒ½: æ¯ä¸ªè¯·æ±‚éƒ½æœ‰å”¯ä¸€çš„trace IDç”¨äºè¿½è¸ª")
    print("="*50)
    
    socketio.run(app, debug=True, host='0.0.0.0', port=5000, threaded=True)