#!/usr/bin/env python3
"""
å‘½ä»¤è¡Œæ‰¹é‡QAç”Ÿæˆè„šæœ¬

è¯¥è„šæœ¬å…è®¸ç”¨æˆ·ä»å‘½ä»¤è¡Œé€‰æ‹©ç§å­æ–‡ä»¶å¹¶è¿›è¡Œæ‰¹é‡QAç”Ÿæˆï¼Œæ”¯æŒï¼š
- é€‰æ‹©ç§å­æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰
- è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
- é…ç½®ç”Ÿæˆå‚æ•°
- è¿›åº¦æ˜¾ç¤º
- æ–­ç‚¹ç»­ä¼ 

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python batch_qa_cli.py                                    # äº¤äº’å¼é€‰æ‹©ç§å­æ–‡ä»¶
    python batch_qa_cli.py --seed-file medical_entities.csv   # æŒ‡å®šç§å­æ–‡ä»¶
    python batch_qa_cli.py --list-seeds                       # åˆ—å‡ºå¯ç”¨çš„ç§å­æ–‡ä»¶
    python batch_qa_cli.py --seed-file test.csv --output custom_output.jsonl --max-nodes 300
    python batch_qa_cli.py --seed-file entities.csv --disable-instant-save  # ç¦ç”¨å³æ—¶ä¿å­˜
    python batch_qa_cli.py --seed-file entities.csv --force-overwrite       # å¼ºåˆ¶è¦†ç›–é‡æ–°å¼€å§‹
"""

import argparse
import asyncio
import csv
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# é¡¹ç›®å¯¼å…¥
from config import setup_global_logging
from lib.run_manager import RunManager
from lib.trace_manager import start_trace

# é…ç½®æ—¥å¿—
log_filename = setup_global_logging()
logger = logging.getLogger(__name__)

class AsyncRateLimiter:
    """å¼‚æ­¥é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, qps: float):
        self.qps = qps
        self.interval = 1.0 / qps if qps > 0 else 0
        self.last_request = 0.0
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        """è·å–è®¿é—®æƒé™ï¼Œç¡®ä¿ä¸è¶…è¿‡QPSé™åˆ¶"""
        if self.qps <= 0:
            return
            
        async with self.lock:
            now = asyncio.get_event_loop().time()
            elapsed = now - self.last_request
            
            if elapsed < self.interval:
                sleep_time = self.interval - elapsed
                await asyncio.sleep(sleep_time)
            
            self.last_request = asyncio.get_event_loop().time()

class BatchQACLI:
    """å‘½ä»¤è¡Œæ‰¹é‡QAç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.seed_files_dir = "evaluation_data/entity_sets"
        self.default_output_dir = "qa_output"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.seed_files_dir, exist_ok=True)
        os.makedirs(self.default_output_dir, exist_ok=True)
    
    def list_available_seed_files(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„ç§å­æ–‡ä»¶"""
        seed_files = []
        if os.path.exists(self.seed_files_dir):
            for file in os.listdir(self.seed_files_dir):
                if file.endswith('.csv'):
                    seed_files.append(file)
        return sorted(seed_files)
    
    def load_entities_from_csv(self, csv_file: str) -> List[str]:
        """ä»CSVæ–‡ä»¶åŠ è½½å®ä½“åˆ—è¡¨"""
        csv_path = os.path.join(self.seed_files_dir, csv_file)
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"ç§å­æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        
        entities = []
        try:
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # æ”¯æŒå¤šç§åˆ—å
                    entity = row.get('entity') or row.get('name') or row.get('å®ä½“') or row.get('åç§°')
                    if entity and entity.strip():
                        entities.append(entity.strip())
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            raise
        
        if not entities:
            raise ValueError("CSVæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å®ä½“æ•°æ®")
        
        return entities
    
    def interactive_select_seed_file(self) -> str:
        """äº¤äº’å¼é€‰æ‹©ç§å­æ–‡ä»¶"""
        seed_files = self.list_available_seed_files()
        
        if not seed_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç§å­æ–‡ä»¶ï¼")
            print(f"è¯·å°†CSVæ ¼å¼çš„ç§å­æ–‡ä»¶æ”¾åœ¨ {self.seed_files_dir} ç›®å½•ä¸‹")
            print("CSVæ–‡ä»¶åº”åŒ…å« 'entity' æˆ– 'name' åˆ—")
            sys.exit(1)
        
        print("\nğŸ“‚ å¯ç”¨çš„ç§å­æ–‡ä»¶:")
        print("=" * 50)
        for i, file in enumerate(seed_files, 1):
            # å°è¯•è¯»å–æ–‡ä»¶ä¿¡æ¯
            try:
                entities = self.load_entities_from_csv(file)
                print(f"{i:2d}. {file} ({len(entities)} ä¸ªå®ä½“)")
            except Exception as e:
                print(f"{i:2d}. {file} (è¯»å–å¤±è´¥: {e})")
        
        print("=" * 50)
        
        while True:
            try:
                choice = input("\nè¯·é€‰æ‹©ç§å­æ–‡ä»¶ç¼–å· (1-{}): ".format(len(seed_files)))
                choice_num = int(choice)
                if 1 <= choice_num <= len(seed_files):
                    return seed_files[choice_num - 1]
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„ç¼–å·")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æ•°å­—")
            except KeyboardInterrupt:
                print("\nç”¨æˆ·å–æ¶ˆæ“ä½œ")
                sys.exit(0)
    
    def generate_default_output_path(self, seed_file: str, entities_count: int) -> str:
        """ç”Ÿæˆé»˜è®¤è¾“å‡ºè·¯å¾„"""
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        base_name = os.path.splitext(seed_file)[0]
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{base_name}_{entities_count}entities_{timestamp}.jsonl"
        return os.path.join(self.default_output_dir, filename)
    
    def load_existing_results(self, output_path: str) -> Dict[str, Any]:
        """åŠ è½½å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ """
        if not os.path.exists(output_path):
            return {}
        
        existing_results = {}
        completed_entities = set()
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        qa_data = json.loads(line)
                        source_entity = qa_data.get('source_entity', '')
                        if source_entity:
                            existing_results[source_entity] = qa_data
                            completed_entities.add(source_entity)
                    except json.JSONDecodeError as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num}: {e}")
                        continue
            
            logger.info(f"åŠ è½½å·²å­˜åœ¨ç»“æœ: {len(existing_results)} ä¸ªQAå¯¹")
            logger.info(f"å·²å®Œæˆå®ä½“: {sorted(list(completed_entities))}")
            return existing_results
            
        except Exception as e:
            logger.error(f"åŠ è½½å·²å­˜åœ¨ç»“æœå¤±è´¥: {e}")
            return {}
    
    def save_single_qa(self, qa_result: Dict[str, Any], output_path: str):
        """å³æ—¶ä¿å­˜å•ä¸ªQAç»“æœ"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # æ·»åŠ å®Œæˆæ—¶é—´æˆ³å’Œé¡ºåºä¿¡æ¯
            qa_result['completed_at'] = datetime.now().isoformat()
            qa_result['save_order'] = datetime.now().timestamp()  # ç”¨äºæ’åº
            
            # è¿½åŠ å†™å…¥JSONLæ–‡ä»¶
            with open(output_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(qa_result, ensure_ascii=False) + '\n')
                f.flush()  # å¼ºåˆ¶å†™å…¥ç£ç›˜
            
            logger.debug(f"å³æ—¶ä¿å­˜QAç»“æœ: {qa_result.get('source_entity', 'unknown')}")
            
        except Exception as e:
            logger.error(f"å³æ—¶ä¿å­˜å¤±è´¥: {e}")
    
    def get_processing_status(self, entities: List[str], output_path: str) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„å¤„ç†çŠ¶æ€ä¿¡æ¯"""
        existing_results = self.load_existing_results(output_path)
        
        completed_entities = set(existing_results.keys())
        remaining_entities = [entity for entity in entities if entity not in completed_entities]
        
        # æŒ‰åŸå§‹é¡ºåºåˆ†æå®Œæˆæƒ…å†µ
        completion_map = {}
        for i, entity in enumerate(entities, 1):
            completion_map[entity] = {
                'index': i,
                'completed': entity in completed_entities,
                'status': 'âœ…' if entity in completed_entities else 'â³'
            }
        
        return {
            'total_entities': len(entities),
            'completed_count': len(completed_entities),
            'remaining_count': len(remaining_entities),
            'completion_rate': len(completed_entities) / len(entities) if entities else 0,
            'completed_entities': sorted(list(completed_entities)),
            'remaining_entities': remaining_entities,
            'completion_map': completion_map
        }
    
    async def batch_generate_qa(
        self, 
        entities: List[str],
        output_path: str,
        sampling_algorithm: str = "max_chain",
        use_unified_qa: bool = True,
        max_nodes: int = 200,
        max_iterations: int = 10,
        parallel_workers: int = 20,
        qps_limit: float = 20,
        enable_resume: bool = True,
        enable_instant_save: bool = True
    ) -> Dict[str, Any]:
        """æ‰¹é‡ç”ŸæˆQA - æ”¯æŒçœŸæ­£çš„å¹¶è¡Œå¤„ç†"""
        start_trace(prefix="batch_cli")
        
        logger.info(f"å¼€å§‹æ‰¹é‡ç”ŸæˆQA: {len(entities)} ä¸ªå®ä½“")
        logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
        logger.info(f"é‡‡æ ·ç®—æ³•: {sampling_algorithm}")
        logger.info(f"QPSé™åˆ¶: {qps_limit}, å¹¶å‘æ•°: {parallel_workers}")
        logger.info(f"å³æ—¶ä¿å­˜: {enable_instant_save}, æ–­ç‚¹ç»­ä¼ : {enable_resume}")
        
        # æ–­ç‚¹ç»­ä¼ : åŠ è½½å·²å­˜åœ¨çš„ç»“æœ
        existing_results = {}
        entities_to_process = entities[:]
        skipped_entities = []
        
        if enable_resume:
            existing_results = self.load_existing_results(output_path)
            if existing_results:
                # è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯
                status = self.get_processing_status(entities, output_path)
                
                # è¿‡æ»¤æ‰å·²ç»å¤„ç†çš„å®ä½“ï¼ˆåŸºäºå®ä½“åç§°ï¼Œè€Œéé¡ºåºï¼‰
                entities_to_process = status['remaining_entities']
                skipped_entities = status['completed_entities']
                
                print(f"\nğŸ”„ æ–­ç‚¹ç»­ä¼ æ¨¡å¼:")
                print(f"   æ€»å®ä½“æ•°: {status['total_entities']} ä¸ª")
                print(f"   å·²å®Œæˆ: {status['completed_count']} ä¸ª ({status['completion_rate']*100:.1f}%)")
                print(f"   å¾…å¤„ç†: {status['remaining_count']} ä¸ª")
                
                # æ˜¾ç¤ºå®ŒæˆçŠ¶æ€å›¾
                print(f"\nğŸ“Š å®ŒæˆçŠ¶æ€:")
                status_line = ""
                for i, entity in enumerate(entities, 1):
                    status_info = status['completion_map'][entity]
                    status_line += status_info['status']
                    if i % 20 == 0:  # æ¯20ä¸ªæ¢è¡Œ
                        print(f"   {status_line}")
                        status_line = ""
                if status_line:
                    print(f"   {status_line}")
                
                print(f"\n   âœ… = å·²å®Œæˆ, â³ = å¾…å¤„ç†")
                logger.info(f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {len(skipped_entities)} ä¸ªå·²å®Œæˆå®ä½“")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_entities = len(entities)
        entities_to_process_count = len(entities_to_process)
        successful_qa = len(existing_results)  # å·²å­˜åœ¨çš„ç»“æœ
        failed_entities = []
        all_qa_results = list(existing_results.values())  # å·²å­˜åœ¨çš„ç»“æœ
        
        # åˆ›å»ºä¿¡å·é‡æ¥æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(parallel_workers)
        
        # åˆ›å»ºé€Ÿç‡é™åˆ¶å™¨
        rate_limiter = AsyncRateLimiter(qps_limit) if qps_limit > 0 else None
        
        # è¿›åº¦è·Ÿè¸ª
        processed_count = 0
        lock = asyncio.Lock()
        
        async def process_single_entity(entity: str, index: int) -> Optional[Dict[str, Any]]:
            """å¤„ç†å•ä¸ªå®ä½“çš„å¼‚æ­¥å‡½æ•°"""
            nonlocal processed_count, successful_qa
            
            async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
                try:
                    if rate_limiter:
                        await rate_limiter.acquire()  # æ§åˆ¶QPS
                    
                    print(f"\nğŸ”„ [{index:3d}/{total_entities}] å¼€å§‹å¤„ç†: {entity}")
                    logger.info(f"å¼€å§‹å¤„ç†å®ä½“ {index}/{total_entities}: {entity}")
                    
                    # åˆ›å»ºè¿è¡Œç®¡ç†å™¨
                    run_manager = RunManager()
                    run_name = f"cli_batch_{entity}_{index}"
                    run_id = run_manager.create_new_run(run_name)
                    
                    print(f"    ğŸ“ åˆ›å»ºè¿è¡Œè®°å½•: {run_id}")
                    logger.info(f"ä¸ºå®ä½“ '{entity}' åˆ›å»ºè¿è¡Œè®°å½•: {run_id}")
                    
                    # è·å–è¿è¡Œä¸“ç”¨é…ç½®
                    run_paths = run_manager.get_run_paths()
                    
                    try:
                        # æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå·²ç»åŒ…å«QAç”Ÿæˆï¼‰
                        print(f"    ğŸ—ï¸  æ„å»ºçŸ¥è¯†å›¾è°±å¹¶ç”ŸæˆQA...")
                        logger.info(f"å¼€å§‹ä¸ºå®ä½“ '{entity}' æ„å»ºçŸ¥è¯†å›¾è°±å¹¶ç”ŸæˆQA")
                        
                        from config import create_run_settings
                        from lib.graphrag_builder import GraphRagBuilder
                        
                        run_settings = create_run_settings(run_paths)
                        graphrag_builder = GraphRagBuilder(settings_instance=run_settings)
                        
                        # æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå†…éƒ¨ä¼šè‡ªåŠ¨ç”ŸæˆQAï¼‰
                        result = await graphrag_builder.build_knowledge_graph(
                            entity,
                            sampling_algorithm=sampling_algorithm,
                            use_unified_qa=use_unified_qa
                        )
                        
                        print(f"    âœ… çŸ¥è¯†å›¾è°±æ„å»ºå’ŒQAç”Ÿæˆå®Œæˆ")
                        logger.info(f"å®ä½“ '{entity}' çŸ¥è¯†å›¾è°±æ„å»ºå’ŒQAç”Ÿæˆå®Œæˆ")
                        
                        # ä¿å­˜è¿è¡Œç»“æœ
                        run_manager.save_result(result, "knowledge_graph_result.json")
                        run_manager.complete_run(success=True)
                        
                        # ç›´æ¥ä½¿ç”¨GraphRagæ„å»ºè¿‡ç¨‹ä¸­ç”Ÿæˆçš„QAç»“æœ
                        qa_pair = result.get('qa_pair', {})
                        
                        if qa_pair and qa_pair.get('question') and qa_pair.get('answer'):
                            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                            qa_result = {
                                'question': qa_pair.get('question', ''),
                                'answer': qa_pair.get('answer', ''),
                                'reasoning_path': qa_pair.get('reasoning_path', ''),
                                'entity_mapping': qa_pair.get('entity_mapping', {}),
                                'generation_metadata': qa_pair.get('generation_metadata', {}),
                                'source_entity': entity,
                                'run_id': run_id,
                                'sampling_algorithm': sampling_algorithm,
                                'timestamp': datetime.now().isoformat()
                            }
                            
                            # è®¡ç®—å½“å‰å®Œæˆè¿›åº¦ï¼ˆåŒ…æ‹¬ä¹‹å‰å·²å®Œæˆçš„ï¼‰
                            current_progress = processed_count + 1 + len(existing_results)
                            print(f"    ğŸ¯ QAå·²ç”Ÿæˆ! æ€»è¿›åº¦: {current_progress}/{total_entities} ({current_progress/total_entities*100:.1f}%)")
                            logger.info(f"å®ä½“ '{entity}' QAç”ŸæˆæˆåŠŸï¼Œæ€»è¿›åº¦: {current_progress}/{total_entities}")
                            
                            # æ¸…ç†GraphRagæ„å»ºå™¨
                            if 'graphrag_builder' in locals():
                                graphrag_builder.cleanup()
                            
                            # å³æ—¶ä¿å­˜
                            if enable_instant_save:
                                self.save_single_qa(qa_result, output_path)
                                print(f"    ğŸ’¾ å³æ—¶ä¿å­˜å®Œæˆ")
                            
                            # æ›´æ–°ç»Ÿè®¡
                            async with lock:
                                processed_count += 1
                                all_qa_results.append(qa_result)
                                successful_qa += 1
                            
                            return qa_result
                        else:
                            print(f"    âš ï¸  QAç”Ÿæˆä¸ºç©º")
                            logger.warning(f"å®ä½“ '{entity}' QAç”Ÿæˆç»“æœä¸ºç©º")
                            
                            # æ¸…ç†GraphRagæ„å»ºå™¨
                            if 'graphrag_builder' in locals():
                                graphrag_builder.cleanup()
                            
                            # æ›´æ–°ç»Ÿè®¡
                            async with lock:
                                processed_count += 1
                            
                            return None
                        
                    except Exception as e:
                        print(f"    âŒ å¤„ç†å¤±è´¥: {str(e)}")
                        logger.error(f"å¤„ç†å®ä½“ '{entity}' å¤±è´¥: {e}")
                        
                        async with lock:
                            processed_count += 1
                            failed_entities.append({'entity': entity, 'error': str(e), 'index': index})
                        
                        run_manager.complete_run(success=False, error_message=str(e))
                        return None
                        
                except Exception as e:
                    print(f"    ğŸ’¥ æœªé¢„æœŸé”™è¯¯: {str(e)}")
                    logger.error(f"å¤„ç†å®ä½“ '{entity}' æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
                    
                    async with lock:
                        processed_count += 1
                        failed_entities.append({'entity': entity, 'error': str(e), 'index': index})
                    
                    return None
        
        try:
            if entities_to_process_count == 0:
                print(f"\nâœ… æ‰€æœ‰å®ä½“å·²å¤„ç†å®Œæˆï¼Œæ— éœ€ç»§ç»­å¤„ç†!")
                print(f"ğŸ“Š æ€»è®¡: {total_entities} ä¸ªå®ä½“ï¼Œå·²å®Œæˆ: {len(existing_results)} ä¸ª")
            else:
                print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {entities_to_process_count} ä¸ªå®ä½“...")
                print(f"ğŸ“Š å¹¶å‘æ•°: {parallel_workers}, QPSé™åˆ¶: {qps_limit}")
                if enable_resume and existing_results:
                    print(f"ğŸ”„ æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {len(existing_results)} ä¸ªå·²å®Œæˆå®ä½“")
                print("=" * 80)
                
                # åˆ›å»ºè¦å¤„ç†çš„ä»»åŠ¡
                tasks = []
                for i, entity in enumerate(entities_to_process, 1):
                    # ä½¿ç”¨å…¨å±€ç´¢å¼•æ¥æ˜¾ç¤ºæ­£ç¡®çš„è¿›åº¦
                    global_index = entities.index(entity) + 1
                    task = asyncio.create_task(process_single_entity(entity, global_index))
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                results = await asyncio.gather(*tasks, return_exceptions=True)
            
            print(f"\n" + "=" * 80)
            
            # æœ€ç»ˆä¿å­˜ç»“æœ
            if not enable_instant_save:
                print(f"\n\nğŸ’¾ ä¿å­˜ {len(all_qa_results)} ä¸ªQAç»“æœåˆ°: {output_path}")
                self.save_qa_results(all_qa_results, output_path)
            else:
                print(f"\n\nâœ… å·²é€šè¿‡å³æ—¶ä¿å­˜å®Œæˆ {len(all_qa_results)} ä¸ªQAç»“æœ: {output_path}")
            
            # ä¿å­˜å¤±è´¥è®°å½•
            if failed_entities:
                failed_path = output_path.replace('.jsonl', '_failed.json')
                with open(failed_path, 'w', encoding='utf-8') as f:
                    json.dump(failed_entities, f, ensure_ascii=False, indent=2)
                print(f"âŒ å¤±è´¥è®°å½•ä¿å­˜åˆ°: {failed_path}")
            
            # ç»Ÿè®¡ç»“æœ
            result_summary = {
                'total_entities': total_entities,
                'processed_entities': processed_count,
                'successful_qa': successful_qa,
                'failed_entities': len(failed_entities),
                'success_rate': successful_qa / total_entities if total_entities > 0 else 0,
                'output_path': output_path,
                'timestamp': datetime.now().isoformat()
            }
            
            return result_summary
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”ŸæˆQAå¤±è´¥: {e}")
            raise
    
    def save_qa_results(self, qa_results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜QAç»“æœåˆ°æ–‡ä»¶"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºJSONLæ ¼å¼
        with open(output_file, 'w', encoding='utf-8') as f:
            for qa in qa_results:
                f.write(json.dumps(qa, ensure_ascii=False) + '\n')
        
        logger.info(f"ä¿å­˜ {len(qa_results)} ä¸ªQAç»“æœåˆ°: {output_path}")
    
    def print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°ç”Ÿæˆæ€»ç»“"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ‰¹é‡QAç”Ÿæˆæ€»ç»“")
        print("=" * 60)
        print(f"æ€»å®ä½“æ•°é‡: {summary['total_entities']}")
        print(f"å·²å¤„ç†å®ä½“: {summary['processed_entities']}")
        print(f"æˆåŠŸç”ŸæˆQA: {summary['successful_qa']}")
        print(f"å¤±è´¥å®ä½“æ•°: {summary['failed_entities']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"è¾“å‡ºæ–‡ä»¶: {summary['output_path']}")
        print(f"å®Œæˆæ—¶é—´: {summary['timestamp']}")
        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å‘½ä»¤è¡Œæ‰¹é‡QAç”Ÿæˆè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s                                           # äº¤äº’å¼é€‰æ‹©ç§å­æ–‡ä»¶
  %(prog)s --seed-file medical_entities.csv         # æŒ‡å®šç§å­æ–‡ä»¶
  %(prog)s --list-seeds                             # åˆ—å‡ºå¯ç”¨ç§å­æ–‡ä»¶
  %(prog)s --seed-file test.csv --output custom.jsonl --sample-size 10
  %(prog)s --seed-file entities.csv --qps-limit 1.5 --parallel-workers 1

è¾“å‡ºæ–‡ä»¶è¯´æ˜:
  - é»˜è®¤è¾“å‡ºåˆ° qa_output/ ç›®å½•
  - æ–‡ä»¶åæ ¼å¼: {ç§å­æ–‡ä»¶å}_{å®ä½“æ•°é‡}entities_{æ—¶é—´æˆ³}.jsonl
  - å¤±è´¥è®°å½•ä¼šä¿å­˜åˆ° {è¾“å‡ºæ–‡ä»¶}_failed.json
        """
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--seed-file', '-s', help='ç§å­æ–‡ä»¶åï¼ˆCSVæ ¼å¼ï¼‰')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--list-seeds', action='store_true', help='åˆ—å‡ºå¯ç”¨çš„ç§å­æ–‡ä»¶')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶çš„å¤„ç†çŠ¶æ€ï¼ˆéœ€è¦åŒæ—¶æŒ‡å®šseed-fileå’Œoutputï¼‰')
    
    # ç”Ÿæˆå‚æ•°

    parser.add_argument('--sampling-algorithm', choices=['mixed', 'augmented_chain', 'community_core_path', 'dual_core_bridge', 'max_chain'], 
                       default='max_chain', help='é‡‡æ ·ç®—æ³•ï¼ˆé»˜è®¤: max_chainï¼‰')
    parser.add_argument('--max-nodes', type=int, default=200, help='æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤: 200ï¼‰')
    parser.add_argument('--max-iterations', type=int, default=10, help='æœ€å¤§è¿­ä»£æ•°ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--use-traditional-qa', action='store_true', help='ä½¿ç”¨ä¼ ç»Ÿä¸¤æ­¥QAç”Ÿæˆï¼ˆé»˜è®¤ä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆå™¨ï¼‰')
    
    # æ€§èƒ½å‚æ•°
    parser.add_argument('--parallel-workers', type=int, default=20, help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 20')
    parser.add_argument('--qps-limit', type=float, default=20, help='QPSé™åˆ¶ï¼ˆé»˜è®¤: 20')
    
    # å³æ—¶ä¿å­˜å’Œæ–­ç‚¹ç»­ä¼ å‚æ•°
    parser.add_argument('--disable-instant-save', action='store_true', help='ç¦ç”¨å³æ—¶ä¿å­˜ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--disable-resume', action='store_true', help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--force-overwrite', action='store_true', help='å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œç¦ç”¨æ–­ç‚¹ç»­ä¼ ')
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºCLIå®ä¾‹
    cli = BatchQACLI()
    
    # åˆ—å‡ºç§å­æ–‡ä»¶
    if args.list_seeds:
        seed_files = cli.list_available_seed_files()
        print(f"\nğŸ“‚ åœ¨ {cli.seed_files_dir} ç›®å½•ä¸‹æ‰¾åˆ° {len(seed_files)} ä¸ªç§å­æ–‡ä»¶:")
        print("=" * 60)
        for i, file in enumerate(seed_files, 1):
            try:
                entities = cli.load_entities_from_csv(file)
                print(f"{i:2d}. {file} ({len(entities)} ä¸ªå®ä½“)")
            except Exception as e:
                print(f"{i:2d}. {file} (è¯»å–å¤±è´¥: {e})")
        print("=" * 60)
        return
    
    # æŸ¥çœ‹å¤„ç†çŠ¶æ€
    if args.status:
        if not args.seed_file or not args.output:
            print("âŒ --status é€‰é¡¹éœ€è¦åŒæ—¶æŒ‡å®š --seed-file å’Œ --output å‚æ•°")
            sys.exit(1)
        
        seed_file = args.seed_file
        if not seed_file.endswith('.csv'):
            seed_file += '.csv'
        
        try:
            entities = cli.load_entities_from_csv(seed_file)
            status = cli.get_processing_status(entities, args.output)
            
            print(f"\nğŸ“Š å¤„ç†çŠ¶æ€æŠ¥å‘Š")
            print("=" * 60)
            print(f"ç§å­æ–‡ä»¶: {seed_file}")
            print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
            print(f"æ€»å®ä½“æ•°: {status['total_entities']}")
            print(f"å·²å®Œæˆ: {status['completed_count']} ä¸ª ({status['completion_rate']*100:.1f}%)")
            print(f"å¾…å¤„ç†: {status['remaining_count']} ä¸ª")
            
            if status['completed_count'] > 0:
                print(f"\nâœ… å·²å®Œæˆçš„å®ä½“:")
                for entity in status['completed_entities']:
                    index = status['completion_map'][entity]['index']
                    print(f"   [{index:3d}] {entity}")
            
            if status['remaining_count'] > 0:
                print(f"\nâ³ å¾…å¤„ç†çš„å®ä½“:")
                for entity in status['remaining_entities']:
                    index = status['completion_map'][entity]['index']
                    print(f"   [{index:3d}] {entity}")
            
            # æ˜¾ç¤ºå®ŒæˆçŠ¶æ€å›¾
            print(f"\nğŸ“Š å®ŒæˆçŠ¶æ€å›¾:")
            status_line = ""
            for i, entity in enumerate(entities, 1):
                status_info = status['completion_map'][entity]
                status_line += status_info['status']
                if i % 20 == 0:  # æ¯20ä¸ªæ¢è¡Œ
                    print(f"   {status_line}")
                    status_line = ""
            if status_line:
                print(f"   {status_line}")
            
            print(f"\n   âœ… = å·²å®Œæˆ, â³ = å¾…å¤„ç†")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ æŸ¥çœ‹çŠ¶æ€å¤±è´¥: {e}")
            sys.exit(1)
        
        return
    
    # é€‰æ‹©ç§å­æ–‡ä»¶
    if args.seed_file:
        seed_file = args.seed_file
        if not seed_file.endswith('.csv'):
            seed_file += '.csv'
    else:
        seed_file = cli.interactive_select_seed_file()
    
    # åŠ è½½å®ä½“
    try:
        entities = cli.load_entities_from_csv(seed_file)
        print(f"\nâœ… æˆåŠŸåŠ è½½ç§å­æ–‡ä»¶: {seed_file}")
        print(f"ğŸ“Š åŒ…å« {len(entities)} ä¸ªå®ä½“")
    except Exception as e:
        print(f"âŒ åŠ è½½ç§å­æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output:
        output_path = args.output
    else:
        output_path = cli.generate_default_output_path(seed_file, len(entities))
    
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")
    
    # å¤„ç†å¼ºåˆ¶è¦†ç›–é€‰é¡¹
    enable_resume = not args.disable_resume and not args.force_overwrite
    enable_instant_save = not args.disable_instant_save
    
    if args.force_overwrite and os.path.exists(output_path):
        print(f"\nâš ï¸  å¼ºåˆ¶è¦†ç›–æ¨¡å¼: å°†åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶ {output_path}")
        try:
            os.remove(output_path)
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    # æ˜¾ç¤ºé…ç½®
    print(f"\nâš™ï¸  ç”Ÿæˆé…ç½®:")
    print(f"   é‡‡æ ·ç®—æ³•: {args.sampling_algorithm}")
    print(f"   æœ€å¤§èŠ‚ç‚¹æ•°: {args.max_nodes}")
    print(f"   æœ€å¤§è¿­ä»£æ•°: {args.max_iterations}")
    print(f"   QAç”Ÿæˆæ–¹å¼: {'ä¼ ç»Ÿä¸¤æ­¥æ³•' if args.use_traditional_qa else 'ç»Ÿä¸€ç”Ÿæˆå™¨'}")
    print(f"   å¹¶å‘å·¥ä½œæ•°: {args.parallel_workers}")
    print(f"   QPSé™åˆ¶: {args.qps_limit}")
    print(f"   å³æ—¶ä¿å­˜: {'å¯ç”¨' if enable_instant_save else 'ç¦ç”¨'}")
    print(f"   æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if enable_resume else 'ç¦ç”¨'}")
    print(f"   æ¨¡ç³ŠåŒ–æ¦‚ç‡: é»˜è®¤0.3 (æš‚ä¸æ”¯æŒè‡ªå®šä¹‰)")
    print(f"   é‡‡æ ·å¤§å°: ç”±GraphRagå†…éƒ¨æ™ºèƒ½å†³å®š")
    
    # ç¡®è®¤å¼€å§‹
    try:
        confirm = input(f"\nğŸš€ æ˜¯å¦å¼€å§‹ç”Ÿæˆ? (y/N): ")
        if confirm.lower() not in ['y', 'yes', 'æ˜¯']:
            print("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·å–æ¶ˆæ“ä½œ")
        sys.exit(0)
    
    # å¼€å§‹ç”Ÿæˆ
    try:
        print(f"\nğŸ”¥ å¼€å§‹æ‰¹é‡QAç”Ÿæˆ...")
        summary = asyncio.run(cli.batch_generate_qa(
            entities=entities,
            output_path=output_path,
            sampling_algorithm=args.sampling_algorithm,
            use_unified_qa=not args.use_traditional_qa,
            max_nodes=args.max_nodes,
            max_iterations=args.max_iterations,
            parallel_workers=args.parallel_workers,
            qps_limit=args.qps_limit,
            enable_resume=enable_resume,
            enable_instant_save=enable_instant_save
        ))
        
        cli.print_summary(summary)
        
        if summary['successful_qa'] > 0:
            print(f"\nğŸ‰ æ‰¹é‡QAç”Ÿæˆå®Œæˆï¼ç”Ÿæˆäº† {summary['successful_qa']} ä¸ªQAå¯¹")
        else:
            print(f"\nâš ï¸  æ‰¹é‡QAç”Ÿæˆå®Œæˆï¼Œä½†æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•QAå¯¹")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print(f"\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡QAç”Ÿæˆå¤±è´¥: {e}")
        logger.error(f"æ‰¹é‡QAç”Ÿæˆå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
