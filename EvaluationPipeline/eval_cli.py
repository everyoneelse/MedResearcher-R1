#!/usr/bin/env python3
"""
è¯„ä¼°ç®¡é“å‘½ä»¤è¡Œå·¥å…·

ä½¿ç”¨å‰é…ç½®APIå¯†é’¥ï¼š
  åœ¨ evaluation_config.json ä¸­è®¾ç½®ï¼š
  - llm.api_key_env: LLM APIå¯†é’¥ç¯å¢ƒå˜é‡å
  
  åœ¨ tools/tool_config.json ä¸­è®¾ç½®ï¼š
  - æœç´¢å’Œè®¿é—®å·¥å…·çš„APIå¯†é’¥é…ç½®

æ•°æ®é›†æ ¼å¼è¦æ±‚ï¼š
- JSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
- å¿…éœ€å­—æ®µï¼šquestion, answer
- å¯é€‰å­—æ®µï¼šcontext, metadata

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python eval_cli.py --mode interactive                      # äº¤äº’å¼ä½“éªŒagentèƒ½åŠ›
    python eval_cli.py --mode batch --dataset sample           # æ‰¹é‡æ¨ç†+è¯„ä¼°ï¼ˆè‡ªåŠ¨ç»­è·‘ï¼‰
    python eval_cli.py --mode batch --dataset sample --rollouts 3  # æ¯é¢˜æ¨ç†3æ¬¡
    python eval_cli.py --mode batch --dataset sample --workers 4   # ä½¿ç”¨4ä¸ªå¹¶è¡Œworker
    python eval_cli.py --list-datasets                         # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
"""

import argparse
import json
import logging
import os
import sys
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  tqdm not available, using simple progress display")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["LANGCHAIN_VERBOSITY"] = "error"
os.environ["OPENAI_LOG_LEVEL"] = "error"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
# å¯¼å…¥æ¨ç†æ¨¡å—
try:
    from src.core.reasoning_engine import create_reasoning_agent
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥æ¨ç†å¼•æ“æ¨¡å—")
    print("è¯·ç¡®ä¿åœ¨EvaluationPipelineç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def setup_logging_silence():
    """å¼ºåˆ¶å±è”½ç¬¬ä¸‰æ–¹åº“çš„å†—ä½™æ—¥å¿—"""
    silence_loggers = [
        'httpx', 'openai', 'urllib3', 'requests', 'httpcore',
        'transformers', 'langchain', 'langchain_core', 'langchain_openai',
        'langgraph', 'tiktoken', 'openai._base_client', 'httpx._client'
    ]
    
    for logger_name in silence_loggers:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.WARNING)
        logger.propagate = False
        
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
    
    root_logger = logging.getLogger()
    if root_logger.level < logging.INFO:
        root_logger.setLevel(logging.INFO)

# ç«‹å³æ‰§è¡Œæ—¥å¿—é™é»˜è®¾ç½®
setup_logging_silence()


def run_interactive_reasoning(reasoning_agent):
    """äº¤äº’å¼ä½“éªŒagentèƒ½åŠ›"""
    print("\nğŸ¤” äº¤äº’å¼æ¨ç†æ¨¡å¼")
    print("=" * 60)
    print("ğŸ’¡ è¾“å…¥é—®é¢˜ä½“éªŒagentçš„æ¨ç†èƒ½åŠ›ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    
    while True:
        try:
            question = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
            if not question:
                print("âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º")
                continue
            
            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            print(f"\nğŸš€ å¼€å§‹æ¨ç†: {question}")
            print("=" * 80)
            
            result = reasoning_agent.run(question)
            
            print("\nğŸ¯ æœ€ç»ˆç»“æœæ‘˜è¦:")
            print("=" * 80)
            print(f"é—®é¢˜: {question}")
            print(f"é¢„æµ‹ç­”æ¡ˆ: {result.get('prediction', 'No answer found.')}")
            print(f"å·¥å…·è°ƒç”¨æ¬¡æ•°: {result.get('tool_calls', 0)}")
            print(f"æ¨ç†è€—æ—¶: {result.get('duration', 0):.2f} ç§’")
            print(f"ç»ˆæ­¢åŸå› : {result.get('termination', 'unknown')}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·é€€å‡º")
            break
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            logger.error(f"äº¤äº’å¼æ¨ç†å¤±è´¥: {e}")


def run_batch_evaluation(reasoning_agent, dataset_name, rollouts=1, workers=10):
    """æ‰¹é‡è¿è¡Œï¼šæ¨ç†+è¯„ä¼°"""
    print(f"\nğŸ“Š æ‰¹é‡è¯„ä¼°æ¨¡å¼: {dataset_name}")
    print("=" * 60)
    
    print("ğŸ”‡ å±è”½ç¬¬ä¸‰æ–¹åº“æ—¥å¿—...")
    setup_logging_silence()
    
    additional_silence = [
        'openai._base_client', 'httpx._client', 'httpcore._sync',
        'transformers.tokenization_utils', 'transformers.modeling_utils',
        'urllib3.connectionpool', 'requests.packages.urllib3'
    ]
    for logger_name in additional_silence:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.CRITICAL)
        logger.propagate = False
    
    # æ£€æŸ¥æ•°æ®é›†
    dataset_path = f"datasets/{dataset_name}.jsonl"
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        print("ğŸ’¡ ä½¿ç”¨ --list-datasets æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
        return
    
    # åŠ è½½æ•°æ®é›†
    try:
        items = []
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    items.append(json.loads(line))
        
        if not items:
            print(f"âŒ æ•°æ®é›†ä¸ºç©º: {dataset_path}")
            return
        
        print(f"ğŸ“‚ æ•°æ®é›†: {dataset_name}")
        print(f"   é—®é¢˜æ•°é‡: {len(items)}")
        print(f"   æ¯é¢˜æ¨ç†æ¬¡æ•°: {rollouts}")
        print(f"   å¹¶è¡Œworkeræ•°: {workers}")
        print(f"   é¢„è®¡æ€»æ¨ç†æ¬¡æ•°: {len(items) * rollouts}")
        
        confirm = input(f"\nğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°? (y/N): ")
        if confirm.lower() not in ['y', 'yes', 'æ˜¯']:
            print("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
            
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return
    
    try:
        os.makedirs("results", exist_ok=True)
        
        completed_tasks = set()
        trajectory_path = f"results/trajectories_{dataset_name}.jsonl"
        
        if os.path.exists(trajectory_path):
            print(f"\nğŸ”„ ç»­è·‘æ¨¡å¼ï¼šæ£€æŸ¥å·²å®Œæˆä»»åŠ¡...")
            try:
                with open(trajectory_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            result = json.loads(line)
                            question = result.get('question', '').strip()
                            rollout = result.get('rollout', 1)
                            completed_tasks.add((question, rollout))
                print(f"   å·²å®Œæˆä»»åŠ¡: {len(completed_tasks)} ä¸ª")
            except Exception as e:
                print(f"   âš ï¸  è¯»å–å·²å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
                completed_tasks = set()
        
        print(f"\nğŸ”¥ å¼€å§‹æ‰¹é‡æ¨ç†...")
        start_time = datetime.now()
        
        tasks_to_process = []
        for i, item in enumerate(items):
            question = item.get('question', '')
            answer = item.get('answer', '')
            
            for rollout_id in range(rollouts):
                if (question.strip(), rollout_id + 1) not in completed_tasks:
                    tasks_to_process.append({
                        'item_index': i,
                        'question': question,
                        'answer': answer,
                        'rollout': rollout_id + 1,
                        'dataset': dataset_name
                    })
        
        print(f"   éœ€è¦å¤„ç†çš„ä»»åŠ¡æ•°: {len(tasks_to_process)}")
        print(f"   è·³è¿‡çš„å·²å®Œæˆä»»åŠ¡: {len(completed_tasks)}")
        
        if not tasks_to_process:
            print("âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ— éœ€å¤„ç†")
        else:
            trajectory_results = []
            token_stats = {'total_tokens': 0, 'token_limited_count': 0, 'max_tokens': 0}
            write_lock = threading.Lock()
            stats_lock = threading.Lock()
            
            def process_single_task(task):
                try:
                    reasoning_result = reasoning_agent.run(task['question'])
                    prediction = reasoning_result.get('prediction', 'No answer found.')
                    
                    trajectory_result = {
                        'question': task['question'],
                        'answer': task['answer'],
                        'prediction': prediction,
                        'rollout': task['rollout'],
                        'dataset': task['dataset']
                    }
                    
                    trajectory_result.update(reasoning_result)
                    
                    with write_lock:
                        with open(trajectory_path, 'a', encoding='utf-8') as f:
                            f.write(json.dumps(trajectory_result, ensure_ascii=False) + '\n')
                    
                    with stats_lock:
                        if 'token_count' in reasoning_result:
                            token_count = reasoning_result['token_count']
                            token_stats['total_tokens'] += token_count

                        if reasoning_result.get('termination') == 'exceed_token_length':
                            token_stats['token_limited_count'] += 1
                    
                    return trajectory_result
                    
                except Exception as e:
                    partial_reasoning_result = {}
                    try:
                        if hasattr(e, 'partial_result'):
                            partial_reasoning_result = e.partial_result
                    except:
                        pass
                    
                    error_result = {
                        'question': task['question'],
                        'answer': task['answer'],
                        'prediction': f'Error: {str(e)}',
                        'rollout': task['rollout'],
                        'error': str(e),
                        'dataset': task['dataset'],
                        'messages': [],
                        'termination': 'error',
                        'tool_calls': 0,
                        'duration': 0,
                        'token_count': 0
                    }
                    
                    if partial_reasoning_result:
                        error_result.update(partial_reasoning_result)
                    
                    with write_lock:
                        with open(trajectory_path, 'a', encoding='utf-8') as f:
                            f.write(json.dumps(error_result, ensure_ascii=False) + '\n')
                    
                    return error_result
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_task = {
                    executor.submit(process_single_task, task): task 
                    for task in tasks_to_process
                }
                
                if TQDM_AVAILABLE:
                    progress_bar = tqdm(
                        total=len(tasks_to_process),
                        desc="ğŸ”¥ æ‰¹é‡æ¨ç†",
                        unit="task",
                        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
                    )
                else:
                    progress_bar = None
                    print("   å¼€å§‹å¤„ç†ä»»åŠ¡...")
                
                completed_count = 0
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        result = future.result()
                        trajectory_results.append(result)
                        completed_count += 1
                        
                        if progress_bar:
                            progress_bar.set_postfix({
                                'question': task['question'][:25] + ('...' if len(task['question']) > 25 else ''),
                                'rollout': task['rollout']
                            })
                            progress_bar.update(1)
                        else:
                            if completed_count % max(1, len(tasks_to_process) // 20) == 0 or completed_count == len(tasks_to_process):
                                progress = completed_count / len(tasks_to_process) * 100
                                print(f"   è¿›åº¦: {completed_count}/{len(tasks_to_process)} ({progress:.1f}%) - æœ€æ–°å®Œæˆ: {task['question'][:30]}...")
                        
                    except Exception as e:
                        completed_count += 1
                        if progress_bar:
                            progress_bar.set_postfix({'error': str(e)[:30]})
                            progress_bar.update(1)
                        else:
                            print(f"   âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
                
                if progress_bar:
                    progress_bar.close()

        print(f"\nğŸ“„ æ¨ç†ç»“æœå·²ä¿å­˜: {trajectory_path}")
        
        if token_stats['total_tokens'] > 0:
            avg_tokens = token_stats['total_tokens'] / len(trajectory_results) if trajectory_results else 0
            print(f"\nğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡:")
            print(f"   æ€»Tokenæ•°: {token_stats['total_tokens']:,}")
            print(f"   å¹³å‡Tokenæ•°: {avg_tokens:.1f}")
            print(f"   æœ€å¤§Tokenæ•°: {token_stats['max_tokens']:,}")
            if token_stats['token_limited_count'] > 0:
                print(f"   âš ï¸  å› Tokenè¶…é™æå‰ç»“æŸ: {token_stats['token_limited_count']} æ¬¡")
        
        print(f"\nğŸ” å¼€å§‹è¯„ä¼°ç»“æœ...")
        
        import sys
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'TrajectoryGenerationPipeline', 'src', 'postprocessing'))
        from evaluation.evaluator import AnswerEvaluator
        
        config_path = os.path.join(
            os.path.dirname(__file__), '..', 
            'TrajectoryGenerationPipeline', 'src', 'postprocessing', 'config.json'
        )
        
        evaluator = AnswerEvaluator(config_path)
        
        all_trajectory_results = []
        if os.path.exists(trajectory_path):
            with open(trajectory_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        all_trajectory_results.append(json.loads(line))
        
        print(f"   ä»æ–‡ä»¶è¯»å– {len(all_trajectory_results)} ä¸ªç»“æœè¿›è¡Œè¯„ä¼°")
        
        print("   ğŸ” å¼€å§‹LLMè¯„ä¼°...")
        evaluated_results = evaluator.evaluate_batch(all_trajectory_results, dataset_type=dataset_name)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        evaluation_path = f"results/evaluation_{dataset_name}_{timestamp}"
        evaluation_stats = evaluator.save_evaluation_results(evaluated_results, evaluation_path, dataset_name)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"\nğŸ“ˆ æ‰¹é‡è¯„ä¼°å®Œæˆï¼Œç”¨æ—¶ {duration:.1f} ç§’")
        print("=" * 60)
        print(f"æ•°æ®é›†: {dataset_name}")
        print(f"æ€»é—®é¢˜æ•°: {len(items)}")
        print(f"æ€»æ¨ç†æ¬¡æ•°: {len(all_trajectory_results)}")
        print(f"æˆåŠŸæ¨ç†: {len([r for r in all_trajectory_results if 'error' not in r])}")
        print(f"å‡†ç¡®ç‡: {evaluation_stats.get('accuracy', 0):.3f} ({evaluation_stats.get('accuracy', 0)*100:.1f}%)")
        print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {evaluation_path}/")
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        logger.error(f"æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def list_datasets():
    """åˆ—å‡ºå¯ç”¨æ•°æ®é›†"""
    print("\nğŸ“‚ å¯ç”¨æ•°æ®é›†åˆ—è¡¨")
    print("=" * 60)
    
    os.makedirs("datasets", exist_ok=True)
    datasets_path = Path("datasets")
    
    datasets = []
    for jsonl_file in datasets_path.glob("*.jsonl"):
        try:
            count = sum(1 for line in open(jsonl_file, 'r') if line.strip())
            size_mb = jsonl_file.stat().st_size / (1024 * 1024)
            modified = datetime.fromtimestamp(jsonl_file.stat().st_mtime)
            
            datasets.append({
                'name': jsonl_file.stem,
                'count': count,
                'size_mb': size_mb,
                'modified': modified
            })
        except Exception as e:
            logger.warning(f"è¯»å–æ•°æ®é›†å¤±è´¥ {jsonl_file}: {e}")
    
    if not datasets:
        print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†")
        print("è¯·å°†JSONLæ ¼å¼çš„æ•°æ®é›†æ”¾åœ¨ datasets/ ç›®å½•ä¸‹")
        print("æ•°æ®é›†æ ¼å¼è¦æ±‚ï¼š")
        print("  - æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡")
        print("  - å¿…éœ€å­—æ®µï¼šquestion, answer")
    else:
        print(f"æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†:")
        print("-" * 80)
        for i, ds in enumerate(sorted(datasets, key=lambda x: x['name']), 1):
            print(f"{i:2d}. {ds['name']}")
            print(f"     é—®é¢˜æ•°é‡: {ds['count']}")
            print(f"     æ–‡ä»¶å¤§å°: {ds['size_mb']:.2f} MB")
            print(f"     ä¿®æ”¹æ—¶é—´: {ds['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
            print()


def create_sample_dataset(name="sample", size=5):
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
    print(f"\nğŸ› ï¸ åˆ›å»ºç¤ºä¾‹æ•°æ®é›†: {name}")
    
    os.makedirs("datasets", exist_ok=True)
    dataset_path = f"datasets/{name}.jsonl"
    
    sample_data = []
    for i in range(size):
        item = {
            'question': f'æµ‹è¯•é—®é¢˜{i+1}ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ',
            'answer': f'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œç¤ºä¾‹ç­”æ¡ˆ{i+1}ã€‚',
            'context': f'è¿™æ˜¯ç¬¬{i+1}ä¸ªæµ‹è¯•é—®é¢˜çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚',
            'metadata': {
                'difficulty': 'easy',
                'category': 'test',
                'source': 'generated'
            }
        }
        sample_data.append(item)
    
    with open(dataset_path, 'w', encoding='utf-8') as f:
        for item in sample_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… ç¤ºä¾‹æ•°æ®é›†åˆ›å»ºå®Œæˆ: {dataset_path}")
    print(f"   åŒ…å« {size} ä¸ªé—®é¢˜")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="è¯„ä¼°ç®¡é“å‘½ä»¤è¡Œå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨å‰é…ç½®APIå¯†é’¥ï¼š
  åœ¨ evaluation_config.json ä¸­è®¾ç½®ï¼š
  - llm.api_key_env: LLM APIå¯†é’¥ç¯å¢ƒå˜é‡å
  
  åœ¨ tools/tool_config.json ä¸­è®¾ç½®ï¼š
  - æœç´¢å’Œè®¿é—®å·¥å…·çš„APIå¯†é’¥é…ç½®

ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --mode interactive                        # äº¤äº’å¼ä½“éªŒagentèƒ½åŠ›
  %(prog)s --mode batch --dataset sample             # æ‰¹é‡æ¨ç†+è¯„ä¼°ï¼ˆè‡ªåŠ¨ç»­è·‘ï¼‰
  %(prog)s --mode batch --dataset sample --workers 4 # ä½¿ç”¨4ä¸ªå¹¶è¡Œworker
  %(prog)s --list-datasets                           # åˆ—å‡ºæ•°æ®é›†
  %(prog)s --create-sample                           # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
        """
    )
    
    parser.add_argument('--mode', choices=['interactive', 'batch'], help='è¿è¡Œæ¨¡å¼ï¼šinteractive(äº¤äº’å¼) æˆ– batch(æ‰¹é‡)')
    parser.add_argument('--dataset', '-d', help='æ•°æ®é›†åç§°ï¼ˆä¸åŒ…å«.jsonlæ‰©å±•åï¼Œç”¨äºbatchæ¨¡å¼ï¼‰')
    
    parser.add_argument('--list-datasets', action='store_true', help='åˆ—å‡ºå¯ç”¨æ•°æ®é›†')
    parser.add_argument('--create-sample', action='store_true', help='åˆ›å»ºç¤ºä¾‹æ•°æ®é›†')
    
    parser.add_argument('--rollouts', type=int, default=3, help='æ¯é¢˜æ¨ç†æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('--workers', type=int, default=10, help='å¹¶è¡Œworkeræ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰')
    
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        if args.list_datasets:
            list_datasets()
            return
        
        if args.create_sample:
            create_sample_dataset()
            return
        
        if args.mode == 'interactive':
            print("ğŸ”§ åˆå§‹åŒ–æ¨ç†å¼•æ“...")
            setup_logging_silence()
            try:
                reasoning_agent = create_reasoning_agent(verbose=True)
                print("âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
                print("ğŸ“¢ å·²å¯ç”¨è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼Œå°†æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ä¸­çš„æ¯æ­¥è¯¦æƒ…")
            except Exception as e:
                print(f"âŒ æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ’¡ è¯·æ£€æŸ¥ evaluation_config.json ä¸­çš„APIå¯†é’¥é…ç½®")
                print("   ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„ llm.api_key_env ç­‰å­—æ®µ")
                sys.exit(1)
            
            run_interactive_reasoning(reasoning_agent)
            
        elif args.mode == 'batch':
            if not args.dataset:
                print("âŒ æ‰¹é‡æ¨¡å¼éœ€è¦æŒ‡å®š --dataset å‚æ•°")
                print("ğŸ’¡ ä½¿ç”¨ --list-datasets æŸ¥çœ‹å¯ç”¨æ•°æ®é›†")
                sys.exit(1)
            
            print("ğŸ”§ åˆå§‹åŒ–æ¨ç†å¼•æ“...")
            setup_logging_silence()
            try:
                reasoning_agent = create_reasoning_agent(verbose=False)  # æ‰¹é‡æ¨¡å¼ä¸å¯ç”¨è¯¦ç»†è¾“å‡º
                print("âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ’¡ è¯·æ£€æŸ¥ evaluation_config.json ä¸­çš„APIå¯†é’¥é…ç½®")
                print("   ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„ llm.api_key_env ç­‰å­—æ®µ")
                sys.exit(1)
            
            run_batch_evaluation(reasoning_agent, args.dataset, args.rollouts, args.workers)
        else:
            print("è¯·æŒ‡å®šè¿è¡Œæ¨¡å¼:")
            print("  --mode interactive # äº¤äº’å¼ä½“éªŒagentèƒ½åŠ›")
            print("  --mode batch       # æ‰¹é‡æ¨ç†+è¯„ä¼°")
            print("  --list-datasets    # åˆ—å‡ºæ•°æ®é›†")
            print("  --create-sample    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 